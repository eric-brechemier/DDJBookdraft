== In The Newsroom

image::figs/incoming/02-00-cover.png[width="4.8in"]

How does data journalism sit within newsrooms around the world? How did leading data journalists convince their colleagues that it is a good idea to publish datasets or launch data-driven news apps? Should journalists learn how to code, or work in tandem with talented developers? In this section we look at the role of data and data journalism at the Australian Broadcasting Corporation, the BBC, the Chicago Tribune, the Guardian, the Texas Tribune, and the Zeit Online. We learn about how to spot and hire good developers, how to engage people around a topic through hackathons and other events, how to collaborate across borders, and business models for data journalism.

=== The ABC's Data Journalism Play 

Now in its 70th year the Australian Broadcasting Corporation is Australia's national public broadcaster. Annual funding is around AUS$1bn which delivers seven radio networks, 60 local radio stations, three digital television services, a new international television service and an online platform to deliver this ever expanding offering of digital and user generated content. At last count there were in excess of 4,500 full time equivalent staff and nearly 70% of them make content.

We are a national broadcaster fiercely proud of our independence; although funded by government, we are separated at arm's length through law. Our traditions are independent public service journalism. The ABC is regarded the most trusted news organization in the country.

These are exciting times; under a managing director (the former newspaper executive Mark Scott) content makers at the ABC have been encouraged to be `agile', as the corporate mantra puts it.

Of course, that's easier said than done. 

But one initiative in recent times designed to encourage this has been  a competitive staff pitch for money to develop multi-platform projects.

This is how the ABC's first ever data journalism project was conceived.

Sometime early in 2010 I wandered into the pitch session to face with three senior `ideas' people with my proposal.

I'd been chewing it over for some time. Greedily lapping up the data journalism that the now legendary Guardian Datablog was offering, and that was just for starters.

It was my argument that no doubt within 5 years the ABC would have its own data journalism unit. It was inevitable, I opined. But the question was how are we going to get there, and who's going to start.

For those readers unfamiliar with the ABC, think of a vast bureaucracy built up over 70 years. Its primary offering was always radio and television. With the advent of online in the last decade this content offering unfurled into text, stills and a degree of interactivity previously unimagined. The web space was forcing the ABC to rethink how it cut the cake (money) and rethink what kind of cake it was baking (content).

It is of course a work in progress. 

But something else was happening with data journalism. Government 2.0 (which as we discovered is largely observed in the breach in Australia) was starting to offer new ways of telling stories that were hitherto buried in the zero's and dots.

All this I said to the folk during my pitch. I also said we needed to identify new skills sets, train journalists in new tools. We needed a project to hit play. 

And they gave me the money.

On the 24th of November 2011 the ABC's multi-platform project and ABC News Online went live with http://bit.ly/abc-coal[`Coal Seam Gas by the Numbers'].

[[FIG021]]
._Coal Seam Gas by the Numbers_ (ABC News Online)
image::figs/incoming/02-01.png[width="4.8in"]

It was five pages of interactive maps, data visualizations and text. 

It wasn't exclusively data journalism, but a hybrid of journalisms that was born of the mix of people on the team and the story, which to put in context is raging as one of the hottest issues in Australia.  

The jewel was an interactive map showing coal seam gas wells and leases in Australia. Users could search by location and switch between modes to show leases or wells. By zooming in users could see who the explorer was, the status of the well and its drill date. Another map showed the location of coal Seam gas activity compared to the location of groundwater systems in Australia.

[[FIG023]]
.Interactive map of gas wells and leases in Australia (ABC News Online)
image::figs/incoming/02-02.png[width="4.8in"]

We had data visualizations which specifically addressed this issue of waste salt and water production that would be produced depending on the scenario that emerged.   

Another section of the project investigated the release of chemicals into a local river system.

==== Our team

  * A web developer and designer
  * A lead journalist  
  * A part time researcher with expertise in data extraction, excel spread sheets and data cleaning
  * A part time junior journalist
  * A consultant executive producer
  * A academic consultant with expertise in data mining, graphic visualization and advanced research skills
  * The services of a project manager and the administrative assistance of the ABC's multi-platform unit
  * Importantly we also had a reference group of journalists and others whom we consulted on a needs basis

==== Where did we get the data from?

The data for the interactive maps were scraped from shapefiles (a common kind of file for geospatial data) downloaded from government websites. 

Other data on salt and water were taken from a variety of reports.

The data on chemical releases was taken from environmental permits issued by the government. 

==== What did we learn?

`Coal Seam Gas by the Numbers' was ambitious in content and scale.  Uppermost in my mind was what did we learn and how might we do it differently next time?

The data journalism project brought a lot of people into the room who do not normally meet at the ABC. In lay terms, the hacks and the hackers. Many of us did not speak the same language or even appreciate what the other does.  Data journalism is disruptive!   

The practical things:

  * Co-location of the team is vital. Our developer and designer were off-site and came in for meetings. This is definitely not optimal! Place in the same room as the journalists. 
  * Our consultant EP was also on another level of the building. We needed to be much closer, just for the drop-by factor
  * Choose a story that is solely data driven.

==== The big picture: some ideas

Big media organzations need to engage in capacity building to meet the challenges of data journalism. My hunch is there are a lot of geeks and hackers hiding in media technical departments desperate to get out. So we need `hack and hacker meets' workshops where the secret geeks, younger journalists, web developers and designers come out to play with more experienced journalists for skill sharing and mentoring. Task: download this data set and go for it! 

Ipso facto data journalism is interdisciplinary. Data journalism teams are made of people who would not in the past have worked together. The digital space has blurred the boundaries.

We live in a fractured, distrustful body politic. The business model that formerly delivered professional independent journalism -- imperfect as it is -- is on the verge of collapse. We ought to ask ourselves, as many now are, what might the world look like without a viable fourth estate? The American journalist and intellectual Walter Lippman remarked in the 1920’s that ``it is admitted that a sound public opinion cannot exist without access to news''. That statement is no less true now. In the 21st century everyone’s hanging out in the blogosphere. It’s hard to tell the spinners, liars, dissemblers and vested interest groups from the professional journalists. Pretty much any site or source can be made to look credible, slick and honest. The trustworthy mastheads are dying in the ditch. And in this new space of junk journalism, hyperlinks can endlessly take the reader to other more useless but brilliant looking sources that keep hyperlinking back into the digital hall of mirrors. The technical term for this is: bullshit baffles brains. 

In the digital space everyone’s a storyteller now, right? Wrong. If professional journalism  -– and by that I mean those who embrace ethical, balanced, courageous truth seeking storytelling –- is to survive then the craft must reassert itself in the digital space. Data journalism is just another tool by which we will navigate the digital space. It’s where we will map, flip, sort, filter, extract and see the story amidst all those 0's and 1's. In the future we’ll be working side by side with the hackers, the developers the designers and the coders. It’s a transition that requires serious capacity building. We need news managers who ``get'' the digital/journalism connection to start investing in the build.

&mdash; _Wendy Carlisle, Australian Broadcasting Corporation_

=== Data Journalism at the BBC

[[FIG024]]
._The World at Seven Billion_ (BBC) 
image::figs/incoming/02-05.png[width="4.8in"]

The term `data journalism' can cover a range of disciplines and is used in varying ways in news organizations, so it may be helpful to define what we mean by `data journalism' at the BBC. Broadly the term covers projects that use data to do one or more of the following:

  * Enable a reader to discover information that is personally relevant
  * Reveal a story that is remarkable and previously unknown
  * Help the reader to better understand a complex issue

These categories may overlap and in an online environment can often benefit from some level of visualization.

==== Make It Personal

On the BBC News website we have been using data to provide services and tools for our users for well over a decade.

The most consistent example, which we first published in 1999, is our http://bbc.in/school-league-tables[school league tables], which use the data published annually by the government. Readers can find local schools by entering a postcode, and compare them on a range of indicators. Education journalists also work with the development team to trawl the data for stories ahead of publication.

When we started to do this there was no official site that provided a way for the public to interrogate the data. But now that the Department for Education has its own comparable service, our offering has shifted to focus more on the stories emerging from the data.

The challenge in this area must be to provide access to data in which there is a clear public interest. A recent example of a project where we exposed a large dataset not normally available to the wider public was the special report http://bbc.in/road-deaths[Every death on every road]. We provided a postcode search allowing users to find the location of all road fatalities in the UK in the past decade.

We http://bbc.in/police-data[visualized some of the main facts and figures] emerging from the police data and, to give the project a more dynamic feel and a human face, we teamed up with the London Ambulance Association and BBC London radio and TV to track crashes across the capital as they happened. This was reported http://bbc.in/road-deaths-feed[live online], as well as via Twitter using the hashtag #crash24 and the collisions were http://bbc.in/road-deaths-map[mapped] as they were reported.

==== Simple Tools

As well as providing ways to explore large data sets, we have also had success creating simple tools for users that provide personally relevant snippets of information. These tools appeal to the time-poor who may not choose to explore lengthy analysis. The ability to easily share a `personal' fact is something we have begun to incorporate as standard.

A light-hearted example of this approach is our feature http://bbc.in/KQsSzB[The world at 7 billion: What's your number?] published to coincide with the official date at which the world's population exceeded 7 billion. By entering their birth date the user could find out what `number' they were, in terms of the global population, when they were born and then share that number via twitter or Facebook. The application used data provided by the UN population development fund. It was very popular, and became the most shared link on Facebook in the UK in 2011.

Another recent example is the BBC http://bbc.in/JepssY[budget calculator] which enabled users to find out how better or worse off they will be when the Chancellor's budget takes effect -- and then share that figure. We teamed up with the accountancy firm KPMG LLP, who provided us with calculations based on the annual budget, and then we worked hard to create an appealing interface that would encourage users to complete the task.

==== Mining The Data

But where is the journalism in all this? Finding stories in data is a more traditional definition of data journalism. Is there an exclusive buried in the database? Are the figures accurate? Do they prove or disprove a problem? These are all questions a data journalist or computer-assisted reporter must ask themselves. But a great deal of time can be taken up sifting through a massive data set in the hope of finding something remarkable.

In this area we have found it most productive to partner with investigative teams or programs which have the expertise and time to investigate a story. The BBC current affairs program Panorama spent months working with the Centre for Investigative Journalism, gathering data on public sector pay. The result was a TV documentary and online the special report http://bbc.in/IKPrL2[Public Sector pay: The numbers] where all the data was published and visualized with sector by sector analysis.

As well as partnering with investigative journalists, having access to numerate journalists with specialist knowledge is essential. When a business colleague on the team analyzed the spending review cuts data put out by the government he came to the conclusion that it was making them sound bigger than they actually were. The result was an exclusive story, http://bbc.in/LcuGFV[Making sense of the data], complemented by a http://bbc.in/IIADrj[clear visualization] which won a Royal Statistical Society award.

==== Understanding An Issue

But data journalism doesn't have to be an exclusive no-one else has spotted. The job of the data visualization team is to combine great design with a clear editorial narrative to provide a compelling experience for the user. Engaging visualizations of the right data can be used to give a better understanding of an issue or story and we frequently use this approach in our storytelling at the BBC. Heat-mapping data over time to give clear view of change is one technique used here in our http://bbc.in/KF7IKU[UK claimant count tracker].

The data feature http://bbc.in/IIAHHI[Eurozone debt web] explores the tangled web of intra-country lending. It helps to explain a complicated issue in a visual way, using colour and proportional arrows combined with clear text. An important consideration is to encourage the user to explore the feature, or follow a narrative, and never feel overwhelmed by the numbers.

==== Team Overview

The team that produces data journalism for the BBC News website is comprised of about 20 journalists, designers and developers.

As well as data projects and visualizations the team produces all the infographics and interactive multimedia features on the news website. Together these form a collection of storytelling techniques we have come to call `visual journalism'. We don't have people who are specifically identified as `data' journalists, but all editorial staff on the team have to be proficient at using basic spreadsheet applications such as Excel and Google Docs to analyze data.

Central to any data projects are the technical skills and advice of our developers and the visualization skills of our designers. While we are all either a journalist, designer or developer `first' we continue to work hard to increase our understanding and proficiency in each other's areas of expertise.

The core products for interrogating data are Excel, Google Docs and Fusion Tables. The team has also, but to a lesser extent, used MySQL and Access databases and Solr for interrogating larger data sets and used RDF and SPARQL to begin looking at ways in which we can model events using Linked Data technologies. Developers will also use their programming language of choice, whether that's ActionScript, Python or Perl, to match, parse or generally pick apart a dataset we might be working on. Perl is used for some of the publishing.

We use Google and Bing Maps and Google Earth along with Esri's ArcMAP for exploring and visualizing geographical data.

For graphics we use the Adobe Suite including After Effects, Illustrator, Photoshop and Flash, although we would rarely publish Flash files on the site these days as JavaScript, particularly JQuery and other JavaScript libraries like Highcharts, Raphael and D3 increasingly meet our data visualization requirements.

&mdash; _Bella Hurrell and Andrew Leimdorfer, BBC_

=== How the News Apps Team at Chicago Tribune Works

[[FIG025]]
.The Chicago Tribune news applications team (Photo by Heather Billings) 
image::figs/incoming/02-00.jpg[width="4.8in"]

The news applications team at the Chicago Tribune is a band of happy hackers embedded in the newsroom. We work closely with editors and reporters to help: (1) research and report stories, (2) illustrate stories online and (3) build evergreen web resources for the fine people of Chicagoland.

It's important that we sit in the newsroom. We usually find work via face-to-face conversations with reporters. They know that we're happy to help write a screen scraper for a crummy government website, tear up a stack of PDFs, or otherwise turn non-data into something you can analyze. It's sort of our team's loss leader; this way we find out about potential data projects at their outset.

Unlike many teams in this field, our team was founded by technologists for whom journalism was a career change. Some of us acquired a masters degree in journalism after several years coding for business purposes, and others were borrowed from the open government community.

We work in an agile fashion. To make sure we're always in sync, every morning begins with a 5-minute stand up meeting. We frequently program in pairs; two developers at one keyboard are often more productive than two developers at two keyboards. Most projects don't take more than a week to produce, but on longer projects we work in week-long iterations, and show our work to stakeholders (reporters and editors usually) every week. ``Fail fast'' is the mantra. If you're doing it wrong, you need to know as soon as possible, especially when you're coding on a deadline!

There's a tremendous upside to hacking iteratively, on a deadline: We're always updating our toolkit. Every week we crank out an app or two, then, unlike normal software shops, we can put it to the back of our mind and move on to the next project. It's a joy we share with the reporters, every week we learn something new.

All app ideas come from the reporters and editors in the newsroom. This, I believe, sets us apart from apps teams in other newsrooms, who frequently spawn their own ideas. We've built strong personal and professional relationships in the newsroom, and folks know that when they have data, they come to us.

Much of our work in the newsroom is reporter support. We help reporters dig through data, turn PDFs back into spreadsheets, screen-scrape websites, etc. It's a service that we like to provide because it gets us in early on the data work that's happening in the newsroom. Some of that work becomes a news application: a map, table, or sometimes a larger-scale website.

Before, we linked to the app from the written story, which didn't result in much traffic. These days apps run near the top of our website, and the app links through to the story, which works nicely for both the app and the story. There is a http://www.chicagotribune.com/news/data/[section of the website for our work], but it's not well-trafficked. But that's not surprising. ``Hey, today I want some data!,'' isn't a very big use case.

We love page views, and we love the accolades of our peers, but that's weak sauce. The motivation should always be impact; on people's lives, on the law, on holding politicians to account, and so on. The written piece will speak to the trend and humanise it with a few anecdotes. But what's the reader to do when they've finished the story? Is their family safe? Are their children being educated properly? Our work sings when it helps a reader find his or her _own_ story in the data. Examples of impactful, personalised work that we've done include our http://nursinghomes.apps.chicagotribune.com/[Nursing Home Safety Reports] and http://schools.chicagotribune.com/[School Report Card] apps.

&mdash; _Brian Boyer, Chicago Tribune_ 

=== Behind the Scenes at the Guardian Datablog 
[[FIG026]]
.The Guardian Datablog production process visualized (The Guardian) 
image::figs/incoming/02-ZZ.jpg[height="6.8in"]

When we launched the Datablog, we had no idea who would be interested in raw data, statistics and visualizations. As someone pretty senior in my office said: ``why would anyone want that?''.

The http://www.guardian.co.uk/datablog[Guardian Datablog], which I edit, was to be a small blog offering the full datasets behind our news stories. Now it consists of a front page (guardian.co.uk/data); searches of world government and global development data; data visualizations by from around the web and Guardian graphic artists, and tools for exploring public spending data. Every day, we use Google spreadsheets to share the full data behind our work; we visualize and analyze that data, then use it to provides stories for the newspaper and the site. 

As a news editor and journalist working with graphics, it was a logical extension of work I was already doing, accumulating new datasets and wrangling with them to try to make sense of the news stories of the day.

The question I was asked has been answered for us. It has been an incredible few years for public data. Obama opened up the US government's data faults as his first legislative act, followed by government data sites around the world: Australia, New Zealand, the British government's Data.gov.uk

We've had the MPs expenses scandal, Britain's most unexpected piece of data journalism; the resulting fallout has meant Westminster is now committed to releasing huge amounts of data every year.

We had a general election where each of the main political parties was committed to data transparency, opening our own data vaults to the world. We've had newspapers devoting valuable column inches to the release of the Treasury's COINS database.

At the same time, as the web pumps out more and more data, readers from around the world are more interested in the raw facts behind the news than ever before. When we launched the Datablog, we thought the audiences would be developers building applications. In fact, it's people wanting to know more about carbon emissions or Eastern European immigration or the breakdown of deaths in Afghanistan, or even the number of times the Beatles used the word ``love'' in their songs (613).

Gradually, the Datablog's work has reflected and added to the stories we faced. We crowdsourced 458,000 documents relating to MPs' expenses and we analyzed the detailed data of which MPs had claimed what. We helped our users explore detailed Treasury spending databases and published the data behind the news.

But the game-changer for data journalism happened in spring 2010, beginning with one spreadsheet: 92,201 rows of data, each one containing a detailed breakdown of a military event in Afghanistan. This was the WikiLeaks war logs. Part one, that is. There were to be two more episodes to follow: Iraq and the cables. The official term for the first two parts was SIGACTS: the US military Significant Actions Database.

News organizations are all about geography; and proximity to the news desk. If you're close, it's easy to suggest stories and become part of the process; conversely out of sight is literally out of mind. Before Wikileaks, we were sat on a different floor, with graphics. Since Wikileaks, we have sat on the same floor, next to the newsdesk. It means that it's easier for us to suggest ideas to the desk, and for reporters across the newsroom to think of us to help with stories.

It's not that long ago journalists were the gatekeepers to official data. We would write stories about the numbers and release them to a grateful public, who were not interested in the raw statistics. The idea of us allowing our raw information into our newspapers was anathema.

Now that dynamic has changed beyond recognition. Our role is becoming interpreters; helping people understand the data, and even just publishing it because it's interesting in itself.

But numbers without analysis are just numbers, which is where we fit in. When Britain's prime minister claims the riots in August 2011 were not about poverty, we were able to map the addresses of the rioters with poverty indicators to show the truth behind the claim.

Behind all our data journalism stories is a process. It's changing all the time as use new tools and techniques. Some people say the answer is to become a sort of super hacker, write code and immerse yourself in SQL. You can decide to take that approach. But a lot of the work we do is just in Excel.

Firstly, we locate the data or receive it from a variety of sources, from breaking news stories, government data, journalists' research and so on. We then start looking at what we can do with the data; do we need to mash it up with another dataset? How can we show changes over time? Those spreadsheets often have to be seriously tidied up; all those extraneous columns and weirdly merged cells really don't help. And that's assuming it's not a PDF, the worst format for data known to humankind.

Often official data comes with the official codes added in; each school, hospital, constituency and local authority has a unique identifier code.

Countries have them too (the UK's code is GB, for instance). They're useful because you may want to start mashing datasets together and it's amazing how many different spellings and word arrangements can get in the way of that. There's Burma and Myanmar, for instance, or Fayette County in the US; there are 11 in states from Georgia to West Virginia. Codes allow us to compare like with like.

At the end of that process is the output; will it be a story or a graphic or a visualization, and what tools will we use? Our top tools are the free ones that we can produce something quickly with. The more sophisticated graphics are produced by our dev team.

Which means we commonly use Google charts for small line graphs and pies, or Google Fusion Tables to create maps quickly and easily.

It may seem new, but really it's not.

In the very first issue of the Manchester Guardian, Saturday 5 May, 1821, the news was on the back page, like all papers of the day. First item on the front page was an ad for a missing Labrador.

And, amid the stories and poetry excerpts, a third of that back page is taken up with, well, facts. A comprehensive table of the costs of schools in the area never before ``laid before the public'', writes ``NH''.

NH wanted his data published because otherwise the facts would be left to untrained clergymen to report. His motivation was that: ``Such information as it contains is valuable; because, without knowing the extent to which education … prevails, the best opinions which can be formed of the condition and future progress of society must be necessarily incorrect.'' In other words, if the people don't know what's going on, how can society get any better?

I can't think of a better rationale now for what we're trying to do. Now what once was a back page story can now make front page news.

&mdash; _Simon Rogers, The Guardian_

=== Data Journalism at the Zeit Online

[[FIG027]]
._PISA based Wealth Comparison_ (Zeit Online)
image::figs/incoming/02-03-AA.png[width="4.8in"]

The http://bit.ly/Pisa_Wealth[PISA based Wealth Comparison] project is an interactive visualization that enables comparison of standards of living in different countries. The interactive uses data from the OECD's comprehensive world education ranking report, http://bit.ly/Pisa_2009[PISA 2009], published in December 2010. The report is based on a questionnaire which asks fifteen-year-old pupils about their living situation at home.

The idea was to analyze and visualize this data to provide a unique way of comparing standards of living in different countries.

First of all our in-house editorial team decided which facts seemed to be useful to make living standards comparable and should be visualized, including:

  * Wealth (number of owned TVs, cars and available bathrooms at home)
  * Family-situation (are there grandparents living with the family together, percentage share of families with only one child, unemployment of parents and mother's job status)
  * Access to knowledge sources (internet at home, frequency of using e-mail and quantity of owned books)
  * Three additional indicators on the level of development of each country.

With the help of the internal design team these facts were translated into self-explanatory icons. A front end design was built to make comparison between the different countries like in a card-game possible.

Next we contacted people from the German http://opendata-network.org/[Open Data Network] to find developers who could help with the project. This community of highly motivated people suggested Gregor Aisch, a very talented information designer, to code the application that would make our dreams come true (without using Flash, which was very important to us!). Gregor created a very high quality and interactive visualization with a beautiful bubble-style, based on the http://raphaeljs.com/[Raphaël-Javascript Library].

The result of our collaboration was a very successful interactive which got a lot of traffic. It is easy to compare any two countries, which makes it useful as a reference tool. This means that we can re-use it in our daily editorial work. For example if we are covering something related to the living situation in Indonesia, we can quickly and easily embed a graphic http://bit.ly/Pisa_Indonesia_Germany[comparing the living situation in Indonesia and Germany]. The know-how transferred to our in-house team was a great investment for future projects.

At the Zeit Online, we've found that http://www.zeit.de/datenjournalismus[our data journalism projects] have brought us a lot of traffic and have helped us to engage audiences in new ways. For example, there was a wide coverage about the situation at the nuclear plant in Fukushima after the Tsunami in Japan. After radioactive material escaped from the power plant, everyone within 30 kilometres of the plant was evacuated. People could read and see a lot about the evacuations. Zeit Online found a innovative way to explain the impact of this to our German audience. We asked: How many people live near a nuclear power plant in Germany? How many people live within a radius of 30 kilometres? A map shows http://bit.ly/near_nuclear[how many people would have to be evacuated in a similar situation in Germany]. The result: lots and lots of traffic and the project went viral over the social media sphere. Data journalism projects can be relatively easily adapted to other languages. We created an English language version about proximity to nuclear power plants in the US, which was a great traffic motor. News organizations want to be recognized as trusted and authoritative sources amongst their readers.  We find that data journalism projects combined with enabling our readers to look and reuse the raw data brings us a high degree of credibility.

For two years the R&D Department and the Editor-in-Chief at the Zeit Online, Wolfgang Blau, have been advocating data journalism as an important way to tell stories. Transparency, credibility and user engagement are important parts of our philosophy. That is why data journalism is a natural part of our current and future work. Data visualizations can bring value to the reception of a story, and are an attractive way for the whole editorial team to present their content.

For example, on 9th November 2011 Deutsche Bank pledged to stop financing cluster bomb manufacturers. But according to a study by non-profit organzation Facing Finance, the bank continued to approve loans to producers of cluster munitions after that promise was made. http://zeit.de/wirtschaft/cluster-munition[Our visualization] based on the data shows the various flows of money to our readers. The different parts of the Deutsche Bank company are arranged at the top, with the companies accused of involvement in building cluster munitions at the bottom. In between, the individual loans are represented along a timeline. Rolling over the circles shows the details of each transaction. Of course the story could have been told as a written article. But the visualization enables our readers to understand and explore the financial dependencies in a more intuitive way.

[[FIG028]]
._The Business of Bombs_ (Zeit Online)
image::figs/incoming/02-03-DD.png[width="4.8in"]

To take another example: the https://www.destatis.de/EN/Homepage.html[German Federal Statistic Office] has published a great dataset on vital statistics for Germany, including http://bit.ly/German_Federal_Statistics[modelling various demographic scenarios up until 2060]. The typical way to represent this is a population pyramid, such as https://www.destatis.de/bevoelkerungspyramide/[the one from the Federal Statistics Agency].

[[FIG029]]
.Population pyramid for Germany (German Federal Statistic Office)
image::figs/incoming/02-03-BB.png[width="4.8in"]

With our colleagues from the science department we tried to give our readers a better way to explore the projected demographic data about our future society. With http://www.zeit.de/wissen/altersstruktur[our visualization], we present a statistically representative group of 40 people of different ages from the years 1950 till 2060.They are organised into eight different groups. It looks like a group photo of German society at different points in time. The same data visualized in a traditional population pyramid gives only a very abstract feeling of the situation, but a group with kids, younger people, adults and elderly people means our readers can relate to the data more easily. You can just hit the play button to start a journey through eleven decades. You can also enter your own year of birth and gender to become part of the group photo: to see your demographic journey through the decades and your own life expectancy.

&mdash; _Sascha Venohr, Zeit Online_

[[FIG0210]]
.visualizing demographic data (Zeit Online)
image::figs/incoming/02-03-CC.png[width="4.8in"]

=== How to Hire a Hacker

One of the things that I am regularly asked by journalists is 'how do I get a coder to help me with my project?'. Don't be deceived into thinking this is a one-way process; civic-minded hackers and data-wranglers are often just as keen to get in touch with journalists. 

Journalists are power-users of data driven tools and services. From the perspective of developers: journalists think outside the box to use data tools in contexts developers haven't always considered before (feedback is invaluable!) they also help to build context and buzz around projects and help to make them relevant. It is a symbiotic relationship.

Fortunately, this means that whether you are looking to hire a hacker or looking for possible collaborations on a shoestring budget, there will more than likely be someone out there who is interested in helping you.

So how do you find them? Says Aron Pilhofer from the New York Times:

[quote]
____
You may find that your organzation already has people with all the skills you need, but they are not necessarily already in your newsroom. Wander around, visit the technology and IT departments and you are likely to strike gold. It is also important to appreciate coder culture, come across someone who has a computer that looks like the one in <<FIG0211>>...then you are probably onto a winner.
____

[[FIG0211]]
.Badge of honor: hackers/coders are often easy to spot
image::figs/incoming/02-04.jpg[width="4.8in"]

Here are a few more ideas:

Post on job websites::
  Identify and post to websites aimed at developers who work in different programming languages. For example, the http://www.python.org/community/jobs/[Python Job Board].
Contact relevant mailing lists::
  For example, the http://bit.ly/nicar-subscribe[NICAR-L] and http://bit.ly/ddj-list[Data Driven Journalism] mailing lists].
Contact relevant organizations::
  For example, if you want to clean up or scrape data from the web, you could contact an organization such as https://scraperwiki.com/[Scraperwiki], who have a great address book of trusted and willing coders.
Join relevant groups/networks::
  Look out for initiatives such as http://hackshackers.com/[Hacks/Hackers] which bring journalists and techies together. Hacks/Hackers groups are now springing up all around the world. You could also try posting something to their http://bit.ly/hacks-hackers-jobs[jobs newsletter].
Local interest communities::
  You could try doing a quick search for an area of expertise in your area (e.g. `javascript' + `london'). Sites such as Meetup.com can also be a great place to start.
Hackathons and competitions::
  Whether or not there is prize money available: app and visualization competitions and development days are often fruitful ground for collaboration and making connections.
Ask a geek!:
  Geeks hang around with other geeks. Word of mouth is always a good way to find good people to work with.

Once you've found a hacker, how do you know if they are any good? We asked Alastair Dant, the Guardian's Lead Interactive Technologist, for his views on how to spot a good one:

They code the full stack::
  When dealing with deadlines, it's better to be a jack of all trades than a master of one. News apps require data wrangling, dynamic graphics and derring-do.
They see the whole picture::
  Holistic thinking favours narrative value over technical detail. I'd rather hear one note played with feeling than unceasing virtuosity in obscure scales. Find out how happy someone is to work alongside a designer.
They tell a good story::
  Narrative presentation requires arranging things in space and time. Find out what project they're most proud of and ask them to walk you through how it was built; this will reveal as much about their ability to communicate as their technical understanding.
They talk things through::
  Building things fast requires mixed teams working towards common goals. Each participant should respect their fellows and be willing to negotiate. Unforeseen obstacles often require rapid re-planning and collective compromise.
They teach themselves::
  Technology moves fast. It's a struggle to keep up with. Having met good developers from all sorts of backgrounds, the most common trait is a willingness to learn new stuff on demand.

&mdash; _Lucy Chambers, Open Knowledge Foundation_


.How To Find Your Dream Developer
****
The productivity difference between a good and a great developer is not linear, it's exponential. Hiring well is extremely important. Unfortunately, hiring well is also very difficult. It's hard enough to vet candidates if you are not an experienced technical manager. Add to that the salaries that news organizations can afford to pay, and you've got quite a challenge.

At Tribune, we recruit with two angles: an emotional appeal and a technical appeal. The emotional appeal is this: Journalism is essential to a functioning democracy. Work here and you can change the world. Technically, we promote how much you'll learn. Our projects are small, fast and iterative. Every project is a new set of tools, a new language, a new topic (fire safety, the pension system) that you must learn. The newsroom is a crucible. I've never managed a team that has learned so much, so fast, as our team.

As for where to look, we've had great luck finding great hackers in the open government community. The Sunlight Labs mailing list is where do-gooder nerds with shitty day jobs hang out at night. Another potential resource is Code for America. Every year, a group of fellows emerges from CfA, looking for their next big project. And as a bonus, CfA has a rigorous interview process; they've already done the vetting for you. Nowadays, programming-interested journalists are also emerging from journalism schools. They're green, but they've got tons of potential.

Lastly, it's not enough to just hire developers. You need technical management. A lone-gun developer (especially fresh from journalism school, with no industry experience) is going to make many bad decisions. Even the best programmer, when left to her own devices, will choose technically interesting work over doing what's most important to your audience. Call this hire a news applications editor, a project manager, whatever. Just like writers, programmers need editors, mentorship and somebody to wrangle them towards making software on deadline.

&mdash; _Brian Boyer, Chicago Tribune_
****


=== Harnessing External Expertise Through Hackathons

[[FIG0212]]
.Journalists and developers at RegioHack (photo by Jerry Vermanen) 
image::figs/incoming/02-XY.jpg[width="4.8in"]

In March 2010, Utrecht based digital culture organzation SETUP put on an event called http://setup.nl/content/hacking-journalism[`Hacking Journalism']. The event was organised to encourage greater collaboration between developers and journalists.

`We organize hackathons to make cool applications, but we can't recognise interesting stories in data. What we build has no social relevance', said the programmers. `We recognize the importance of data journalism, but we don't have all the technical skills to build the things we want', said the journalists.

Working for a regional newspaper, there was no money or incentive to hire a programmer for the newsroom. Data journalism was still an unknown quantity for Dutch newspapers at that time.

The hackathon model was perfect. A relaxed environment for collaboration, with plenty of pizza and energy drinks. http://www.regiohack.nl/[RegioHack] was a hackathon organised by my employer, the regional newspaper http://www.destentor.nl/[De Stentor], our sister publication http://www.tctubantia.nl/[TC Tubantia] and http://saxion.nl/[Saxion Hogescholen Enschede], who provided the location for the event.

The setup was as following: everyone could enlist for a 30-hour hackathon. We provided the food and drink. We aimed for 30 participants, which we divided into 6 groups. These groups would focus on different topics, such as crime, health, transport, safety, ageing and power. For us, the three main objectives for this event were as follows:

Find stories::
  For us, data journalism is something new and unknown. The only way we can prove its use, is through well crafted stories. We planned to produce at least three data stories.

Connect people::
  We, the journalists, don't know how data journalism is done and we don't pretend to. By putting journalists, students and programmers in one room for 30 hours, we want them to share knowledge and insights.

Host a social event::
  Newspapers don't organise a lot of social events, let alone hackathons. We wanted to experience how such an event can yield results. In fact, the event could have been tense: 30 hours with strangers, lots of jargon, bashing your head against basic questions, working out of your comfort zone. By making it a social event (remember the pizza and energy drink?) we wanted to create an environment in which journalists and programmers could feel comfortable and collaborate effectively.

Before the event, TC Tubantia had an interview with the widow of a policeman who had written a book on her husband's working years. She also had a document with all registered murders in the eastern part of the Netherlands, maintained by her husband since 1945. Normally, we would publish this document on our website. This time, we made a http://bit.ly/tableau-dashboard[dashboard using the Tableau software]. We also http://bit.ly/regiohack-blog[blogged] about how this came together on our RegioHack site.

During the hackathon, one project group came up with the subject of development of schools and the ageing of our region. http://bit.ly/tableau-workbook[By making a visualization of future projections], we understood which cities would get in trouble after a few years of decline in enrolments. With this insight, we made an article on how this would affect schools in our region.

We also started a very ambitious project, called De Tweehonderd van Twente (in English, The Two Hundred of Twente) to determine who had the most power in our region and build a database of the most influential people. Through a Google-ish calculation -- who has the most ties with powerful organizations -- a list of influential people will be composed. This could lead to a series of articles, but it's also a powerful tool for journalists. Who has connections with who? You can ask questions to this database and use it in our daily routine. Also, this database has cultural value. Artists already asked if they could use this database when finished to make interactive art installations.

[[FIG0213]]
.New communities around data journalism (photo by Jerry Vermanen)
image::figs/incoming/02-YY.jpg[width="4.8in"]

After RegioHack, we noticed that journalists considered data journalism as a viable addition to traditional journalism. My colleagues continued to use and build on the techniques learned on the day to create more ambitious and technical projects such as a database of the administrative costs of housing. With this data, I made http://bit.ly/stentor-map[an interactive map in Fusion Tables]. We asked our readers to play around with the data and crowdsourced results (http://bit.ly/scratchbook-crowdsourcing[here], for example). After a lot of questions on how we made a map in Fusion Tables, I also recorded a http://bit.ly/vermanen-video[video tutorial].

What did we learn? We learned a lot, but we also came along a lot of obstacles. We recognized these four:

Where to begin: question or data?::
  Almost all projects stalled when searching for information. Most of the time, they began with a journalistic question. But then? What data is available? Where can you find it? And when you find this data, can you answer your question with it. Journalists usually know where they can find information when doing research for an article. With data journalism, most journalists don't know what information is available.

Little technical knowledge::
  Data journalism is quite a technical discipline. Sometimes you have to scrape, other times you'll have to do some programming to visualize your results. For excellent data journalism, you'll need two aspects: the journalistic insight of an experienced journalist and the technical know-how of a digital all-rounder. During RegioHack, this was not a common presence.

Is it news?::
  Participants mostly used one dataset to discover news, instead of searching interconnections between different sources. The reason for this: you need some statistical knowledge to to verify news from data journalism.

What's the routine?::
  What above all comes down to, is that there's no routine. The participants have some skills under their belt, but don't know how and when to use them. One journalist compared it with baking a cake. ‘We have all the ingredients: flour, eggs, milk, etcetera. Now we throw it all in a bag, shake it and hope a cake comes out of it.' Indeed, we have all the ingredients, but don't know what the recipe is.

What now? Our first experiences with data journalism could help other journalists or programmers aspiring the same field of work and we are working to produce a report.

Also, we are considering how to continue RegioHack in a hackathon form. We found it fun, educational and productive and a great introduction to data journalism.

But for data journalism to work, we have to integrate it in the newsroom. Journalists have to think in data, in addition to quotes, press releases, council meetings and so on. By doing RegioHack, we proved to our audience that data journalism isn't just hype. We can write better informed and more distinctive articles, while presenting our readers different articles in print and online.

&mdash; _Jerry Vermanen, NU.nl_

=== Following the Money: Data Journalism and Cross-Border Collaboration ===

[[FIG0214]]
._The Investigative Dashboard_ (OCCRP)
image::figs/incoming/02-RR.png[width="4.8in"]

Investigative journalists and citizens interested in uncovering organised crime and corruption that affect the lives of billions worldwide gain, with each passing day, unprecedented access to information. Huge volumes of data are made available online by governments and other organizations and it seems that much needed information is more and more in everyone's grasp. However, at the same time, corrupt officials in governments and organised crime groups are doing their best to conceal information in order to hide their misdeeds. They make efforts to keep people in the dark while conducting ugly deals that cause disruptions at all society levels and lead to conflict, famine or other types of crisis.

It is the duty of investigative journalists to expose such wrongdoings and by doing so to disable corrupt and criminal mechanisms.

There are three main guidelines that, if followed, can lead to good, thorough journalism when investigating major acts of corruption and crime even in the most austere of environments.

Think Outside Your Country::
  In many instances it is much easier to get information from abroad than from within the country where the investigative journalist operates. Information gathered from abroad via foreign information databases or by using other countries' access to information laws might be just what is needed to put the investigative puzzle together. On top of that, criminals and corrupt officials don't keep their money in the place they stolen it from. They would rather deposit it in foreign banks or they would rather invest in other countries. Crime is global. Databases that assist the investigative journalist in tracking the money worldwide can be found in many places on the Internet. For example, the http://www.investigativedashboard.org/category/wwd/[Investigative Dashboard] enables journalists to follow the money across borders.

Make Use of the Existing Investigative Journalism Networks::
  Investigative journalists all over the world are grouped in
organzations such as http://www.reportingproject.net/[The Organized Crime and Corruption Reporting
Project], http://www.fairreporters.org/[The African Forum for Investigative
Reporting], http://arij.net/[The Arab Reporters for Investigative
Journalism], http://www.gijn.org/[The Global investigative Journalism Network]. Journalists can also make use of professional journalism platforms such as IJNet where global journalism related information is exchanged on daily basis. Many of the reporters grouped in networks work on similar issues and confront similar situations so it makes a lot of sense to exchange information and methods. Emailing lists or social network groups are attached to these networks so it is quite easy to get in touch with fellow journalists and to ask for information or advice. Investigative stories ideas can also be gathered from such forums and emailing lists.

Make Use of Technology and Collaborate with Hackers::
  Software helps investigative journalists access and process information. Various types of software assist the investigator in cutting through the noise, in digging and making sense of large volumes of data and in finding the right documents needed to break the story. There are many ready-made software programs that can be used as tools for analyzing, gathering or interpreting information and, more important, investigative journalists need to be aware that there are scores of computer programmers ready to help if asked. These programmers or hackers know how to obtain and handle information and they can assist a big deal with the investigative effort. These programmers, some of them members of global open data movements, can become invaluable allies in the fight against crime and corruption. They can assist journalists in gathering and in analyzing information.

A good example of an interface between programmers and citizens is https://scraperwiki.com/[ScraperWiki], a site where journalists
can ask programmers for help with extracting data from websites. Investigative Dashboard http://bit.ly/dashboard-resources[maintains a list] of ready-made tools that could help to journalist gather, shape and analyze data can be found here.

The usefulness of the above-mentioned guidelines has been visible in many instances. One good example is the work of Khadija Ismayilova, a very experienced Azeri investigative reporter who works in a very austere environment when it comes to information access. Ms. Ismayilova has to overcome obstacles on daily basis in order to offer the Azeri public good and reliable information. In June of 2011, Khadija Ismayilova, an investigative reporter with Radio Free Europe/Radio Liberty's (RFE/RL) Baku based office reported that the daughters of the Azeri president, Ilham Aliyev, secretly run http://bit.ly/rferl-azerfon[a fast rising telecom company Azerfon] through offshore companies based in Panama. The company boasts nearly 1.7 million subscribers, covers 80 percent of the country's territory and was, at the time, Azerbaijan's only provider of 3G services. Ismayilova spent three years trying to find out who were the owners of the telecom company but the government refused to disclose shareholder information and lied numerous times about the company's ownership. They even claimed that the company was owned by the German based Siemens AG, a claim that has been flatly denied by the German corporation. The Azeri reporter managed to find out that Azerfon was owned by a few Panama based private companies and this seemed to be a dead end to her reporting until help from outside was employed. In early 2011, Ms. Ismayilova learned, through the Investigative Dashboard that Panama based companies can be tracked down through http://ohuiginn.net/panama/[an application] developed by programmer and activist Dan O'Huiginn. It was then when she finally managed to uncover the fact that the president's two daughters were involved with the telecom company through the Panama based entities.

In fact, O'Huiginn created a tool that helped journalists from all over the world to report on corruption as Panama, a very well known offshore haven, has been widely used by corrupt officials from all over as a place to hide stolen money: from cronies of the former Egyptian president, Hosni Mubarak to dirty officials in the Balkans or in Latin America. What the programmer-activist has done is called web scraping; a method that allows the extraction and reshaping of information so that it can be used by investigators. O'Huiginn scraped the http://www.registro-publico.gob.pa/[Panama registry of companies] because this registry, although open, only allowed searches if the investigative reporter knew the name of the commercial company he or she was looking for. This limited the possibility to investigate as usually reporters look for names of persons in order to track down their assets. The programmer extracted the data and created a new web site where names-based searches are also possible. The new web site allowed investigative reporters in many countries to fish for information, to run names of officials in governments and Parliaments and to check if they secretly owned corporations in Panama just as the family of the Azerbaijan president.

There are other advantages to using the guidelines highlighted above, besides better access to information. One of them has to do with minimising harm and insuring better protection for investigative reporters who work in hostile environments. This is due to the fact that when working in a network the journalist is not alone, the investigative reporter works with colleagues in other countries so it is harder for criminals to pinpoint whom they think is guilty for their wrongdoings being exposed. As a result, retaliation by governments and corrupt officials is much harder to achieve.

Another thing to be kept in mind is that information that doesn't seem very valuable in a geographical area might be crucially important in another. The exchange of information over investigative network can lead to breaking very important stories. For example, the information that a Romanian was caught in Colombia with 1 kilogram of cocaine is most probably not front-page news in Bogota but could be very important to the Romanian public if a local reporter manages to find out that the person who was caught with the narcotics is working for the government in Bucharest.

Efficient investigative reporting is the result of cooperation between investigative journalists, programmers and others who want to use data to contribute to create a cleaner, fairer and more just global society.

&mdash; _Paul Radu, Organized Crime and Corruption Reporting Project_ 

=== Our Stories Come As Code 

[[FIG0215]]
._Airport noise map_ (Taz.de)
image::figs/incoming/02-TT.png[width="4.8in"]

http://www.opendatacity.de/[OpenDataCity] was founded towards the end of 2010. There was pretty much nothing that you could call data journalism happening in Germany at this time.

Why did we do this? Many times we heard people working for newspapers and broadcasters say: ``No, we are not ready to start a dedicated data journalism unit in our newsroom. But we would be happy to outsource this to someone else.''

As far as we know, we are the only company specialising exclusively in data journalism in Germany. There are currently three of us: two of us with a journalism background and one with a deep understanding of code and visualization. We work with a handful of freelance hackers, designers and journalists.

In the last twelve months we have undertaken four data journalism projects with newspapers, and have offered training and consultancy to media workers, scientists and journalism schools. The first app we did was an http://bit.ly/taz-airport-noise[interactive tool on airport noise] around the the newly built airport in Berlin with TAZ. Our next notable project was an http://bit.ly/zeit-telephone[application about data retention] of the mobile phone usage of a German politician with ZEIT Online. For this we won a http://bit.ly/grimme-award[Grimme Online Award] and a Lead Award in Germany, and an Online Journalism Award by the http://bit.ly/online-news-award[Online Journalism Association] in the US. At the time of writing, we are have several projects in the pipeline, ranging from simpler interactive infographics up to designing and developing a kind of data journalism middleware.

Of course, winning prizes helps to built a reputation. But when we talk to the publishers, who have to approve the projects, our argument for investing into data journalism is not about winning prizes. Rather it is about getting attention over a longer period in a sustainable way. Building things for their long term impact, not for the scoop, which often is forgotten after a few days.

Here are three arguments which we have used to encourage publishers to undertake longer term projects:

Data projects don't date::
  Depending on their design, new material can be added to data journalism apps. And they are not just for the users, but can be used internally for reporting and analysis. If you're worried that this means that your competitors will also benefit from your investment, you could keep some features or some data for internal use only.
You can build on your past work::
  When undertaking a data project, you will often create bits of code which can be reused or updated. The next project might take half the time, because you know much better what to do (and what not to) and you have bits and pieces you can build on.
Data journalism pays for itself::
  Data driven projects are cheaper than traditional marketing campaigns. Online news outlets will often invest in things like Search Engine Optimization (SEO) and Search Engine Marketing (SEM). A executed data project will normally generate a lot of clicks and buzz, and may go viral. Publishers will typically pay less for this then trying to generate the same attention by clicks and links through SEM.

Our work is not very different from other new media agencies: providing applications or services for news outlets. But maybe we differ in that we think of ourselves first and foremost as journalists. In our eyes the products we deliver are articles or stories, albeit ones which are provided not in words and pictures, audio or video, but in code. When we are talking about data journalism we have to talk about technology, software, devices and how to tell a story with them.

To give an example: we just finished working on an application, which pulls in realtime data via a scraper from the German railway website. Thus enabling us to develop an interactive http://zugmonitor.sueddeutsche.de/[Train Monitor] for Süddeutsche Zeitung, showing the delays of long-distance trains in realtime. The application data is updated every minute or so and we are providing an API for it, too. We started doing this several months ago, and have so far collected a huge dataset which grows every hour. By now it amounts to hundreds of thousands of rows of data. The project enables the user to explore this realtime data, and to do research in the archive of previous months. In the end the story we are telling will be significantly defined by the individual action of the users.

In traditional journalism, due to the linear character of written or broadcasted media, we have to think about a beginning, the end, the story arc and the length and angle of our piece. With data journalism things are different. There is a beginning, yes. People come to the website and get a first impression of the interface. But then they are on their own. Maybe they stay for a minute, or half an hour.

Our job as data journalists is to provide the framework or environment for this. As well as the coding and data management bits, we have to thing of clever ways to design experiences. The User Experience (UX) derives mostly from the (Graphical) User Interface (GUI). In the end this is the part which will make or break a project. You could have the best code working in the background handling an exiting dataset. But if the front-end sucks, nobody will care about it.

There is still a lot to learn about and to experiment with. But luckily there is the games industry, which has been innovating with respect to digital narratives, ecosystems and interfaces for several decades now. So when developing data journalism applications we should watch closely how game design works and how stories are told in games. Why are casual games like Tetris such fun? And what makes the open worlds of sandbox games like Grand Theft Auto or Skyrim rock?

We think that data journalism is here to stay. In a few years data journalism workflows will be quite naturally be embedded in newsrooms, because news websites will have to change. The amount of data that is publicly available will keep on increasing. But luckily new technologies will continue to enable us to find new ways of telling stories. Some of the stories will be driven by data and many of applications and services will have a journalistic character. The interesting question is: which strategy are newsrooms going to develop to foster this process? Are they going to build up teams of data journalists integrated into their newsroom? Will there be R&D departments, a bit like in-house startups? Or will parts of the work be outsourced to specialized companies? We are still right at the beginning and only time will tell.

&mdash; _Lorenz Matzat, OpenDataCity_

=== Kaas & Mulvad: Semi-finished Content for Stakeholder Groups 

[[FIG0216]]
.Stakeholder media companies (Fagblaget3F)
image::figs/incoming/02-MM.png[width="4.8in"]

Stakeholder media is an emerging sector, largely overlooked by media theorists, which has the potential to have a tremendous impact either through online networks or by providing content to news media. It can be defined as (usually online) media that is controlled by organzational or institutional stakeholders, and which is used to advance certain interests and communities. NGOs typically create such media; so do consumer groups, professional associations, labour unions, etc. The key limit on its ability to influence public opinion or other stakeholders is often that it lacks capacity to undertake discovery of important information, even more so than the downsized news media. Kaas og Mulvad, a for-profit Danish corporation, is one of the first investigative media enterprises that provides expert capacity to these stakeholder outlets.

The firm originated in 2007 as a spinoff of the non-profit Danish Institute for Computer-Assisted Reporting (Dicar), which sold investigative reports to media and trained journalists in data analysis. Its founders, Tommy Kaas and Nils Mulvad, were previously reporters in the news industry. Their new firm offers what they call ``data plus journalistic insight'' (content which remains semi-finished, requiring further editing or rewriting) mainly to stakeholder media, which finalise the content into news releases or stories and distribute it through both news media and their own outlets (such as websites). Direct clients include government institutions, PR firms, labour unions and NGOs such as EU Transparency and the World Wildlife Fund. The NGO work includes monitoring farm and fishery subsidies, and regular updates on EU lobbyist activities generated through ``scraping'' of pertinent websites. Indirect clients include foundations that fund NGO projects. The firm also works with the news industry; a tabloid newspaper purchased their celebrity monitoring service, for example.

Data journalism projects in their portfolio include:

http://bit.ly/3F-unemployment[Unemployment Map for 3F]:: 
  A data visualization with key figures about unemployment in Denmark undertaken for 3F, which is the union for unskilled labour in Denmark.

http://bit.ly/3F-living[Living Conditions for 3F]::
  Another project for 3F shows how different living conditions are in different parts of Denmark. The map shows 24 different indicators for living conditions.

http://bit.ly/3F-debt-index[Debt for ``Ugebrevet A4'']::
  A project that calculates a ``debt index'' and visualizes the differences in private economy.

http://bit.ly/3F-dangerous-facilities[Dangerous Facilities in Denmark]::
  A project which maps and analyzes the proximity of dangerous facilities to kindergartens and other day care institutions, undertaken for ``Børn&Unge'', a magazine published by BUPL, the Danish Union of Early Childhood and Youth Educators.

http://data.vestas.com[Corporate Responsibility Data for Vestas]::
  Data visualization on five areas of CR-data for the Danish wind turbine company, Vestas, with auto-generated text. Automatically updated on a quarterly basis with 400 webpages from world scale data down to the single production unit.

http://xpoint.experian.dk/navnekort[Name Map for Experian]::
  Type in your last name and look at the distribution of this name around different geographical areas in Denmark.

http://ekstrabladet.dk/kup/fodevarer[Smiley Map for Ekstra Bladet]::
  Every day we extract all the bad food inspections and map all the latest on a map for the Danish tabloid Ekstra Bladet (see half way down the website for the map).

Kass og Mulvad are not the first journalists to work with stakeholder media. Greenpeace, for example, routinely engages journalists as collaborators for its reports. But we know of no other firm whose offerings to stakeholder media are data-driven; it is much more typical for journalists to work with NGOs as reporters, editors or writers. The current focus in computer-assisted news media is on search and discovery (think of Wikileaks); here again Kaas og Mulvad innovate, by focusing on data analysis. Their approach requires not only programming skills, but also understanding of what kind of information can make a story with impact. It can safely be said that anyone wishing to imitate their service would probably have to acquire those two skill sets through partnership, because individuals rarely possess both.

==== Processes: Innovative IT plus analysis

The firm undertakes about 100 projects per year, ranging in duration from a few hours to a few months. It also continuously invests in projects that expand its capacity and offerings. The celebrity monitoring service was one such experiment. Another involved scraping the Internet for news of home foreclosures and creating maps of the events. The partners say that their first criteria for projects is whether they enjoy the work and learn from it; markets are sought after a new service is defined. They make it clear that in the news industry, they found it difficult to develop new methods and new business.

Comments Mulvad:

____
We have no editors or bosses to decide which projects we can do, which software or hardware we can buy. We can buy the tools according to project needs, like the best solutions for text scraping and mining. Our goal is to be cutting edge in these areas. We try to get customers who are willing to pay, or if the project is fun we do it for a lower charge.
____

==== Value created: Personal and firm brands and revenue

Turnover in 2009 was approximately 2.5 million Danish kroner, or €336,000. The firm also sustains the partners' reputations as cutting edge journalists, which maintains demand for their teaching and speaking services. Their public appearances, in turn, support the firm's brand.

==== Key insights of this example

  * The news industry's crisis of declining capacity is also a crisis of under- utilisation of capacity. Kaas and Mulvad had to leave the news industry to do work they valued, and that pays. Nothing prevented a news organzation from capturing that value.
  * In at least some markets, there exists a profitable market for ``semi-finished'' content that can serve the interests of stakeholder groups.
  * However, this opportunity raises the issue of how much control journalists can exercise over the presentation and use of their work by third parties. We recall that this issue already exists within the news industry (where editors can impose changes on a journalist's product), and it has existed within other media industries (such as the film industry, where conflicts between directors and studios over ``final cuts'' are hardly rare). It is not a particular moral hazard of stakeholder media, but it will not disappear, either. More attention is needed to the ethics of this growing reality and market.
  * From a revenue standpoint, a single product or service is not enough. Successful watchdog enterprises would do better to take a portfolio approach, in which consulting, teaching, speaking and other services bring in extra revenue, support the watchdog brand.

&mdash; _Edited excerpt from Mark Lee Hunter and Luk N. Van Wassenhove, ``Disruptive News Technologies: Stakeholder Media and the Future of Watchdog Journalism Business Models''. INSEAD Working Paper, 2010_

=== Business Models for Data Journalism 

Amidst all the interest and hope regarding data-driven journalism there is one question that newsrooms are always curious about: what are the business models?

While we must be careful about making predictions, a look at the recent history and current state of the media industry can help to give us some insight. Today there are many news organizations who have gained by adopting new approaches.

Terms like `data journalism', and the newest buzzword ``data science'' may sound like they describe something new, but this is not strictly true. Instead these new labels are just ways of characterizing a shift that has been gaining strength over decades.

Many journalists seem to be unaware of the size of the revenue that is already generated through data collection, data analytics and visualization. This is the business of information refinement. With data tools and technologies it is increasingly possible to shed a light on highly complex issues, be this international finance, debt, demography, education and so on. The term `business intelligence' describes a variety of IT concepts aiming to provide a clear view on what is happening in commercial corporations. The big and profitable companies of our time, including McDonalds, Zara or H&M rely on constant data tracking to turn out a profit. And it works pretty well for them.

What is changing right now is that the tools developed for this space are now becoming available for other domains, including the media. And there are journalists who get it. Take Tableau, a company providing a suite of visualization tools. Or the ``Big Data'' movement, where technology companies use (often open source) software packages to dig through piles of data, extracting insights in milliseconds.

These technologies can now be applied to journalism. Teams at The Guardian and The New York Times are constantly pushing the boundaries in this emerging field. And what we are currently seeing is just the tip of the iceberg.

But how does this generate money for journalism? The big, worldwide market that is currently opening up is all about transformation of publicly available data into something our that we can process: making data visible and making it human. We want to be able to relate to the big numbers we hear every day in the news -- what the millions and billions mean for each of us.

There are a number of very profitable data-driven media companies, who have simply applied this principle earlier than others. They enjoy healthy growth rates and sometimes impressive profits. One example: Bloomberg. The company operates about 300,000 terminals and delivers financial data to it's users. If you are in the money business this is a power tool. Each terminal comes with a color coded keyboard and up to 30,000 options to look up, compare, analyze and help you to decide what to do next. This core business generates an estimated US $6.3 billion per year, at least this what http://nyti.ms/IQcRgY[a piece by the New York Times estimated in 2008]. As a result, Bloomberg has been hiring journalists left, right and centre, they bought the venerable but loss-making ``Business Week'' and so on.

Another example is the Canadian media conglomerate today known as Thomson Reuters. They started with one newspaper, bought up a number of well known titles in the UK, and then decided two decades ago to leave the newspaper business. Instead they have grown based on information services, aiming to provide a deeper perspective for clients in a number of industries. If you worry about how to make money with specialized information, the advice would be to just http://en.wikipedia.org/wiki/The_Thomson_Corporation[read about the company's history in Wikipedia].

And look at the Economist. The magazine has built an excellent, influential brand on its media side. At the same time the ``Economist Intelligence Unit'' is now more like a consultancy, reporting about relevant trends and forecasts for almost any country in the world. They are employing hundreds of journalists and claim to serve about 1.5 million customers worldwide.

And there are many niche data-driven services that could serve as inspiration: eMarketer in the US, providing comparisons, charts and advice for anybody interested in internet marketing. Stiftung Warentest in Germany, an institution looking into the quality of products and services. Statista, again from Germany, a start-up helping to visualize publicly available information.

Around the world there is currently a wave of startups in this sector, naturally covering a wide range of areas; for example, Timetric, which aims to ``reinvent business research'', OpenCorporates, Kasabi, Infochimps and Data Market. Many of these are arguably experiments, but together they can be taken as an important sign of change.

Then there is the public media, which in terms of data-driven journalism is a sleeping giant. In Germany €7.2 billion per year are flowing into this sector. Journalism is a special product: if done well it is not just about making money, but serves an important role in society. Once it is clear that data journalism can provide better, more reliable insights more easily, some of this money could be used for new jobs in newsrooms.

With data journalism, it is not just about being first but about being a trusted source of information. In this multi-channel world, attention can be generated in abundance, but _trust_ is an increasingly scarce resource. Data journalists can help to collate, synthesize and present diverse and often difficult sources of information in a way which gives their audience real insights into complex issues. Rather than just recycling press releases and retelling stories they've heard elsewhere, data journalists can give readers a clear, comprehensible and preferably customizable perspective with interactive graphics and direct access to primary sources. Not trivial, but certainly valuable.

So what is the best approach for aspiring data journalists to explore this field and convince management to support innovative projects?

The first step should be to look for immediate opportunities close to home: low hanging fruit. For example, you might already have collections of structured texts and data that you could use. A prime example of this is the ``Homicide database'' of the Los Angeles Times. Here data and visualizations are the core, not an afterthought. The editors collect all the crimes they find and only then write articles based on this. Over time, such collections are becoming better, deeper and more valuable.

This might not work the first time. But it will over time. One very hopeful indicator here is that the Texas Tribune and ProPublica, which are both arguably post-print media companies, reported that funding for their non-profit journalism organizations exceeded their goals much earlier than planned.

Becoming proficient in all things data -- whether as a generalist or as a specialist focused on one aspect of the data food chain -- provides a valuable perspective for people who believe in journalism. One well-known publisher in Germany recently said in an interview: ``There is this new group who call themselves data journalists. And they are not willing to work for peanuts anymore''.

&mdash; _Mirko Lorenz, Deutsche Welle_