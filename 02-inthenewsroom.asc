== 2. In The Newsroom ==

image::figs/incoming/02-00-cover.png[width=600]

===  The ABC's Data Journalism Play (Wendy Carlisle, Australian Broadcasting Corporation) ===

Now in its 70th year the Australian Broadcasting Corporation is Australia's national public broadcaster. Annual funding is around AUS$1bn which delivers seven radio networks, 60 local radio stations, three digital television services, a new international television service and an online platform to deliver this ever expanding offering of digital and user generated content. At last count there were in excess of 4,500 full time equivalent staff and nearly 70% of them make content.

We are a national broadcaster fiercely proud of our independence - because although funded by government - we are separated at arm's length through law. Our traditions are independent public service journalism. The ABC is regarded the most trusted news organisation in the country.

These are exciting times  and under a managing director - the former newspaper executive Mark Scott - content makers at the ABC have been encouraged to  as the corporate mantra puts it - be `agile'.

Of course that's easier said than done. 

But one initiative in recent times designed to encourage this has been  a competitive staff pitch for money to develop multi-platform projects.

This is how the ABC's first ever data journalism project was conceived.

Sometime early in 2010 I wandered into the pitch session to face with three senior `ideas' people with my proposal.

I'd been chewing it over for some time. Greedily lapping up the data journalism that the now legendary Guardian data journalism blog was offering, and that was just for starters.

It was my argument that no doubt within 5 years the ABC would have its own data journalism unit. It was inevitable, I opined. But the question was how are we going to get there, and whose going to start.

For those readers unfamiliar with the ABC  think of a vast bureaucracy built up over 70 years. Its primary offering was always radio and television. With the advent of online in the last decade this content offering unfurled into text, stills and a degree of interactivity previously unimagined. The web space was forcing the ABC to rethink how it cut the cake (money) and rethink what kind of cake it was baking (content).

It is of course a work in progress. 

But something else was happening with data journalism. Government 2.0 (which as we discovered is largely observed in the breach in Australia) was starting to offer new ways of telling stories that were hitherto buried in the zero's and dots.

All this I said to the folk during my pitch. I also said we needed to identify new skills sets, train journalists in new tools. We needed a project to hit play. 

And they gave me the money.

On the 24th of November 2011 the ABC's multiplatform project and ABC News Online went live with http://www.abc.net.au/news/specials/coal-seam-gas-by-the-numbers/promise/[`Coal Seam Gas by the Numbers'].

image::figs/incoming/02-01.png[width=600]

It was five pages of interactive maps, data visualisations and text. 

It wasn't exclusively data journalism - but a hybrid of journalisms that was born of the mix of people on the team and the story, which to put in context is raging as one of the hottest issues in Australia.  

The jewel was an interactive map showing coal seam gas wells and leases in Australia. Users could search by location and switch between modes to show leases or wells. By zooming in users could see who the explorer was, the status of the well and its drill date. Another map showed the location of coal Seam gas activity compared to the location of groundwater systems in Australia. 

image::figs/incoming/02-02.png[width=600]

We had data visualisations which specifically addressed this issue of waste salt and water production that would be produced depending on the scenario that emerged.   

Another section of the project investigated the release of chemicals into a local river system

*Our team*

  * A web developer and designer. 
  * A lead journalist  
  * A part time researcher with expertise in data extraction, excel spread sheets and data cleaning
  * A part time junior journalist.
  * A consultant executive producer
  * A academic consultant with expertise in data mining, graphic visualisation and advanced research skills.
  * The services of a project manager and the administrative assistance of the ABC's multi-platform unit.
  * Importantly we also had a reference group of journalists and others whom we consulted on a needs basis.

*Where did we get the data from?*

The data for the interactive maps were scrapped from shapefiles (a common kind of file for geospatial data) downloaded from government websites. 

Other data on salt and water were taken from a variety of reports.

The data on chemical releases was taken from Environmental permits issued by the government. 

*What did we learn?*

`Coal Seam Gas by the Numbers' was an ambitious in content and scale.  Uppermost in my mind was what did we learn and how might we do it differently next time?

The data journalism project brought a lot of people into the room who do not normally meet at the ABC. In lay terms - the hacks and the hackers. Many of us did not speak the same language or even appreciate what the other does.  Data journalism is disruptive!   

The practical things:

  * Co-location of the team is vital. Our developer and designer were off-site and came in for meetings. This is definitely not optimal! Place in the same room as the journalists. 
  * Our consultant EP was also on another level of the building. We needed to be much closer, just for the drop-by factor
  * Choose a story that is solely data driven.

**The big picture: some ideas**

  * Big media organisations need to engage in capacity building to meet the challenges of data journalism. My hunch is there are a lot of geeks and hackers hiding in media technical departments desperate to get out. So -we need `hack and hacker meets' workshops where the secret geeks, younger journalists, web developers and designers come out to play with more experienced journalists for skill sharing and mentoring. Task: download this data set and go for it! 
  * Ipso facto Data journalism is interdisciplinary. Data journalism teams are made of people who would not in the past have worked together. The digital space has blurred the boundaries.
  * We live in a fractured, distrustful body politic.  The digital space makes everyone a content maker: we can all be journalists now, right?  Wrong. Journalists need to reassert themselves as ethical, trustworthy, honest story tellers. Data journalism offers the opportunity to turn  the signal to noise ratio into something we can all understand. Journalists are going to need to be literate in data journalism tools to
  * Data journalism is still all about story telling.
  * Increasingly the journalists of tomorrow will be data journalists.
  * Australia is behind Europe and the United States in data journalism. Why? That's another discussion.

=== How the News Apps Team at Chicago Tribune Works (Brian Boyer, Chicago Tribune) ===

image::figs/incoming/02-00.jpg[width=600]

The news applications team at the Chicago Tribune is a band of happy hackers embedded in the newsroom. We work closely with editors and reporters to help: (1) research and report stories, (2) illustrate stories online and (3) build evergreen web resources for the fine people of Chicagoland.

It's important that we sit in the newsroom. We usually find work via face-to-face conversations with reporters. They know that we're happy to help write a screen scraper for a crummy government website, tear up a stack of PDFs, or otherwise turn non-data into something you can analyse. It's sort of our team's loss leader - this way we find out about potential data projects at their outset.

Unlike many teams in this field, our team was founded by technologists for whom journalism was a career change. Some of us acquired a masters degree in journalism after several years coding for business purposes, and others were borrowed from the open government community.

We work in an agile fashion. To make sure we're always in sync, every morning begins with a 5-minute stand up meeting. We frequently program in pairs - two developers at one keyboard are often more productive than two developers at two keyboards. Most projects don't take more than a week to produce, but on longer projects we work in week-long iterations, and show our work to stakeholders - reporters and editors usually - every week. ``Fail fast'' is the mantra. If you're doing it wrong, you need to know as soon as possible, especially when you're coding on a deadline!

There's a tremendous upside to hacking iteratively, on a deadline: We're always updating our toolkit. Every week we crank out an app or two, then, unlike normal software shops, we can put it to the back of our mind and move on to the next project. It's a joy we share with the reporters, every week we learn something new.

All app ideas come from the reporters and editors in the newsroom. This, I believe, sets us apart from apps teams in other newsrooms, who frequently spawn their own ideas. We've built strong personal and professional relationships in the newsroom, and folks know that when they have data, they come to us.

Much of our work in the newsroom is reporter support. We help reporters dig through data, turn PDFs back into spreadsheets, screen-scrape websites, etc. It's a service that we like to provide because it gets us in early on the data work that's happening in the newsroom. Some of that work becomes a news application - a map, table, or sometimes a larger-scale website.

Before, we linked to the app from the written story, which didn't result in much traffic. These days apps run near the top of our website, and the app links through to the story, which works nicely for both the app and the story. There is a http://www.chicagotribune.com/news/data/[section of the website for our work], but it's not well-trafficked. But that's not surprising. ``Hey, today I want some data'' isn't a very big use case.

We love page views, and we love the accolades of our peers, but that's weak sauce. The motivation should always be impact - on people's lives, on the law, on holding politicians to account, and so on. The written piece will speak to the trend and humanise it with a few anecdotes. But what's the reader to do when they've finished the story? Is their family safe? Are their children being educated properly? Our work sings when it helps a reader find his or her _own_ story in the data. Examples of impactful, personalised work that we've done include our http://nursinghomes.apps.chicagotribune.com/[Nursing Home Safety Reports] and http://schools.chicagotribune.com/[School Report Card] apps.

=== Behind the Scenes at the Guardian Datablog (Simon Rogers, Guardian) ===

image::figs/incoming/02-ZZ.jpg[width=600]

When we launched the Datablog, we had no idea who would be interested in raw data, statistics and visualisations. As someone pretty senior in my office said: ``why would anyone want that?''.

The http://www.guardian.co.uk/datablog[Guardian Datablog] – which I edit - was to be a small blog offering the full datasets behind our news stories. Now it consists of a front page (guardian.co.uk/data); searches of world government and global development data; data visualisations by from around the web and Guardian graphic artists, and tools for exploring public spending data. Every day, we use Google spreadsheets to share the full data behind our work; we visualise and analyse that data - and use it to provides stories for the newspaper and the site.

As a news editor and journalist working with graphics, it was a logical extension of work I was already doing, accumulating new datasets and wrangling with them to try to make sense of the news stories of the day.

The question I was asked has been answered for us. It has been an incredible few years for public data. Obama opened up the US government’s data faults as his first legislative act, followed by government data sites around the world – Australia, New Zealand, the British government’s Data.gov.uk

We've had the MPs expenses scandal - Britain’s most unexpected piece of data journalism – the resulting fallout has meant Westminster is now committed to releasing huge amounts of data every year.

We had a general election where each of the main political parties was committed to data transparency, opening our own data vaults to the world. We’ve had newspapers devoting valuable column inches to the release of the Treasury’s COINS database.

At the same time, as the web pumps out more and more data, readers from around the world are more interested in the raw facts behind the news than ever before. When we launched the Datablog, we thought the audiences would be developers building applications. In fact, it's people wanting to know more about carbon emissions or Eastern European immigration or the breakdown of deaths in Afghanistan – or even the number of times the Beatles used the word ``love'' in their songs (613).

Gradually, the Datablog's work has reflected and added to the stories we faced. We crowdsourced 458,000 documents relating to MPs' expenses and we analysed the detailed data of which MPs had claimed what. We helped our users explore detailed Treasury spending databases and published the data behind the news.

But the game-changer for data journalism happened in spring 2010, beginning with one spreadsheet: 92,201 rows of data, each one containing a detailed breakdown of a military event in Afghanistan. This was the WikiLeaks war logs. Part one, that is. There were to be two more episodes to follow: Iraq and the cables. The official term for the first two parts was SIGACTS: the US military Significant Actions Database.

News organisations are all about geography - and proximity to the news desk. If you're close, it's easy to suggest stories and become part of the process; conversely out of sight is literally out of mind. Before Wikileaks, we were sat on a different floor, with graphics. Since Wikileaks, we have sat on the same floor, next to the newsdesk. It means that it's easier for us to suggest ideas to the desk, and for reporters across the newsroom to think of us to help with stories.

It's not that long ago journalists were the gatekeepers to official data. We would write stories about the numbers and release them to a grateful public, who - by and large - were not interested in the raw statistics. The idea of us allowing our raw information into our newspapers was anathema.

Now that dynamic has changed beyond recognition. Our role is becoming interpreters, helping people understand the data - and even just publishing it because it's interesting in itself.

But numbers without analysis are just numbers – which is where we fit in. When Britain’s prime minister claims the riots in August 2011 were not about poverty, we were able to map the addresses of the rioters with poverty indicators to show the truth behind the claim.

Behind all our data journalism stories is a process. It’s changing all the time as use new tools and techniques. Some people say the answer is to become a sort of super hacker, write code and immerse yourself in SQL. You can decide to take that approach. But a lot of the work we do is just in Excel.

Firstly, we locate the data or receive it from a variety of sources, from breaking news stories, government data, journalists' research and so on. We then start looking at what we can do with the data - do we need to mash it up with another dataset? How can we show changes over time? Those spreadsheets often have to be seriously tidied up - all those extraneous columns and weirdly merged cells really don't help. And that's assuming it's not a PDF, the worst format for data known to humankind.

Often official data comes with the official codes added in – each school, hospital, constituency and local authority has a unique identifier code.

Countries have them too (the UK's code is GB, for instance). They're useful because you may want to start mashing datasets together and it's amazing how many different spellings and word arrangements can get in the way of that. There's Burma and Myanmar, for instance, or Fayette County in the US – there are 11 in states from Georgia to West Virginia. Codes allow us to compare like with like.

At the end of that process is the output - will it be a story or a graphic or a visualisation, and what tools will we use? Our top tools are the free ones that we can produce something quickly with – with the more sophisticated graphics produced by our dev team.

Which means we commonly use Google charts for small line graphs and pies, or Google Fusion Tables to create maps quickly and easily.

It may seem new, but really it's not.

In the very first issue of the Manchester Guardian, Saturday 5 May, 1821, the news was on the back page, like all papers of the day. First item on the front page was an ad for a missing Labrador.

And, amid the stories and poetry excerpts, a third of that back page is taken up with, well, facts. A comprehensive table of the costs of schools in the area never before ``laid before the public'', writes ``NH''.

NH wanted his data published because otherwise the facts would be left to untrained clergymen to report. His motivation was that: ``Such information as it contains is valuable; because, without knowing the extent to which education … prevails, the best opinions which can be formed of the condition and future progress of society must be necessarily incorrect.'' In other words, if the people don't know what's going on, how can society get any better?

I can’t think of a better rationale now for what we’re trying to do. Now what once was a back page story can now make front page news.

=== Measuring the Impact of Data Journalism (Texas Tribune) ===

Waiting for text.

=== Data Journalism at the Zeit Online (Sascha Venohr, Zeit Online) ===

image::figs/incoming/02-03.png[width=600]

The http://opendata.zeit.de/pisa-wohlstands-vergleich/visualisierung.php#/en/DEU-OECD[PISA based Wealth Comparison] project is an interactive visualisation that enables comparison of standards of living in different countries. The interactive uses data from the OECD's comprehensive world education ranking report, http://en.wikipedia.org/wiki/Programme_for_International_Student_Assessment[PISA 2009], published in December 2010. The report is based on a questionnaire which asks fifteen-year-old pupils about their living situation at home.

The idea was to analyse and visualise this data to provide a unique way of comparing standards of living in different countries.

First of all our in-house editorial team decided which facts seemed to be useful to make living standards comparable and should be visualised, including:

  * Wealth (number of owned TVs, cars and available bathrooms at home)
  * Family-situation (are there grandparents living with the family together, percentage share of families with only one child, unemployment of parents and mother's job status)
  * Access to knowledge sources (internet at home, frequency of using e-mail and quantity of owned books)
  * Three additional indicators on the level of development of each country.

With the help of the internal design team these facts were translated into self-explanatory icons. A front end design was built to make comparison between the different countries like in a card-game possible.

Next we contacted people from the German http://opendata-network.org/[German Open Data Network] to find developers who could help with the project. This community of highly motivated people suggested Gregor Aisch, a very talented information designer, to code the application that would make our dreams come true (without using flash - which was very important to us!). Gregor created a very high quality and interactive visualisation with a beautiful bubble-style, based on the http://raphaeljs.com/[Raphaël-Javascript Library].

The result of our hand in hand work was a very successful interactive which got a lot of traffic. It is easy to compare any two countries, which makes it useful as a reference tool. This means that we can re-use it in our daily editorial work. For example if we are covering something related to the living situation in Indonesia, we can quickly and easily embed a graphic http://opendata.zeit.de/pisa-wohlstands-vergleich/visualisierung.php#/en/DEU-IDN[comparing the living situation in Indonesia and Germany]. The know-how transferred to our in house-team was a great investment for future projects.

=== How to Hire a Hacker (Lucy Chambers, Open Knowledge Foundation) ===

One of the things that I am regularly asked by journalists is 'how do I get a coder to help me with my project?'. Don't be deceived into thinking this is a one-way process; civic-minded hackers and data-wranglers are often just as keen to get in touch with journalists. 

Journalists are power-users of data driven tools and services. From the perspective of developers: journalists think outside the box to use data tools in contexts developers haven't always considered before (feedback is invaluable!) they also help to build context and buzz around projects and help to make them relevant. It is a symbiotic relationship.

Fortunately, this means that whether you are looking to hire a hacker or looking for possible collaborations on a shoestring budget, there will more than likely be someone out there who is interested in helping you.

So how do you find them? Says Aron Pilhofer from the New York Times:


[quote]
____
You may find that your organisation already has people with all the skills you need, but they are not necessarily already in your newsroom. Wander around, visit the technology and IT departments and you are likely to strike gold. It is also important to appreciate coder culture, come across someone who has a computer that looks like this...

image::figs/incoming/02-04.jpg[width=600]

...then you are probably onto a winner.

____

Here are a few more ideas:

  * *Post on job websites*. Identify and post to websites aimed at developers who work in different programming languages. For example, the http://www.python.org/community/jobs/[Python Job Board].
  * *Contact relevant mailing lists*. For example, the http://www.ire.org/resource-center/listservs/subscribe-nicar-l/[NICAR-L] and http://lists.okfn.org/mailman/listinfo/data-driven-journalism[Data Driven Journalism] mailing lists.
  * *Contact relevant organisations*. For example, if you want to clean up or scrape data from the web, you could contact an organisation such as https://scraperwiki.com/[Scraperwiki], who have a great address book of trusted and willing coders.
  * *Join relevant groups/networks*. Look out for initiatives such as http://hackshackers.com/[Hacks/Hackers] which bring journalists and techies together. Hacks/Hackers groups are now springing up all around the world. You could also try posting something to their http://hackshackers.com/blog/2012/02/25/subscribe-to-hackshackers-jobs-newsletter/[jobs newsletter].
  * *Local interest communities*. You could try doing a quick search for an area of expertise in your area (e.g. `javascript' + `london'). Sites such as Meetup.com can also be a great place to start.
  * *Hackathons and competitions*. Whether or not there is prize money available: app and visualisation competitions and development days are often fruitful ground for collaboration and making connections.
  * *Ask a nerd!* Nerds hang around with more nerds. Word of mouth is always a good way to find good people to work with.

Once you've found a hacker, how do you know if they are any good? We asked Alastair Dant, the Guardian's Lead Interactive Technologist, for his views on how to spot a good one:

  * *They code the full stack*. When dealing with deadlines, it's better to be a jack of all trades than a master of one. News apps require data wrangling, dynamic graphics and derring-do.
  * *They see the whole picture*. Holistic thinking favours narrative value over technical detail. I'd rather hear one note played with feeling than unceasing virtuosity in obscure scales. Find out how happy someone is to work alongside a designer.
  * *They tell a good story*. Narrative presentation requires arranging things in space and time. Find out what project they're most proud of and ask them to walk you through how it was built – this will reveal as much about their ability to communicate as their technical understanding.
  * *They talk things through*. Building things fast requires mixed teams working towards common goals. Each participant should respect their fellows and be willing to negotiate. Unforeseen obstacles often require rapid re-planning and collective compromise.
  * *They teach themselves*. Technology moves fast. It's a struggle to keep up with. Having met good developers from all sorts of backgrounds, the most common trait is a willingness to learn new stuff on demand.

We also asked Brian Boyer from the Chicago Tribune for his views on how to find your dream developer:

____
The productivity difference between a good and a great developer is not linear, it's exponential. Hiring well is extremely important. Unfortunately, hiring well is also very difficult. It's hard enough to vet candidates if you are not an experienced technical manager. Add to that the salaries that news organisations can afford to pay, and you've got quite a challenge.

At Tribune, we recruit with two angles: an emotional appeal and a technical appeal. The emotional appeal is this: Journalism is essential to a functioning democracy. Work here and you can change the world. Technically, we promote how much you'll learn. Our projects are small, fast and iterative. Every project is a new set of tools, a new language, a new topic (fire safety, the pension system) that you must learn. The newsroom is a crucible. I've never managed a team that has learned so much, so fast, as our team.

As for where to look, we've had great luck finding great hackers in the open government community. The Sunlight Labs mailing list is where do-gooder nerds with shitty day jobs hang out at night. Another potential resource is Code for America. Every year, a group of fellows emerges from CfA, looking for their next big project. And as a bonus, CfA has a rigorous interview process - they've already done the vetting for you. Nowadays, programming-interested journalists are also emerging from journalism schools. They're green, but they've got tons of potential.

Lastly, it's not enough to just hire developers. You need technical management. A lone-gun developer (especially fresh from journalism school, with no industry experience) is going to make many bad decisions. Even the best programmer, when left to her own devices, will choose technically interesting work over doing what's most important to your audience. Call this hire a news applications editor, a project manager, whatever. Just like writers, programmers need editors, mentorship and somebody to wrangle them towards making software on deadline.
____

=== Harnessing External Expertise Through Hackthons (Jerry Vermanen, NU.nl) ===

image::figs/incoming/02-XY.jpg[width=600]

In March 2010, Utrecht based digital culture organisation SETUP put on an event called http://setup.nl/content/hacking-journalism[`Hacking Journalism']. The event was organised to encourage greater collaboration between developers and journalists.

`We organize hackathons to make cool applications, but we can't recognise interesting stories in data. What we build has no social relevance', said the programmers. `We recognize the importance of data journalism, but we don't have all the technical skills to build the things we want', said the journalists.

Working for a regional newspaper, there was no money or incentive to hire a programmer for the newsroom. Data journalism was still an unknown quantity for Dutch newspapers at that time.

The hackathon model was perfect. A relaxed environment for collaboration, with plenty of pizza and energy drinks. http://www.regiohack.nl/[RegioHack] was a hackathon organised by my employer, the regional newspaper http://www.destentor.nl/[De Stentor], our sister publication http://www.tctubantia.nl/[TC Tubantia] and http://saxion.nl/[Saxion Hogescholen Enschede], who provided the location for the event.

The setup was as following: everyone could enlist for a 30-hour hackathon. We provided the food and drink. We aimed for 30 participants, which we divided into 6 groups. These groups would focus on different topics, such as crime, health, transport, safety, ageing and power. For us, the three main objectives for this event were as follows:

*1. Find stories.* For us, data journalism is something new and unknown. The only way we can prove its use, is through well crafted stories. We planned to produce at least three data stories.

*2. Connect people.* We, the journalists, don't know how data journalism is done and we don't pretend to. By putting journalists, students and programmers in one room for 30 hours, we want them to share knowledge and insights.

*3. Host a social event.* Newspapers don't organise a lot of social events, let alone hackathons. We wanted to experience how such an event can yield results. In fact, the event could have been tense: 30 hours with strangers, lots of jargon, bashing your head against basic questions, working out of your comfort zone. By making it a social event - remember the pizza and energy drink? - we wanted to create an environment in which journalists and programmers could feel comfortable and collaborate effectively.

Before the event, TC Tubantia had an interview with the widow of a policeman who had written a book on her husband's working years. She also had a document with all registered murders in the eastern part of the Netherlands, maintained by her husband since 1945. Normally, we would publish this document on our website. This time, we made a http://www.tctubantia.nl/regio/9810350/Moord-en-doodslag-in-Twente.ece[dashboard using the Tableau software]. We also http://www.regiohack.nl/regiohack-blog/een-moord-voor-goede-gegevens/[blogged] about how this came together on our RegioHack site.

During the hackathon, one project group came up with the subject of development of schools and the ageing of our region. http://public.tableausoftware.com/views/Krimpleerlingaantalshrinkingnumberofstudents/Dashboard1?:embed=yes&:toolbar=yes&:tabs=yes[By making a visualisation of future projections], we understood which cities would get in trouble after a few years of decline in enrolments. With this insight, we made an article on how this would affect schools in our region.

We also started a very ambitious project, called De Tweehonderd van Twente (in English, The Two Hundred of Twente) to determine who had the most power in our region and build a database of the most influential people. Through a Google-ish calculation - who has the most ties with powerful organisations - a list of influential people will be composed. This could lead to a series of articles, but it's also a powerful tool for journalists. Who has connections with who? You can ask questions to this database and use it in our daily routine. Also, this database has cultural value. Artists already asked if they could use this database when finished to make interactive art installations.

image::figs/incoming/02-YY.jpg[width=600]

After RegioHack, we noticed that journalists considered data journalism as a viable addition to traditional journalism. My colleagues continued to use and build on the techniques learned on the day to create more ambitious and technical projects such as a database of the administrative costs of housing. With this data, I made http://www.destentor.nl/regio/10168441/.ece[an interactive map in Fusion Tables]. We asked our readers to play around with the data and crowdsourced results (http://tjoadesign.nl/blog/?p=439[here], for example). After a lot of questions on how we made a map in Fusion Tables, I also recorded a http://www.jerryvermanen.nl/2012/01/tutorial-fusion-tables/[video tutorial].

What did we learn? We learned a lot, but we also came along a lot of obstacles. We recognized these four:

*1. Where to begin: question or data?* Almost all projects stalled when searching for information. Most of the time, they began with a journalistic question. But then? What data is available? Where can you find it? And when you find this data, can you answer your question with it. Journalists usually know where they can find information when doing research for an article. With data journalism, most journalists don't know what information is available.

*2. Little technical knowledge*. Data journalism is quite a technical discipline. Sometimes you have to scrape, other times you'll have to do some programming to visualise your results. For excellent data journalism, you'll need two aspects: the journalistic insight of an experienced journalist and the technical know-how of a digital all-rounder. During RegioHack, this was not a common presence.

*3. Is it news?* Participants mostly used one dataset to discover news, instead of searching interconnections between different sources. The reason for this: you need some statistical knowledge to to verify news from data journalism.

*4. What's the routine?* What above all comes down to, is that there’s no routine. The participants have some skills under their belt, but don’t know how and when to use them. One journalist compared it with baking a cake. ‘We have all the ingredients: flour, eggs, milk, etcetera. Now we throw it all in a bag, shake it and hope a cake comes out of it.’ Indeed, we have all the ingredients, but don’t know what the recipe is.

What now? Our first experiences with data journalism could help other journalists or programmers aspiring the same field of work and we are working to produce a report.

Also, we are considering how to continue RegioHack in a hackathon form. We found it fun, educational and productive and a great introduction to data journalism.

But for data journalism to work, we have to integrate it in the newsroom. Journalists have to think in data, in addition to quotes, press releases, council meetings and so on. By doing RegioHack, we proved to our audience that data journalism isn't just hype. We can write better informed and more distinctive articles, while presenting our readers different articles in print and online.

=== Following the Money: Data Journalism and Cross-Border Collaboration (Paul Radu, Organized Crime and Corruption Reporting Project) ===

image::figs/incoming/02-RR.png[width=600]

Investigative journalists and citizens interested in uncovering organised crime and corruption that affect the lives of billions worldwide gain, with each passing day, unprecedented access to information. Huge volumes of data are made available online by governments and other organisations and it seems that much needed information is more and more in everyone’s grasp. However, at the same time, corrupt officials in governments and organised crime groups are doing their best to conceal information in order to hide their misdeeds. They make efforts to keep people in the dark while conducting ugly deals that cause disruptions at all society levels and lead to conflict, famine or other types of crisis.

It is the duty of investigative journalists to expose such wrongdoings and by doing so to disable corrupt and criminal mechanisms.

There are three main guidelines that, if followed, can lead to good, thorough journalism when investigating major acts of corruption and crime even in the most austere of environments:

*1. Think Outside Your Country*

In many instances it is much easier to get information from abroad than from within the country where the investigative journalist operates. Information gathered from abroad via foreign information databases or by using other countries’ access to information laws might be just what is needed to put the investigative puzzle together. On top of that, criminals and corrupt officials don’t keep their money in the place they stolen it from. They would rather deposit it in foreign banks or they would rather invest in other countries. Crime is global.

Databases that assist the investigative journalist in tracking the money worldwide can be found in many places on the Internet. For example, the http://www.investigativedashboard.org/category/wwd/[Investigative Dashboard] enables journalists to follow the money across borders.

*2. Make Use of the Existing Investigative Journalism Networks*

Investigative journalists all over the world are grouped in
organisations such as http://www.reportingproject.net/[The Organized Crime and Corruption Reporting
Project], http://www.fairreporters.org/[The African Forum for Investigative
Reporting], http://arij.net/[The Arab Reporters for Investigative
Journalism], http://www.gijn.org/[The Global investigative Journalism Network].

Journalists can also make use of professional journalism platforms such as IJNet where global journalism related information is exchanged on daily basis.

Many of the reporters grouped in networks work on similar issues and confront similar situations so it makes a lot of sense to exchange information and methods. Emailing lists or social network groups are attached to these networks so it is quite easy to get in touch with fellow journalists and to ask for information or advice. Investigative stories ideas can also be gathered from such forums and emailing lists.

*3. Make Use of Technology and Collaborate with Hackers*

Software helps investigative journalists access and process information. Various types of software assist the investigator in cutting through the noise, in digging and making sense of large volumes of data and in finding the right documents needed to break the story. There are many ready-made software programs that can be used as tools for analysing, gathering or interpreting information and, more important, investigative journalists need to be aware that there are scores of computer programmers ready to help if asked. These programmers or hackers know how to obtain and handle information and they can assist a big deal with the investigative effort. These programmers, some of them members of global open data movements, can become invaluable allies in the fight against crime and corruption. They can assist journalists in gathering and in analysing information.

A good example of an interface between programmers and citizens is https://scraperwiki.com/[Scraperwiki], a site where journalists
can ask programmers for help with extracting data from websites. Investigative Dashboard http://www.investigativedashboard.org/2011/02/software-resources/[maintains a list] of ready-made tools that could help to journalist gather, shape and analyse data can be found here.

The usefulness of the above-mentioned guidelines has been visible in many instances. One good example is the work of Khadija Ismayilova, a very experienced Azeri investigative reporter who works in a very austere environment when it comes to information access. Ms. Ismayilova has to overcome obstacles on daily basis in order to offer the Azeri public good and reliable information. In June of 2011, Khadija Ismayilova, an investigative reporter with Radio Free Europe/Radio Liberty’s (RFE/RL) Baku based office reported that the daughters of the Azeri president, Ilham Aliyev, secretly run http://www.rferl.org/content/azerbaijan_president_aliyev_daughters_tied_to_telecoms_firm/24248340.html[a fast rising telecom company Azerfon] through offshore companies based in Panama. The company boasts nearly 1.7 million subscribers, covers 80 percent of the country’s territory and was, at the time, Azerbaijan’s only provider of 3G services. Ismayilova spent three years trying to find out who were the owners of the telecom company but the government refused to disclose shareholder information and lied numerous times about the company's ownership. They even claimed that the company was owned by the German based Siemens AG, a claim that has been flatly denied by the German corporation. The Azeri reporter managed to find out that Azerfon was owned by a few Panama based private companies and this seemed to be a dead end to her reporting until help from outside was employed. In early 2011, Ms. Ismayilova learned, through the Investigative Dashboard that Panama based companies can be tracked down through http://ohuiginn.net/panama/[an application] developed by programmer and activist Dan O'Huiginn. It was then when she finally managed to uncover the fact that the president's two daughters were involved with the telecom company through the Panama based entities.

In fact, O'Huiginn created a tool that helped journalists from all over the world to report on corruption as Panama, a very well known offshore haven, has been widely used by corrupt officials from all over as a place to hide stolen money: from cronies of the former Egyptian president, Hosni Mubarak to dirty officials in the Balkans or in Latin America. What the programmer-activist has done is called web scraping; a method that allows the extraction and reshaping of information so that it can be used by investigators. O'Huiginn scraped the http://www.registro-publico.gob.pa/[Panama registry of companies] because this registry, although open, only allowed searches if the investigative reporter knew the name of the commercial company he or she was looking for. This limited the possibility to investigate as usually reporters look for names of persons in order to track down their assets. The programmer extracted the data and created a new web site where names-based searches are also possible. The new web site allowed investigative reporters in many countries to fish for information, to run names of officials in governments and Parliaments and to check if they secretly owned corporations in Panama just as the family of the Azerbaijan president.

There are other advantages to using the guidelines highlighted above, besides better access to information. One of them has to do with minimising harm and insuring better protection for investigative reporters who work in hostile environments. This is due to the fact that when working in a network the journalist is not alone, the investigative reporter works with colleagues in other countries so it is harder for criminals to pinpoint whom they think is guilty for their wrongdoings being exposed. As a result, retaliation by governments and corrupt officials is much harder to achieve.

Another thing to be kept in mind is that information that doesn’t seem very valuable in a geographical area might be crucially important in another. The exchange of information over investigative network can lead to breaking very important stories. For example, the information that a Romanian was caught in Colombia with 1 kilogram of cocaine is most probably not front-page news in Bogota but could be very important to the Romanian public if a local reporter manages to find out that the person who was caught with the narcotics is working for the government in Bucharest.

Efficient investigative reporting is the result of cooperation between investigative journalists, programmers and others who want to use data to contribute to create a cleaner, fairer and more just global society.

=== Our Stories Come As Code (Lorenz Matzat, OpenDataCity.de) ===

image::figs/incoming/02-TT.png[width=600]

http://www.opendatacity.de/[OpenDataCity] was founded towards the end of 2010. There was pretty much nothing that you could call data journalism happening in Germany at this time.

Why did we do this? Many times we heard people working for newspapers and broadcasters say: ``No, we are not ready to start a dedicated data journalism unit in our newsroom. But we would be happy to outsource this to someone else.''

As far as we know, we are the only company specialising exclusively in data journalism in Germany. There are currently three of us: two of us with a journalism background and one with a deep understanding of code and visualisation. We work with a handful of freelance hackers, designers and journalists.

In the last twelve month we have undertaken four data journalism projects with newspapers, and have offered training and consultancy to media workers, scientists and journalism schools. The first app we did was an http://www.taz.de/1/berlin/fluglaerm-bbi/[interactive tool on airport noise] around the the newly built airport in Berlin with TAZ. Our next notable project was an http://www.zeit.de/datenschutz/malte-spitz-data-retention[application about data retention] of the mobile phone usage of a German politician with ZEIT Online. For this we won a http://www.grimme-institut.de/html/index.php?id=1345[Grimme Online Award] and a Lead Award in Germany, and an Online Journalism Award by the http://journalists.org/2011/09/25/2011-online-journalism-award-winners-announced/[Online Journalism Association] in the US. At the time of writing, we are have several projects in the pipeline - ranging from simpler interactive infographics to full blown realtime data applications.

Of course, winning prizes helps to built a reputation. But when we talk to the publishers, who have to approve the projects, our argument for investing into data journalism is not about winning prizes. Rather it is about getting attention over a longer period in a sustainable way. Building things for their long term impact, not for the scoop, which often is forgotten after some days.

Here are three arguments which we have used to encourage publishers to undertake longer term projects:

1. *Data projects don't date*. Depending on their design, new material can be added to data journalism apps. And they are not just for the users, but can be used internally for reporting and analysis. If you're worried that this means that your competitors will also benefit from your investment, you could keep some features or some data for internal use only.

2. *You can build on your past work*. When undertaking a data project, you will often create bits of code which can be reused or updated. The next project might take half the time, because you know much better what to do (and what not to) and you have bits and pieces you can build on.

3. *Data journalism pays for itself*. Data driven projects are cheaper than traditional marketing campaigns. Online news outlets will often invest in things like Search Engine Optimisation (SEO) and Search Engine Marketing (SEM). A executed data project will normally generate a lot of clicks and buzz, and may go viral. Publishers will typically pay less for this then trying to generate the same attention by clicks and links through SEM.

Our work is not very different from other new media agencies: providing applications or services for news outlets. But maybe we differ in that we think of ourselves first and foremost as journalists. In our eyes the products we deliver are articles or stories - albeit ones which are provided not in words and pictures, audio or video, but in code. When we are talking about data journalism we have to talk about technology, software, devices and how to tell a story with them.

To give an example: we are currently working on an application, which pulls in real-time data via a scraper. The data is updated every minute or so. We started doing this some months ago, and have so far collected a huge dataset which grows every hour. By now it amounts to some hundreds of thousands of rows of data. We are creating a data-article which enables the user to explore this realtime data, and to do research in the archive of the foregone months. In the end the story we are telling will be significantly defined by the individual action of the users.

In traditional journalism, due to the linear character of written or broadcasted media, we have to think about a beginning, the end, the story arc and the length and angle of our piece. With data journalism things are different. There is a beginning, yes. People come to the website and get a first impression of the interface. But then they are on their own. Maybe they stay for a minute - or half an hour.

Our job as data journalists is to provide the framework or environment for this. As well as the coding and data management bits, we have to thing of clever ways to design experiences. The User Experience (UX) derives mostly from the (Graphical) User Interface (GUI). In the end this is the part which will make or break a project. You could have the best code working in the background handling an exiting dataset. But if the front-end sucks, nobody will care about it.

There is still a lot to learn about and to experiment with. But luckily there is the games industry, which has been innovating with respect to digital narratives, ecosystems and interfaces for several decades now. So when developing data journalism applications we should watch closely how game design works and how stories are told in games. Why are casual games like Tetris such fun? And what makes the open worlds of sandbox games like Grand Theft Auto or Skyrim rock?

=== Kaas og Mulvad: Semi-finished Content for Stakeholder Groups (Mark Lee Hunter and Luk N. Van Wassenhove, INSEAD) ===

image::figs/incoming/02-MM.png[width=600]

Stakeholder media is an emerging sector, largely overlooked by media theorists, which has the potential to have a tremendous impact either through online networks or by providing content to news media. It can be defined as (usually online) media that is controlled by organisational or institutional stakeholders, and which is used to advance certain interests and communities. NGOs typically create such media; so do consumer groups, professional associations, labour unions, etc. The key limit on its ability to influence public opinion or other stakeholders is often that it lacks capacity to undertake discovery of important information, even more so than the downsized news media. Kaas og Mulvad, a for-profit Danish corporation, is one of the first investigative media enterprises that provides expert capacity to these stakeholder outlets.

The firm originated in 2007 as a spinoff of the non-profit Danish Institute for Computer-Assisted Reporting (Dicar), which sold investigative reports to media and trained journalists in data analysis. Its founders, Tommy Kaas and Nils Mulvad, were previously reporters in the news industry. Their new firm offers what they call ``data plus journalistic insight'' - that is, content which remains semi-finished, requiring further editing or rewriting - mainly to stakeholder media, which finalise the content into news releases or stories and distribute it through both news media and their own outlets (such as websites). Direct clients include government institutions, PR firms, labour unions and NGOs such as EU Transparency and the World Wildlife Fund. The NGO work includes monitoring farm and fishery subsidies, and regular updates on EU lobbyist activities generated through ``scraping'' of pertinent websites. Indirect clients include foundations that fund NGO projects.The firm also works with the news industry - for example, a tabloid newspaper purchased their celebrity monitoring service.

Data journalism projects in their portfolio include:

  * http://46.38.167.183/unemployment/?focus=map[Unemployment Map for 3F] - 
A data visualisation with key figures about unemployment in Denmark - in general and among unskilled workers - undertaken for 3F, which is the union for unskilled labour in Denmark.
  * http://fagbladet3f.dk/temaer/udkant/kort[Living Conditions for 3F] - Another project for 3F shows how different living conditions are in different parts of Denmark. The map shows 24 different indicators for living conditions.
  * http://www.ugebreveta4.dk/2012/201206/Mandag/Sjaellaenderne_er_fanget_i_gaeld/Ugebreveta4s_kortlaegning_af_daarlige_betalere.aspx[Debt for ``Ugebrevet A4''] - A project that calculates a ``debt index'' – and visualizes the differences in private economy.
  * http://www.bupl.dk/iwfile/BALG-8F9BSD/$file/ALLESIDER_062011.pdf[Dangerous Facilities in Denmark] - A project which maps and analyses the proximity of dangerous facilities to kindergartens and other day care institutions, undertaken for ``Børn&Unge'', a magazine published by BUPL, the Danish Union of Early Childhood and Youth Educators.
  * http://data.vestas.com[Corporate Responsibility Data for Vestas] - Data visualisation on five areas of CR-data for the Danish wind turbine company, Vestas, with auto-generated text. Automatically updated on a quarterly basis with 400 webpages from world scale data down to the single production unit.
  * http://xpoint.experian.dk/navnekort[Name Map for Experian] – Type in your last name and look at the distribution of this name around different geographical areas in Denmark.
  * http://ekstrabladet.dk/kup/fodevarer[Smiley Map for Ekstra Bladet] – Every day we extract all the bad food inspections and map all the latest on a map for the Danish tabloid Ekstra Bladet (see half way down the website for the map).

Kass og Mulvad are not the first journalists to work with stakeholder media. Greenpeace, for example, routinely engages journalists as collaborators for its reports. But we know of no other firm whose offerings to stakeholder media are data-driven; it is much more typical for journalists to work with NGOs as reporters, editors or writers. The current focus in computer-assisted news media is on search and discovery (think of Wikileaks); here again Kaas og Mulvad innovate, by focusing on data analysis. Their approach requires not only programming skills, but also understanding of what kind of information can make an impactful story. It can safely be said that anyone wishing to imitate their service would probably have to acquire those two skill sets through partnership, because individuals rarely possess both.

**1. Processes: Innovative IT plus analysis**

The firm undertakes about 100 projects per year, ranging in duration from a few hours to a few months. It also continuously invests in projects that expand its capacity and offerings. The celebrity monitoring service was one such experiment. Another involved scraping the Internet for news of home foreclosures and creating maps of the events. The partners say that their first criteria for projects is whether they enjoy the work and learn from it; markets are sought after a new service is defined. They make it clear that in the news industry, they found it difficult to develop new methods and new business.

Comments Mulvad:

____
We have no editors or bosses to decide which projects we can do, which software or hardware we can buy. We can buy the tools according to project needs - like the best solutions for text scraping and mining. Our goal is to be cutting edge in these areas. We try to get customers who are willing to pay, or if the project is fun we do it for a lower charge.
____

**2. Value created: Personal and firm brands and revenue**

Turnover in 2009 was approximately 2.5 million Danish kroner, or €336,000. The firm also sustains the partners' reputations as cutting edge journalists, which maintains demand for their teaching and speaking services. Their public appearances, in turn, support the firm's brand.

**3. Key insights of this example**

  * The news industry's crisis of declining capacity is also a crisis of under- utilisation of capacity. Kaas and Mulvad had to leave the news industry to do work they valued, and that pays. Nothing prevented a news organisation from capturing that value.
  * In at least some markets, there exists a profitable market for ``semi-finished'' content that can serve the interests of stakeholder groups.
  * However, this opportunity raises the issue of how much control journalists can exercise over the presentation and use of their work by third parties. We recall that this issue already exists within the news industry (where editors can impose changes on a journalist's product), and it has existed within other media industries (such as the film industry, where conflicts between directors and studios over ``final cuts'' are hardly rare). It is not a particular moral hazard of stakeholder media, but it will not disappear, either. More attention is needed to the ethics of this growing reality and market.
  * From a revenue standpoint, a single product or service is not enough. Successful watchdog enterprises would do better to take a portfolio approach, in which consulting, teaching, speaking and other services bring in extra revenue, support the watchdog brand.

_Edited excerpt from Mark Lee Hunter and Luk N. Van Wassenhove, ``Disruptive News Technologies: Stakeholder Media and the Future of Watchdog Journalism Business Models''. INSEAD Working Paper, 2010_

=== Business Models for Data Journalism (Mirko Lorenz, Deutsche Welle) ===

Amidst all the interest and hope regarding data-driven journalism there is one question that newsrooms are always curious about: what are the business models?

While we must be careful about making predictions, a look at the recent history and current state of the media industry can help to give us some insight. Today there are many news organisations who have gained by adopting new approaches.

Terms like `data journalism', and the newest buzzword ``data science'' may sound like they describe something new, but this is not strictly true. Instead these new labels are just ways of characterising a shift that has been gaining strength over decades.

Many journalists seem to be unaware of the size of the revenue that is already generated through data collection, data analytics and visualisation. This is the business of information refinement. With data tools and technologies it is increasingly possible to shed a light on highly complex issues, be this international finance, debt, demography, education and so on. The term `business intelligence' describes a variety of IT concepts aiming to provide a clear view on what is happening in commercial corporations. The big and profitable companies of our time, including McDonalds, Zara or H&M rely on constant data tracking to turn out a profit. And it works pretty well for them.

What is changing right now is that the tools developed for this space are now becoming available for other domains, including the media. And there are journalists who get it. Take Tableau, a company providing a suite of visualisation tools. Or the ``Big Data'' movement, where technology companies use (often open source) software packages to dig through piles of data, extracting insights in milliseconds.

These technologies can now be applied to journalism. Teams at The Guardian and The New York Times are constantly pushing the boundaries in this emerging field. And what we are currently seeing is just the tip of the iceberg.

But how does this generate money for journalism? The big, worldwide market that is currently opening up is all about transformation of publicly available data into something our that we can process: making data visible and making it human. We want to be able to relate to the big numbers we hear every day in the news - what the millions and billions mean for each of us.

There are a number of very profitable data-driven media companies, who have simply applied this principle earlier than others. They enjoy healthy growth rates and sometimes impressive profits. One example: Bloomberg. The company operates about 300,000 terminals and delivers financial data to it's users. If you are in the money business this is a power tool. Each terminal comes with a colour coded keyboard and up to 30,000 options to look up, compare, analyse and help you to decide what to do next. This core business generates an estimated US $6.3 billion per year, at least this what http://www.nytimes.com/2009/11/15/business/media/15bloom.html?pagewanted=all[a piece by the New York Times estimated in 2008]. As a result, Bloomberg has been hiring journalists left, right and centre, they bought the venerable but loss-making ``Business Week'' and so on.

Another example is the Canadian media conglomerate today known as Thomson Reuters. They started with one newspaper, bought up a number of well known titles in the UK, and then decided two decades ago to leave the newspaper business. Instead they have grown based on information services, aiming to provide a deeper perspective for clients in a number of industries. If you worry about how to make money with specialised information, the advice would be to just http://en.wikipedia.org/wiki/The_Thomson_Corporation[read about the company's history in Wikipedia].

And look at the Economist. The magazine has built an excellent, influential brand on its media side. At the same time the ``Economist Intelligence Unit'' is now more like a consultancy, reporting about relevant trends and forecasts for almost any country in the world. They are employing hundreds of journalists and claim to serve about 1.5 million customers worldwide.

And there are many niche data-driven services that could serve as inspiration: eMarketer in the US, providing comparisons, charts and advice for anybody interested in internet marketing. Stiftung Warentest in Germany, an institution looking into the quality of products and services. Statista, again from Germany, a start-up helping to visualise publicly available information.

Around the world there is currently a wave of startups in this sector, naturally covering a wide range of areas - for example, Timetric, which aims to ``reinvent business research'', OpenCorporates, Kasabi, Infochimps and Data Market. Many of these are arguably experiments, but together they can be taken as an important sign of change.

Then there is the public media - which in terms of data-driven journalism is a sleeping giant. In Germany €7.2 billion per year are flowing into this sector. Journalism is a special product: if done well it is not just about making money, but serves an important role in society. Once it is clear that data journalism can provide better, more reliable insights more easily, some of this money could be used for new jobs in newsrooms.

With data journalism, it is not just about being first but about being a trusted source of information. In this multi-channel world, attention can be generated in abundance, but _trust_ is an increasingly scarce resource. Data journalists can help to collate, synthesise and present diverse and often difficult sources of information in a way which gives their audience real insights into complex issues. Rather than just recycling press releases and retelling stories they've heard elsewhere, data journalists can give readers a clear, comprehensible and preferably customisable perspective with interactive graphics and direct access to primary sources. Not trivial, but certainly valuable.

So what is the best approach for aspiring data journalists to explore this field and convince management to support innovative projects?

The first step should be to look for immediate opportunities close to home: low hanging fruit. For example, you might already have collections of structured texts and data that you could use. A prime example of this is the ``Homicide database'' of the Los Angeles Times. Here data and visualisations are the core, not an afterthought. The editors collect all the crimes they find and only then write articles based on this. Over time, such collections are becoming better, deeper and more valuable.

This might not work the first time. But it will over time. One very hopeful indicator here is that the Texas Tribune and ProPublica, which are both arguably post-print media companies, reported that funding for their non-profit journalism organisations exceeded their goals much earlier than planned.

Becoming proficient in all things data - whether as a generalist or as a specialist focused on one aspect of the data food chain - provides a valuable perspective for people who believe in journalism. One well-known publisher in Germany recently said in an interview: ``There is this new group who call themselves data journalists. And they are not willing to work for peanuts anymore''.