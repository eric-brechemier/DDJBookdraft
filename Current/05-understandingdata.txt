== 5. Understanding data ==

Other ideas for this section:

  * Preparing Data
  * Reconciliation
  * Combining Datasets (cf: http://www.opendatacookbook.net/wiki/recipe/a_dataset_mixture_with_yahoo_pipes)
  * Text Mining

=== Become data literate in 3 simple steps by Nicolas Kayser-Bril (J++) ===

Just as literacy refers to "the ability to read for knowledge, write coherently and think critically about printed material" data-literacy is the ability to consume for knowledge, produce coherently and think critically about data. Data literacy includes statistical literacy but also understanding how to work with large data sets, how they were produced, how to connect various data sets and how to interpret them. This chapter will focus on the former. The latter is covered in chapter 3.2 Working with data.

Innumeracy is a big thing

Poynter's News University offers classes of http://www.newsu.org/courses/math-journalists[Math for journalists], in which reporters get help with concepts such as percentage changes and averages. Interestingly enough, these concepts are being taught simultaneously near Poynter's offices, in Floridian schools, to fifth grade pupils (age 10-11), http://www.k12.com/courses/scope-sequence/ma5f1[as the curriculum attests].

That journalists need help in math topics normally covered before high school shows how far newsrooms are from being data literate. This does not go without problems. How can a data-journalist make use of a bunch of numbers on climate change if she doesn't know what a confidence interval means? How can a data-reporter write a story on income distribution if he cannot tell the http://karenberger.suite101.com/mean-median-and-mode-journalists-guide-to-calculating-averages-a352390[mean from the median]?

A reporter certainly does not need a degree in statistics to become more efficient when dealing with data. When faced with numbers, a few simple tricks can help her get a much better story. As Max Planck Institute professor http://datadrivenjournalism.net/news_and_analysis/the_importance_of_numeracy_for_data_journalists[Gerd Gigerenzer says] says, better tools will not lead to better journalism if they are not used with insight.

Get data literate in 3 simple steps!

Even if you lack any knowledge of math or stats, you can easily become a seasoned data-journalist by asking 3 very simple questions.

**1. How was the data collected?**

Amazing GDP growth

The easiest way to show off with spectacular data is to fabricate it. It sounds obvious, but data as commonly commented upon as GDP figures can very well be phony. Former British ambassador Craig Murray reports in his book, http://www.amazon.com/Murder-Samarkand-Ambassadors-Controversial-Defiance/dp/1845962214[Murder in Samarkand], that growth rates in Uzbekistan are subject to intense negotiations between the local government and international bodies. In other words, it has nothing to do with the local economy.

GDP is used as the number one indicator because governments need it to watch over their main source of income - VAT. When a government is not funded by VAT, or when it does not make its budget public, it has no reason to collect GDP data and will be better-off fabricating them.

Crime is always on the rise

"Crime in Spain grew by 3%", http://internacional.elpais.com/internacional/2008/09/29/actualidad/1222639208_850215.html[writes El Pais]. Brussels is prey to increased crime from illegal aliens and drug addicts, http://www.rtl.be/info/votreregion/bruxelles/835282/criminalite-en-hausse-a-bruxelles-la-faute-aux-illegaux-et-aux-drogues-[says RTL]. This type of reporting based on police-collected statistics is common, but it doesn't tell us much about violence.

We can trust that within the European Union, the data isn't tampered with. But police personnel respond to incentives. When performance is linked to elucidation rate, for instance, policemen have an incentive to reports as much as possible on incidents that don't require an investigation. One such crime is smoking pot. This explains why drug-related crimes in France increased fourfold in the last 15 years while consumption remained constant.

What you can do

When in doubt about a number's credibility, always double check, just as you'd have if it had been a quote from a politician. In the Uzbek case, a phone call to someone who's lived there for a while suffices ('Does it feel like the country is 3 times as rich as it was in 1995, as official figures show?').

For police data, sociologists often carry out 'victimization' studies, in which they ask people if they are subject to crime. These studies are much less volatile than police data. Maybe that's the reason why they don't make headlines.

Other tests let you assess precisely the credibility of the data, such as Benford's law, but none will replace your own critical thinking.

**2. What's in there to learn?**

Risk of Multiple Sclerosis doubles when working at night

Surely any German in her right mind would stop working night shifts after http://www.dmsg.de/multiple-sklerose-news/index.php?w3pid=news&kategorie=forschung&anr=2476[reading this headline]. But the article doesn't tell us what the risk really is in the end.

Take 1,000 Germans. A single one will develop MS over his lifetime. Now, if every one of these 1,000 Germans worked night shifts, the number of MS sufferers would jump to 2. The additional risk of developing MS when working in shifts is 1 in 1,000, not 100%. Surely this information is more useful when pondering whether to take the job.

On average, 1 in every 15 Europeans totally illiterate

The above headline looks frightening. It is also absolutely true. Among the 500 million Europeans, 36 million probably don't know how to read. As an aside, 36 million are also under 7 (data from http://epp.eurostat.ec.europa.eu/portal/page/portal/statistics/search_database[Eurostat]).

When writing about an average, always think "an average of what?" Is the reference population homogeneous? Uneven distribution patterns explain why most people drive better than average, for instance. Many people have zero or just one accident over their lifetime. A few reckless drivers have a great many, pushing the average number of accidents way higher than what most people experience. The same is true of the income distribution: most people earn less than average.

What you can do

Always take the distribution and base rate into account. Checking for the mean and median, as well as mode (the most frequent value in the distribution) helps you gain insights in the data. Knowing the order of magnitude makes contextualization easier, as in the MS example. Finally, reporting in natural frequencies (1 in 100) is way easier for readers to understand that using percentage (1%).

**3. How reliable is the information?**

The sample size problem

"80% dissatisfied with the judicial system", says a survey http://www.diariodenavarra.es/noticias/mas_actualidad/sociedad/una_encuesta_revela_que_casi_los_espanoles_esta_insatisfecho_con_justicia_50090_1035.html[reported in Zaragoza-based Diaro de Navarra]. How can one extrapolate from 800 respondents to 46 million Spaniards? Surely this is full of hot air.

When researching a large population (over a few thousands), you rarely need more than a thousand respondents to achieve a margin of error under 3%. It means that if you were to retake the survey with a totally different sample, 9 times out of 10, the answers you'll get will be within a 3% interval of the results you had the first time around. Statistics are a powerful thing, and sample sizes are rarely to blame in dodgy surveys.

Drinking tea lowers the risk of stroke

Articles about the benefits of tea-drinking are commonplace. This short http://www.welt.de/print-welt/article249051/Tee_schuetzt_vor_Herzinfarkt.html[item in Die Welt] saying that tea lowers the risk of myocardial infarction is no exception. Although the effects of tea are seriously studied by some, many pieces of research fail to take into account lifestyle factors, such as diet, occupation or sports.

In most countries, tea is a beverage for the health-conscious upper classes. If researchers don't control for lifestyle factors in tea studies, they tell us nothing more than 'rich people are healthier - and they probably drink tea'.

What you can do

The math behind correlations and error margins in the tea studies are certainly correct, at least most of the time. But if researchers don't look for co-correlations (e.g. drinking tea correlates with doing sports), their results are of little value.

As a journalist, it makes little sense to challenge the numerical results of a study, such as the sample size, unless there are serious doubts about it. However, it is easy to see if researchers failed to take into account relevant pieces of information.

Read on

Just as you wouldn't trust a person you've just met on the street (or in government), don't trust data you've just been introduced to. The resources below will give you more tools to assess the credibility of your data:

Tools

Section 3.3 of this book
http://www.tcij.org/training-material/data-journalism[Excel for data journalists]: How do simple operations (averages, proportions) in Excel.

Blogs

http://xkcd.com/[XKCD]: Nerdy strips that often make use of statistical concepts.
http://understandinguncertainty.org/blog[Understanding uncertainty]: Analysis of risk-related current events.

Books

http://www.amazon.com/Calculated-Risks-Know-Numbers-Deceive/dp/0743254236/ref=sr_1_2?ie=UTF8&qid=1314092449&sr=8-2[Calculated Risks], by Gerd Gigerenzer. The most comprehensive piece on how to communicate risks effectively and how to understand what a percentage really means. If you want to be able to debunk PR mystifications, that's the place to start.
http://www.amazon.com/Tiger-That-Isnt-Andrew-Dilnot/dp/1846681111/ref=sr_1_1?ie=UTF8&qid=1321014673&sr=8-1[The Tiger that isn't], by Andrew Dilnot & Michael Blastland. Lots of examples (from the UK) examining journalistic failures at fending off data-driven PR.
http://www.amazon.com/Numbers-Rule-Your-World-Probabilities/dp/0071626530/ref=sr_1_1?ie=UTF8&qid=1314092432&sr=8-1[Numbers Rule Your World], by Kaiser Fung. The easier-to-read version of Gigerenzer's book. Through 6 in-depth examples, it introduces you to similar concepts but remains shallow.
http://www.amazon.com/Proofiness-Dark-Arts-Mathematical-Deception/dp/0670022160/ref=sr_1_1?ie=UTF8&qid=1314092460&sr=8-1[Proofiness], by Charles Seife. Explains in an enjoyable way how numbers can be fabricated ‚Äì and how you can deconstruct them.

=== Tips for working with numbers in the news from Michael Blastland (BBC) ===

////

Insert: picture relevant to this section

////

1. The best tip for handling data is to enjoy yourself. Data can appear forbidding. But allow it to intimidate you and you'll get nowhere. Treat it as something to play with and explore and it will often yield secrets and stories with surprising ease. So handle it simply as you'd handle other evidence, without fear or favour. In particular, think of this as an exercise in imagination. Be creative by thinking of the alternative stories that might be consistent with the data and explain it better, then test them against more evidence. 'What other story could explain this?' is a handy prompt to think about how this number, this obviously big or bad number, this clear proof of this or that, might be nothing of the sort.

2. Don't confuse scepticism about data with cynicism. Scepticism is good; cynicism has simply thrown up its hands and quit. If you believe in data journalism, and you probably do or you wouldn't be reading this book, then you must believe that data has something far better to offer than the lies and damned lies of caricature or the killer facts of swivel-eyed headlines. Data often give us profound knowledge, if used carefully. We need to be neither cynical nor na‚àö√òve, but alert.

3. If I tell you that drinking has gone up during the recession, you might tell me it's because everyone is depressed. If I tell you that drinking is down, you might tell me it's because everyone is broke. In other words, what the data says makes no difference to the interpretation that you are determined to put on it, namely that things are terrible one way or the other. If it goes up, it's bad, if it goes down, it's bad. The point here is that if you believe in data, try to let it speak before you slap on your own mood, beliefs or expectations. There's so much data about that you will often be able to find confirmation of your prior beliefs if you simply look around a bit. In other words, data journalism, to me at least, adds little value if you are not open-minded. It is only as objective as you strive to make it, and not by virtue of being based on numbers.

4. Uncertainty is ok. We associate numbers with authority and certainty. Often as not, the answer is that there is no answer, or the answer may be the best we have but still wouldn't hit a barn door for accuracy. I think we should say these things. If that sounds like a good way of killing stories, I'd argue that it's a great way of raising new questions. Equally, there can often be more than one legitimate way of cutting the data. Numbers don't have to be either true or false.

5. The investigation is a story. The story of how you tried to find out can make great journalism, as you go from one piece of evidence to another - and this applies in spades to the evidence from data, where one number will seldom do. Different sources provide new angles, new ideas, richer understanding. I wonder if we're too hung up on wanting to be authoritative and tell people the answer - and so we miss a trick by not showing the sleuthing.

6. The best questions are the old ones: is that really a big number? Where did it come from? Are you sure it counts what you think it counts? These are generally just prompts to think around the data, the stuff at the edges that got squeezed by looking at a single number, the real-life complications, the wide range of other potential comparisons over time, group or geography; in short, context.

To add:
http://blogs.channel4.com/factcheck/welcome-to-the-new-factcheck-blog/18


=== Basic steps in working with data by Steve Doig (Walter Cronkite School of Journalism) ===

Working with data implies identifying, collecting, cleaning, analyzing, using and sharing the data you are using for your story. When working with data, you will find this iterative and potentially messy. Sometimes you will know exactly what you are looking for, and sometimes you will go fishing. Occasionally a story will emerge from the data. Often, you will be able to complement a story with more hard evidence or helpful information so readers can understand better.

There are several key concepts you need to understand when starting a data project:
Data requestscollection should begin with a list of questions you want to answer.
Data often is messy and needs to be cleaned.
Data may have undocumented features
Be aware of the provenance and terms of use of your data (see chapter 2. Getting Data)
Understand the limitations of your data (Is it reliable? Is it accurate?) See section 3.1 Data Literacy of this handbook)
Be transparent about how you used the data so others can use it.

**1. Know the questions you want to answer**

In many ways, working with data is like interviewing a live source. You ask questions of the data and get it to reveal the answers. But just as a source can only give answers about that which he or she has information, a data set can only answer questions for which it has the right records and the proper variables. This means that you should consider carefully what questions you need to answer even before you acquire your data. Basically, you work backwards. First, list the data-evidenced statements you want to make in your story. Then decide which variables and records you would have to acquire and analyze in order to make those statements.
Consider an example involving local crime reports. Let's say you want to do a story looking at crime patterns in your city, and the statements you want to make involve the times of day and the days of a week in which different kinds of crimes are most likely to happen, as well as what parts of town are hot spots for various crime categories.
You would realize that your data request has to include the date and the time each crime was reported, the kind of crime (murder, theft, burglary, etc.) as well as the address of where the crime occurred. So Date, Time, Crime Category and Address are the minimum variables you need to answer those questions.
But be aware that there are a number of potentially interesting questions that this four-variable data set CAN'T answer, like the race and gender of victims, or the total value of stolen property, or which officers are most productive in making arrests. Also, you may only be able to get records for a certain time period, like the past three years, which would mean you couldn't say anything about whether crime patterns have changed over a longer period of time. Those questions may be outside of the planned purview of your story, and that's fine. But you don't want to get into your data analysis and suddenly decide you need to know what percent of crimes in different parts of town are solved by arrest.
One lesson here is that it's often a good idea to request ALL the variables and records in the database, rather than the subset that could answer the questions for the immediate story. (In fact, getting all the data can be cheaper than getting a subset, if you have to pay the agency for the programming necessary to write out the subset.) You can always subset the data on your own, and having access to the full data set will let you answer new questions that may come up in your reporting and even produce new ideas for follow-up stories. It may be that confidentiality laws or other policies mean that some variables, such as the identities of victims or the names of confidential informants, can't be released. But even a partial database is much better than none, as long as you understand which questions the redacted database can - and - can't answer.

**2. Find the right data sources**

Where do you begin? There are a number of ways to get data. 
Scraping (See chapter 2.3.1 Scraping data)
Download
FOI request (See section 2.2 Asking for data)
Purchasing datasets

For ways to get data see chapter 2. Getting Data.

**3. Checking the quality of your data and cleaning messy data**

One of the biggest problems in database work when working with government data is that often you will be using for analysis reasons data that has been gathered for bureaucratic reasons. The problem is that the standard of accuracy for those two is quite different.
For example, a key function of a criminal justice system database is to make sure that defendant Jones is brought from the jail to be in front of Judge Smith at the time of his hearing. For that purpose, it really doesn't matter a lot if Jones' birth date is incorrect, or that his street address is misspelled, or even if his middle initial is wrong. Generally, the system still can use this imperfect record to get Jones to Smith's courtroom at the appointed time.
But such errors can skew a data journalist's attempts to discover the patterns in the database. For that reason, the first big piece of work to undertake when you acquire a new data set is to examine how messy it is and then clean it up.

The things you need to check include:
Input mistakes, e.g. letters and other odd characters in columns of digits
Logical errors, e.g. proportions with a percentage greater than 100%

A good quick way to look for messiness is to create frequency tables of the categorical variables, the ones that would be expected to have a relatively small number of different values. (When using Excel, for instance, you can do this by using Filter or Pivot Tables on each categorical variable.)
Take "Gender", an easy example. You may discover that your Gender field includes any of a mix of values like these: Male, Female, M, F, 1, 0, MALE, FEMALE, etc., including misspellings like Femal. To do a proper gender analysis, you must standardize - Decide on M and F, perhaps, and then change all the variations to match the standards. Another common database with these kinds of problems are American campaign finance records, where the Occupation field might list "Lawyer", "Attorney", "Atty", "Counsel", "Trial Lawyer" and any of a wealth of variations and misspellings; again, the trick is to standardize the occupation titles into a shorter list of possibilities.
Data cleanup gets even more problematic when working with names. Are "Joseph T. Smith", "Joseph Smith", "J.T. Smith", "Jos. Smith" and "Joe Smith" all the same person? It may take looking at other variables like address or date of birth, or even deeper research in other records, to decide. But tools like Google Refine can make the cleanup and standardization task faster and less tedious.
Logical errors, e.g. proportions with a percentage greater than 100%
Anomalous figures, e.g. data points that can't possibly be true
Outliers, e.g. isolated or inconsistent data points

Recognising anomalous figures requires some prior knowledge on your part, a familiarity with the subject of the data. If you were collecting the total number of US states won by candidate in the American presidential election, for example, you wouldn't expect to see Arnold Schwarzenegger on the list (however much you might want it to be true).

Correlation between two variables can help detect outliers in your data. Deciding when to ignore them in your analysis can be tricky, however. The human brain detects meaningful patterns in random data, which means we can be tempted to remove outliers to create a clearer picture of what we hope to find. If you keep looking hard enough at the data, patterns will emerge from the noise. The danger is that you'll manipulate your data to fit the story you're looking for, cherry-picking the points that help support a result you're hoping to get, rather than giving the true story.

**4. Data may have undocumented features**

The Rosetta Stone of any database is the so-called data dictionary. Typically, this file (it may be text or pdf or even a spreadsheet) will tell you how the data file is formatted (delimited text, fixed width text, Excel, dBase, et al.), the order of the variables, the names of each variable and the datatype of each variable (text string, integer, decimal, et al.) You will use this information to help you properly import the data file into the analysis software you intend to use (Excel, Access, SPSS, Fusion Tables, any of various flavors of SQL, et al.)
The other key element of a data dictionary is an explanation of any codes being used by particular variables. For instance, Gender may be coded so that 1=Male and 0=Female. Crimes may be coded by your jurisdiction's statute numbers for each kind of crime. Hospital treatment records may use any of hundreds of 5-digit codes for the diagnoses of the conditions for which a patient is being treated. Without the data dictionary, these data sets could be difficult or even impossible to analyze properly.
But even with a data dictionary in hand, there can be problems. An example happened to reporters at the Miami Herald in Florida some years ago when they were doing an analysis of the varying rates of punishment that different judges were giving to people arrested for driving while intoxicated. The reporters acquired the conviction records from the court system and analyzed the numbers in the three different punishment variables in the data dictionary: Amount of prison time given, amount of jail time given, and amount of fine given. These numbers varied quite a bit amongst the judges, giving the reporters' evidence for a story about how some judges were harsh and some were lenient.
But for every judge, about 1-2 percent of the cases showed no prison time, no jail time and no fine. So the chart showing the sentencing patterns for each judge included a tiny amount of cases as "No punishment," almost as an afterthought. When the story and chart was printed, the judges howled in complaint, saying the Herald was accusing them of breaking a state law that required that anyone convicted of drunk driving be punished.
So the reporters went back to Clerk of the Court's office that had produced the data file and asked what had caused this error. They were told that the cases in question involved indigent defendants with first-time arrests. Normally they would be given a fine, but they had no money. So the judges were sentencing them to community service, such as cleaning litter along the roads. As it turned out, the law requiring punishment had been passed after the database structure had been created. So all the court clerks knew that in the data, zeros in each of the the prison-jail-fine variables meant community service. However, this WASN'T noted in the data dictionary, and therefore caused a Herald correction to be written.
The lesson in this case is to always ask the agency giving you data if there are any undocumented elements in the data, whether it is newly-created codes that haven't been included in the data dictionary, changes in the file layout, or anything else. Also, always examine the results of your analysis and ask "Does this make sense?" The Herald reporters were building the chart on deadline and were so focused on the average punishment levels of each judge that they failed to pay attention to the scant few cases that seemed to show no punishment. They should have asked themselves if it made sense that all the judges seemed to be violating state law, even if only to a tiny degree.


**5. Analyzing your data**

After collecting data (Chapter 2) it is important to understand your dataset.
What type of analysis do you need to do? What are you comparing?
Time-based
Geographic regions
Cross-sectional: what are the portions of the whole (e.g., by race for census data)

Depending on what kind of data analysis you are doing, there are a range of different types of tools you can use. For this see chapter 3.3 Tools for Analysing data.	

Detecting dodgy data

Not all data comes from measurement of real-life numbers. From individuals to organisations, the pressure to support an argument with stats can unfortunately lead people to fabricate, falsify and omit data.

Even those who understand the importance of good data aren't beyond reproach: for example, http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0005738[2% of scientists have admitted to tampering with data]. This is a conservative estimate based on surveys, so the true number of researchers who have committed fraud through data manipulation may be much higher.

So how do you identify dodgy data? Although it's impossible to spot fraud simply by looking at numbers directly, luckily there are a few statistical tests that can help you detect anomalies. One of the most practical is a strange statistical phenomenon known as Benford's law, or the leading-digit law.

According to Benford's law, in a list of numbers from natural sources of data, the leading digit (e.g. the 1 in 10) is not distributed randomly. While you might expect the digits 0 to 9 to each be found in 11 per cent of cases, the digit 1 actually appears about 30% of the time. The other digits appear in lower and lower frequencies, following an exponential decline, ending with numbers that have a leading 9 only occurring in under 5% of numbers from the list.

////

Insert: Creative Commons image of Benford curve here?

////


This phenomenon appears in many lists of data from natural sources, which means that deviations from Benford's law may indicate false data. If you plot each leading digit in a data set against the frequency with which they appear, you should get a graph showing an exponential decline, which is what you would expect from Benford's law.

To test whether your data is dodgy, all you have to do it take the leading digit from a list of numbers in your data, tally the number of times each digit appears, and then superimpose this over a Benford curve. If your data matches the expected curve, then your data is probably free of fabrication.

- Link to 3.3 tools for analysing data (more specific on the types of things you can do to mend data with

=== The £32 loaf of bread by Claire Miller (WalesOnline) ===

////

Insert: screenshot from article

////

A story for Wales on Sunday about how much the Welsh Government is spending on prescriptions for gluten-free products, contained the headline figure that it was paying ¬£32 for a loaf of bread. (http://www.walesonline.co.uk/news/wales-news/2011/07/17/prescriptions-for-gluten-free-bread-costing-welsh-taxpayers-32-a-loaf-91466-29067430/)

However, this was actually 11 loaves that cost £2.82 each.

The figures, from a Welsh Assembly written answer and a Welsh NHS statistics release, listed the figure as cost per prescription item. However, they gave no additional definition in the data dictionary of what a prescription item might refer or how a separate quantity column might define it.

The assumption was that it referred to an individual item, e.g. a loaf of bread, rather than what it actually was, a pack of several loafs.

No one, not the people who answered the written answer or the press office, when it was put to them, raised the issue about quantity...until the Monday after the story was published.

So do not assume the people responsible for the data will realise the data is not clear even when you tell them your mistaken assumption.

Generally newspapers want things that make good headlines, so unless something obviously contradicts an interpretation, t is usually easier to go with what makes a good headline and not check too closely and risk the story collapsing, especially on deadline.

But the responsibility is to check the ridiculous claims even if it drops the story down the newslist.

=== Fresh out of the packet. Data journalists discuss their tools of choice. ===

////

Insert: picture of lots of tools arranged on screen, ready to go

////

Psssss. That is the sound of your data decompressing from its airtight wrapper. Now what? What do you look for? And what tools do you use to get stuck in?

Infobox:
Tools of choice at the Chicago Tribune
by Brian Boyer

Our tools of choice include Python and Django. For hacking, scraping and playing with data, and PostGIS, QGIS and the MapBox toolkit for building crazy web maps. R and NumPy + MatPlotLib are currently battling for supremacy as our kit of choice for exploratory data analysis, though our favorite data tool of late is homegrown: CSVKit. More or less everything we do is deployed in the cloud.

Todo:

  * Reorganise notes below into a section with concise and compelling anecdotes rather than a list of software packages and web services

To ask:

  * Friedrich Lindenberg
  * KNB
  * Simon Rogers
  * NYT
  * Tony @ OUseful
  * Examples for how people used different tools - e.g. ScraperWiki, Google Refine, Google Motion Charts, etc.

### Tools and techniques for analysing data ###

3.3 Tools, tutorials and case studies for all steps of the data journalism process

Overview: Overview of different types of tools for analysing and working with datasets, examples of how they can be used, examples of how they have been used by journalists.
Authors: Liliana Bounegru, Lucy Chambers, Claire Miller
Length: 1-2 pages per case study


**1. Collection + 2. Filtering**

**1. Tools for data collection, storing, cleaning, structuring, quality checking and annotation**

**2. Tools for data filtering, interrogation, combination and analysis**
Beginner level
Spreadsheets
Manual on Excel and Pivot Tables: http://www.tcij.org/training-material/data-journalism
Google Spreadsheets
Tutorial Excel: http://blog.buzzdata.com/post/11607498580/visualizing-torontos-water-usage-a-tutorial
Google Refine
http://vis.stanford.edu/wrangler/[Stanford Data Wrangler]
http://datapatterns.org/[Data Patterns]
http://www.poynter.org/how-tos/digital-strategies/155975/how-journalists-can-use-google-refine-to-clean-dirty-data-sets/[How journalists can use Google Refine to clean 'dirty' data sets]
http://overview.ap.org/[The Overview Project]
http://googlerefine.blogspot.com/
Cleaning data with Google Refine
http://www.opendatacookbook.net/wiki/recipe/sliced_and_diced_aid_data_with_google_refine
http://www.propublica.org/nerds/item/using-google-refine-for-data-cleaning
Combining data with Google Refine
Databases
Microsoft Access / Filemaker (Mac)
open-source SQL database managers
http://www.w3schools.com/sql/default.asp[SQL Tutorial]
https://github.com/tthibo/SQL-Tutorial#readme[A Gentle Introduction to SQL Using SQLite]
http://notebook.okfn.org/2011/10/08/introducing-sql-for-lightweight-data-manipulation/[Introducing SQL for Lightweight Data Manipulation]
http://helpmeinvestigate.posterous.com/structured-query-language-sql-an-introduction[Structured Query Language (SQL): an introduction]NoSQL Databases
Key Value Store
Document Store
Google Fusion Tables
demo in http://www.opendatacookbook.net/wiki/recipe/fusion_cooking_with_foraged_data[Open Data Cookbook]
http://support.google.com/fusiontables/bin/answer.py?hl=en&answer=185991[What options exist for displaying icons, lines and polygon colors on the map?]
http://blog.buzzdata.com/post/12201120862/how-to-map-in-fusion-tables-a-basic-tutorial[How to map in Fusion Tables: a basic tutorial]
http://michelleminkoff.com/2011/08/21/how-to-combine-multiple-fusion-tables-into-one-map/?utm_source=feedburner&utm_medium=feed&utm_campaign=Feed%3A+michelleminkofffeed+%28Michelle+Minkoff%29&utm_content=Google+Reader[How to combine multiple Fusion Tables into one map]
http://michelleminkoff.com/2011/10/30/answering-some-faqs-about-fusion-tables/?utm_source=feedburner&utm_medium=feed&utm_campaign=Feed%3A+michelleminkofffeed+%28Michelle+Minkoff%29&utm_content=Google+Reader[Answering some FAQs about Fusion Tables]

Yahoo Pipes:
http://www.slideshare.net/onlinejournalist/introduction-to-yahoo-pipes[Introduction to Yahoo Pipes]
http://www.youtube.com/watch?v=J3tS_DkmbVA[Learn How to Build a Pipe in Just a Few Minutes on Yahoo!]
http://blog.ouseful.info/2010/10/27/discovering-co-location-communities-tweets-near-wherever/[Discovering Co-location Communities ‚Äì Twitter Maps of Tweets Near Wherever‚Ä¶]


XML

http://onlinejournalismblog.com/2011/08/05/sftw-asking-questions-of-a-webpage-and-finding-out-when-those-answers-change/



Advanced level
Graph Database
Graph/Network Analysis
Comparing data links
UCINet, Gephi, NodeXL
Example: Social networks
R
http://www.peteraldhous.com/CAR/Aldhous_CAR2011_RforStats.pdf[R for Statistics: First Steps] (PDF) by Peter Aldhous, http://jacobfenton.s3.amazonaws.com/R-handson.pdf[Hands-on R, a step-by-step tutorial] (PDF) by Jacob Fenton, and the project's own An Introduction to R.
The http://www.r-statistics.com/tag/visualization/[R Statistics blog] has a number of visualization samples.
http://www.readwriteweb.com/hack/2011/09/unlocking-big-data-with-r.php[Unlocking Big Data with R]
http://blog.ouseful.info/2011/10/31/power-tools-for-aspiring-data-journalists-r/[Power Tools for Aspiring Data Journalists: Funnel Plots in R]
Book for journalists: Where are the bodies buried on the web? Big data for journalists: 'This short guide will cover my favorite resources, along with a few examples of how they've been used to create compelling journalism.'
http://csvkit.readthedocs.org/en/latest/index.html[CSVKit] ("Got a fixed-width file and wish it was easier to work with? Look at FFS It's a list of schemas for converting fixed-width files to CSV using this tool)



Network Analysis
Download this detailed http://casci.umd.edu/images/4/46/NodeXL_tutorial_draft.pdf[free NodeXL tutorial] (PDF) or these basic step-by-step instructions on http://faculty.washington.edu/pnhoward/teaching/newmedia/socialnetworkmap.pdf[analyzing your own Facebook social network] (PDF).
-http://onlinejournalismblog.com/2011/11/15/network-analysis-html5-data-journalism/[Following the money: making networks visible with HTML5]



Expert level
Statistical analysis
Searching for complex relationships, useful when working with a subset of a data
SAS
SPSS
R


**3. Tools for publishing data, visualising data and building news apps**

Mapping and visualisation
http://www.gpsvisualizer.com/geocoder/[GPS Visualiser]
http://onlinejournalismblog.com/2011/08/12/how-to-convert-eastingnorthing-into-latlong-for-an-interactive-map/[How to: convert easting/northing into lat/long for an interactive map]
http://www.doogal.co.uk/[Postcode Finder]

**4. Tools for distributing your data projects**

Twitter

**5. Tools for analysing the impact of your data projects**

Google Analytics

add propublica tutorials to this chapter

Add: Edward Borasky DDJ Develper Studio - open sources - ask Liliana


=== Start with the data, finish with a story by Caelainn Barr (EU Data Journalist) ===


Approach 1: Think Headline


To draw your readers in you have to be able to hit them with a headline figure that makes them sit up and take notice. You should almost be able to read the story without having to know that it comes from a dataset.

Make it exciting and remember who your audience are as you go.

One example of this can be found in a project carried out by the Bureau of Investigative Journalism using the EU Commission's http://ec.europa.eu/beneficiaries/fts/index_en.htm[Financial Transparency System]. The story was constructed by approaching the data set with specific queries in mind.

We looked through the data for key terms like 'cocktail', 'golf' and 'away days'. This allowed us to determine what the Commission had spent on these items and raised plenty of questions and story lines to follow up.

But key terms don't always give you what you want, sometimes you have to sit back and think about what you're really asking for. During this project we also wanted to find out how much commissioners spent on private jet travel but the as the data set didn't contain the phrase 'private jet' we had to get the name of their travel providers by other means. Once we knew the name of the service provider to the Commission, 'Abelag', we were able to query the data to find out how much was being spent on services provided by Abelag.

In this approach we had a clearly defined objective in querying the data; to find a figure that would provide a headline-the colour followed.

Approach 2: Approach with a blacklist and look for exclusions


An easy way to pull storylines from data is to know what you shouldn't find in there!
A good example of how this can work is illustrated by the collaborative EU Structural Funds project between the Financial Times and the Bureau of Investigative Journalism.

We queried the data, based on the Commission's own rules about what kinds of companies and associations should be prohibited from receiving structural funds. One example was expenditure on tobacco and tobacco producers.

By querying the data with the names of tobacco companies, producers and growers we found data that revealed British American Tobacco were receiving ‚Äö√á¬®1.5m for a factory in Germany.

As the funding was outside the rules of Commission expenditure, it was a quick way to find a story in the data.

Approach 3: Just explore

You never know what you might find in a dataset, so just have a look. You have to be quite bold and this approach generally works best when trying to identify obvious characteristics that will show up through filtering (the biggest, extremes, most common etc.).
