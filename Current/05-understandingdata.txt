== 5. Understanding data ==

=== Become Data Literate in 3 Simple Steps (Nicolas Kayser-Bril, J++) ===

Just as literacy refers to ``the ability to read for knowledge, write coherently and think critically about printed material'' data-literacy is the ability to consume for knowledge, produce coherently and think critically about data. Data literacy includes statistical literacy but also understanding how to work with large data sets, how they were produced, how to connect various data sets and how to interpret them.

Poynter's News University offers classes of http://www.newsu.org/courses/math-journalists[Math for journalists], in which reporters get help with concepts such as percentage changes and averages. Interestingly enough, these concepts are being taught simultaneously near Poynter's offices, in Floridian schools, to fifth grade pupils (age 10-11), http://www.k12.com/courses/scope-sequence/ma5f1[as the curriculum attests].

That journalists need help in math topics normally covered before high school shows how far newsrooms are from being data literate. This does not go without problems. How can a data-journalist make use of a bunch of numbers on climate change if she doesn't know what a confidence interval means? How can a data-reporter write a story on income distribution if he cannot tell the http://karenberger.suite101.com/mean-median-and-mode-journalists-guide-to-calculating-averages-a352390[mean from the median]?

A reporter certainly does not need a degree in statistics to become more efficient when dealing with data. When faced with numbers, a few simple tricks can help her get a much better story. As Max Planck Institute professor http://datadrivenjournalism.net/news_and_analysis/the_importance_of_numeracy_for_data_journalists[Gerd Gigerenzer says] says, better tools will not lead to better journalism if they are not used with insight.

Even if you lack any knowledge of math or stats, you can easily become a seasoned data-journalist by asking 3 very simple questions.

*QUESTION ONE: How was the data collected?*

_Amazing GDP growth_

The easiest way to show off with spectacular data is to fabricate it. It sounds obvious, but data as commonly commented upon as GDP figures can very well be phony. Former British ambassador Craig Murray reports in his book, http://www.amazon.com/Murder-Samarkand-Ambassadors-Controversial-Defiance/dp/1845962214[Murder in Samarkand], that growth rates in Uzbekistan are subject to intense negotiations between the local government and international bodies. In other words, it has nothing to do with the local economy.

GDP is used as the number one indicator because governments need it to watch over their main source of income - VAT. When a government is not funded by VAT, or when it does not make its budget public, it has no reason to collect GDP data and will be better-off fabricating them.

_Crime is always on the rise_

``Crime in Spain grew by 3%'', http://internacional.elpais.com/internacional/2008/09/29/actualidad/1222639208_850215.html[writes El Pais]. Brussels is prey to increased crime from illegal aliens and drug addicts, http://www.rtl.be/info/votreregion/bruxelles/835282/criminalite-en-hausse-a-bruxelles-la-faute-aux-illegaux-et-aux-drogues-[says RTL]. This type of reporting based on police-collected statistics is common, but it doesn't tell us much about violence.

We can trust that within the European Union, the data isn't tampered with. But police personnel respond to incentives. When performance is linked to elucidation rate, for instance, policemen have an incentive to reports as much as possible on incidents that don't require an investigation. One such crime is smoking pot. This explains why drug-related crimes in France increased fourfold in the last 15 years while consumption remained constant.

_What you can do_

When in doubt about a number's credibility, always double check, just as you'd have if it had been a quote from a politician. In the Uzbek case, a phone call to someone who's lived there for a while suffices (`Does it feel like the country is 3 times as rich as it was in 1995, as official figures show?').

For police data, sociologists often carry out 'victimisation' studies, in which they ask people if they are subject to crime. These studies are much less volatile than police data. Maybe that's the reason why they don't make headlines.

Other tests let you assess precisely the credibility of the data, such as Benford's law, but none will replace your own critical thinking.

*2. What's in there to learn?*

_Risk of Multiple Sclerosis doubles when working at night_

Surely any German in her right mind would stop working night shifts after http://www.dmsg.de/multiple-sklerose-news/index.php?w3pid=news&kategorie=forschung&anr=2476[reading this headline]. But the article doesn't tell us what the risk really is in the end.

Take 1,000 Germans. A single one will develop MS over his lifetime. Now, if every one of these 1,000 Germans worked night shifts, the number of MS sufferers would jump to 2. The additional risk of developing MS when working in shifts is 1 in 1,000, not 100%. Surely this information is more useful when pondering whether to take the job.

_On average, 1 in every 15 Europeans totally illiterate_

The above headline looks frightening. It is also absolutely true. Among the 500 million Europeans, 36 million probably don't know how to read. As an aside, 36 million are also under 7 (data from http://epp.eurostat.ec.europa.eu/portal/page/portal/statistics/search_database[Eurostat]).

When writing about an average, always think ``an average of what?'' Is the reference population homogeneous? Uneven distribution patterns explain why most people drive better than average, for instance. Many people have zero or just one accident over their lifetime. A few reckless drivers have a great many, pushing the average number of accidents way higher than what most people experience. The same is true of the income distribution: most people earn less than average.

_What you can do_

Always take the distribution and base rate into account. Checking for the mean and median, as well as mode (the most frequent value in the distribution) helps you gain insights in the data. Knowing the order of magnitude makes contextualization easier, as in the MS example. Finally, reporting in natural frequencies (1 in 100) is way easier for readers to understand that using percentage (1%).

*3. How reliable is the information?*

_The sample size problem_

``80% dissatisfied with the judicial system'', says a survey http://www.diariodenavarra.es/noticias/mas_actualidad/sociedad/una_encuesta_revela_que_casi_los_espanoles_esta_insatisfecho_con_justicia_50090_1035.html[reported in Zaragoza-based Diaro de Navarra]. How can one extrapolate from 800 respondents to 46 million Spaniards? Surely this is full of hot air.

When researching a large population (over a few thousands), you rarely need more than a thousand respondents to achieve a margin of error under 3%. It means that if you were to retake the survey with a totally different sample, 9 times out of 10, the answers you'll get will be within a 3% interval of the results you had the first time around. Statistics are a powerful thing, and sample sizes are rarely to blame in dodgy surveys.

_Drinking tea lowers the risk of stroke_

Articles about the benefits of tea-drinking are commonplace. This short http://www.welt.de/print-welt/article249051/Tee_schuetzt_vor_Herzinfarkt.html[item in Die Welt] saying that tea lowers the risk of myocardial infarction is no exception. Although the effects of tea are seriously studied by some, many pieces of research fail to take into account lifestyle factors, such as diet, occupation or sports.

In most countries, tea is a beverage for the health-conscious upper classes. If researchers don't control for lifestyle factors in tea studies, they tell us nothing more than `rich people are healthier - and they probably drink tea'.

_What you can do_

The math behind correlations and error margins in the tea studies are certainly correct, at least most of the time. But if researchers don't look for co-correlations (e.g. drinking tea correlates with doing sports), their results are of little value.

As a journalist, it makes little sense to challenge the numerical results of a study, such as the sample size, unless there are serious doubts about it. However, it is easy to see if researchers failed to take into account relevant pieces of information.

=== Tips for Working with Numbers in the News (Michael Blastland, BBC) ===

1. The best tip for handling data is to enjoy yourself. Data can appear forbidding. But allow it to intimidate you and you'll get nowhere. Treat it as something to play with and explore and it will often yield secrets and stories with surprising ease. So handle it simply as you'd handle other evidence, without fear or favour. In particular, think of this as an exercise in imagination. Be creative by thinking of the alternative stories that might be consistent with the data and explain it better, then test them against more evidence. `What other story could explain this?' is a handy prompt to think about how this number, this obviously big or bad number, this clear proof of this or that, might be nothing of the sort.

2. Don't confuse scepticism about data with cynicism. Scepticism is good; cynicism has simply thrown up its hands and quit. If you believe in data journalism, and you probably do or you wouldn't be reading this book, then you must believe that data has something far better to offer than the lies and damned lies of caricature or the killer facts of swivel-eyed headlines. Data often give us profound knowledge, if used carefully. We need to be neither cynical nor naive, but alert.

3. If I tell you that drinking has gone up during the recession, you might tell me it's because everyone is depressed. If I tell you that drinking is down, you might tell me it's because everyone is broke. In other words, what the data says makes no difference to the interpretation that you are determined to put on it, namely that things are terrible one way or the other. If it goes up, it's bad, if it goes down, it's bad. The point here is that if you believe in data, try to let it speak before you slap on your own mood, beliefs or expectations. There's so much data about that you will often be able to find confirmation of your prior beliefs if you simply look around a bit. In other words, data journalism, to me at least, adds little value if you are not open-minded. It is only as objective as you strive to make it, and not by virtue of being based on numbers.

4. Uncertainty is ok. We associate numbers with authority and certainty. Often as not, the answer is that there is no answer, or the answer may be the best we have but still wouldn't hit a barn door for accuracy. I think we should say these things. If that sounds like a good way of killing stories, I'd argue that it's a great way of raising new questions. Equally, there can often be more than one legitimate way of cutting the data. Numbers don't have to be either true or false.

5. The investigation is a story. The story of how you tried to find out can make great journalism, as you go from one piece of evidence to another - and this applies in spades to the evidence from data, where one number will seldom do. Different sources provide new angles, new ideas, richer understanding. I wonder if we're too hung up on wanting to be authoritative and tell people the answer - and so we miss a trick by not showing the sleuthing.

6. The best questions are the old ones: is that really a big number? Where did it come from? Are you sure it counts what you think it counts? These are generally just prompts to think around the data, the stuff at the edges that got squeezed by looking at a single number, the real-life complications, the wide range of other potential comparisons over time, group or geography; in short, context.

=== Basic Steps in Working with Data (Steve Doig, Walter Cronkite School of Journalism) ===

There are at least three key concepts you need to understand when starting a data project:

  * Data requests should begin with a list of questions you want to answer.
  * Data often is messy and needs to be cleaned.
  * Data may have undocumented features

*Know the Questions You Want to Answer*

In many ways, working with data is like interviewing a live source. You ask questions of the data and get it to reveal the answers. But just as a source can only give answers about which he or she has information, a data set can only answer questions for which it has the right records and the proper variables. This means that you should consider carefully what questions you need to answer even before you acquire your data. Basically, you work backwards. First, list the data-evidenced statements you want to make in your story. Then decide which variables and records you would have to acquire and analyse in order to make those statements.

Consider an example involving local crime reports. Let's say you want to do a story looking at crime patterns in your city, and the statements you want to make involve the times of day and the days of a week in which different kinds of crimes are most likely to happen, as well as what parts of town are hot spots for various crime categories.

You would realize that your data request has to include the date and the time each crime was reported, the kind of crime (murder, theft, burglary, etc.) as well as the address of where the crime occurred. So Date, Time, Crime Category and Address are the minimum variables you need to answer those questions.

But be aware that there are a number of potentially interesting questions that this four-variable data set CAN'T answer, like the race and gender of victims, or the total value of stolen property, or which officers are most productive in making arrests. Also, you may only be able to get records for a certain time period, like the past three years, which would mean you couldn't say anything about whether crime patterns have changed over a longer period of time. Those questions may be outside of the planned purview of your story, and that's fine. But you don't want to get into your data analysis and suddenly decide you need to know what percent of crimes in different parts of town are solved by arrest.

One lesson here is that it's often a good idea to request ALL the variables and records in the database, rather than the subset that could answer the questions for the immediate story. (In fact, getting all the data can be cheaper than getting a subset, if you have to pay the agency for the programming necessary to write out the subset.) You can always subset the data on your own, and having access to the full data set will let you answer new questions that may come up in your reporting and even produce new ideas for follow-up stories. It may be that confidentiality laws or other policies mean that some variables, such as the identities of victims or the names of confidential informants, can't be released. But even a partial database is much better than none, as long as you understand which questions the redacted database can - and - can't answer.

*Cleaning Messy Data*

One of the biggest problems in database work is that often you will be using for analysis reasons data that has been gathered for bureaucratic reasons. The problem is that the standard of accuracy for those two is quite different.

For example, a key function of a criminal justice system database is to make sure that defendant Jones is brought from the jail to be in front of Judge Smith at the time of his hearing. For that purpose, it really doesn't matter a lot if Jones' birth date is incorrect, or that his street address is misspelled, or even if his middle initial is wrong. Generally, the system still can use this imperfect record to get Jones to Smith's courtroom at the appointed time.

But such errors can skew a data journalist's attempts to discover the patterns in the database. For that reason, the first big piece of work to undertake when you acquire a new data set is to examine how messy it is and then clean it up. A good quick way to look for messiness is to create frequency tables of the categorical variables, the ones that would be expected to have a relatively small number of different values. (When using Excel, for instance, you can do this by using Filter or Pivot Tables on each categorical variable.)

Take ``Gender'', an easy example. You may discover that your Gender field includes any of a mix of values like these: Male, Female, M, F, 1, 0, MALE, FEMALE, etc., including misspellings like `Femal'. To do a proper gender analysis, you must standardise – Decide on M and F, perhaps, and then change all the variations to match the standards. Another common database with these kinds of problems are American campaign finance records, where the Occupation field might list ``Lawyer'', ``Attorney'', ``Atty'', ``Counsel'', ``Trial Lawyer'' and any of a wealth of variations and misspellings; again, the trick is to standardise the occupation titles into a shorter list of possibilities.

Data cleanup gets even more problematic when working with names. Are ``Joseph T. Smith'', ``Joseph Smith'', ``J.T. Smith'', ``Jos. Smith'' and ``Joe Smith'' all the same person? It may take looking at other variables like address or date of birth, or even deeper research in other records, to decide. But tools like Google Refine can make the cleanup and standardisation task faster and less tedious.

*Data May Have Undocumented Features*

The Rosetta Stone of any database is the so-called data dictionary. Typically, this file (it may be text or PDF or even a spreadsheet) will tell you how the data file is formatted (delimited text, fixed width text, Excel, dBase, et al.), the order of the variables, the names of each variable and the datatype of each variable (text string, integer, decimal, et al.) You will use this information to help you properly import the data file into the analysis software you intend to use (Excel, Access, SPSS, Fusion Tables, any of various flavours of SQL, et al.)

The other key element of a data dictionary is an explanation of any codes being used by particular variables. For instance, Gender may be coded so that `1=Male' and `0=Female'. Crimes may be coded by your jurisdiction's statute numbers for each kind of crime. Hospital treatment records may use any of hundreds of 5-digit codes for the diagnoses of the conditions for which a patient is being treated. Without the data dictionary, these data sets could be difficult or even impossible to analyse properly.

But even with a data dictionary in hand, there can be problems. An example happened to reporters at the Miami Herald in Florida some years ago when they were doing an analysis of the varying rates of punishment that different judges were giving to people arrested for driving while intoxicated. The reporters acquired the conviction records from the court system and analysed the numbers in the three different punishment variables in the data dictionary: amount of prison time given, amount of jail time given, and amount of fine given. These numbers varied quite a bit amongst the judges, giving the reporters' evidence for a story about how some judges were harsh and some were lenient.

But for every judge, about 1-2 percent of the cases showed no prison time, no jail time and no fine. So the chart showing the sentencing patterns for each judge included a tiny amount of cases as ``No punishment,'' almost as an afterthought. When the story and chart was printed, the judges howled in complaint, saying the Herald was accusing them of breaking a state law that required that anyone convicted of drunk driving be punished.

So the reporters went back to Clerk of the Court's office that had produced the data file and asked what had caused this error. They were told that the cases in question involved indigent defendants with first-time arrests. Normally they would be given a fine, but they had no money. So the judges were sentencing them to community service, such as cleaning litter along the roads. As it turned out, the law requiring punishment had been passed after the database structure had been created. So all the court clerks knew that in the data, zeros in each of the prison-jail-fine variables meant community service. However, this WASN'T noted in the data dictionary, and therefore caused a Herald correction to be written.

The lesson in this case is to always ask the agency giving you data if there are any undocumented elements in the data, whether it is newly-created codes that haven't been included in the data dictionary, changes in the file layout, or anything else. Also, always examine the results of your analysis and ask ``Does this make sense?'' The Herald reporters were building the chart on deadline and were so focused on the average punishment levels of each judge that they failed to pay attention to the scant few cases that seemed to show no punishment. They should have asked themselves if it made sense that all the judges seemed to be violating state law, even if only to a tiny degree.

=== The £32 Loaf of Bread (Claire Miller, Wales Online) ===

image::Figures/05-AA.png[width=600]

A story for Wales on Sunday about how much the Welsh Government is spending on prescriptions for gluten-free products, contained the http://www.walesonline.co.uk/news/wales-news/2011/07/17/prescriptions-for-gluten-free-bread-costing-welsh-taxpayers-32-a-loaf-91466-29067430/[headline figure] that it was paying £32 for a loaf of bread. However, this was actually 11 loaves that cost £2.82 each.

The figures, from a Welsh Assembly written answer and a Welsh NHS statistics release, listed the figure as cost per prescription item. However, they gave no additional definition in the data dictionary of what a prescription item might refer or how a separate quantity column might define it.

The assumption was that it referred to an individual item, e.g. a loaf of bread, rather than what it actually was, a pack of several loafs.

No one, not the people who answered the written answer or the press office, when it was put to them, raised the issue about quantity until the Monday after the story was published.

So do not assume the people responsible for the data will realise the data is not clear even when you tell them your mistaken assumption.

Generally newspapers want things that make good headlines, so unless something obviously contradicts an interpretation, it is usually easier to go with what makes a good headline and not check too closely and risk the story collapsing, especially on deadline.

But journalists have a responsibility to check the ridiculous claims even if it means that this drops the story down the news list.

=== Start With the Data, Finish With a Story (Caelainn Barr, EU Data Journalist) ===

*Approach 1: Think Headline*

To draw your readers in you have to be able to hit them with a headline figure that makes them sit up and take notice. You should almost be able to read the story without having to know that it comes from a dataset. Make it exciting and remember who your audience are as you go.

One example of this can be found in a project carried out by the Bureau of Investigative Journalism using the EU Commission's http://ec.europa.eu/beneficiaries/fts/index_en.htm[Financial Transparency System]. The story was constructed by approaching the data set with specific queries in mind.

We looked through the data for key terms like `cocktail', `golf' and `away days'. This allowed us to determine what the Commission had spent on these items and raised plenty of questions and story lines to follow up.

But key terms don't always give you what you want, sometimes you have to sit back and think about what you're really asking for. During this project we also wanted to find out how much Commissioners spent on private jet travel but the as the data set didn't contain the phrase `private jet' we had to get the name of their travel providers by other means. Once we knew the name of the service provider to the Commission, `Abelag', we were able to query the data to find out how much was being spent on services provided by Abelag.

With this approach we had a clearly defined objective in querying the data; to find a figure that would provide a headline - the colour followed.

*Approach 2: Approach with a Blacklist and Look for Exclusions*

An easy way to pull storylines from data is to know what you shouldn't find in there! A good example of how this can work is illustrated by the collaborative EU Structural Funds project between the Financial Times and the Bureau of Investigative Journalism.

We queried the data, based on the Commission's own rules about what kinds of companies and associations should be prohibited from receiving structural funds. One example was expenditure on tobacco and tobacco producers.

By querying the data with the names of tobacco companies, producers and growers we found data that revealed British American Tobacco were receiving €1.5m for a factory in Germany.

As the funding was outside the rules of Commission expenditure, it was a quick way to find a story in the data.

*Approach 3: Just Explore*

You never know what you might find in a dataset, so just have a look. You have to be quite bold and this approach generally works best when trying to identify obvious characteristics that will show up through filtering (the biggest, extremes, most common etc.).

=== Data Stories (Martin Rosenbaum, BBC) ===

Data journalism can sometimes give the impression that it is mainly about presentation of data - such as visualisations which quickly and powerfully convey an understanding of an aspect of the figures, or interactive searchable databases which allow individuals to look up say their own local street or hospital. All this can be very valuable, but like other forms of journalism, data journalism should also be about stories. So what are the kinds of stories you can find in data? Based on my experience at the BBC, I have drawn up a list or `typology' of different kinds of data stories.

I think it helps to bear this list below in mind, not only when you are analysing data, but also at the stage before that, when you are collecting it (whether looking for publicly available datasets or compiling freedom of information requests).

*1. Measurement*

The simplest story - counting or totalling something.

`Local councils across the country spent a total of £x billion on paper clips last year'

But it's often difficult to know if that's a lot or a little. For that, you need context - which can be provided by:

*2. Proportion*

`Last year local councils spent two-thirds of their stationery budget on paper clips'

Or

*3. Internal comparison*

`Local councils spend more on paper clips than on providing meals-on-wheels for the elderly'

Or

*4. External comparison*

`Council spending on paper clips last year was twice the nation's overseas aid budget'

Or there are other ways of exploring the data in a contextual or comparative way:

*5. Change over time*

`Council spending on paper clips has trebled in the past four years'

Or

*6. `League tables'*

These are often geographical or by institution, and you must make sure the basis for comparison is fair, e.g. taking into account the size of the local population.

`Borsetshire Council spends more on paper clips for each member of staff than any other local authority, at a rate four times the national average'

Or you can divide the data subjects into groups:

*7. Analysis by categories*

`Councils run by the Purple Party spend 50% more on paper clips than those controlled by the Yellow Party'

Or you can relate factors numerically

*8. Association*

`Councils run by politicians who have received donations from stationery companies spend more on paper clips, with spending increasing on average by £100 for each pound donated'

But, of course, always remember that correlation and causation are not the same thing.

So if you're investigating paper clip spending, are you also getting the following figures:

  * Total spending to provide context?
  * Geographical/historical/other breakdowns to provide comparative data?
  * The additional data you need to ensure comparisons are fair, such as population size?
  * Other data which might provide interesting analysis to compare or relate the spending to?

=== Data Journalists Discuss Their Tools of Choice (Various Contributors) ===

////

Insert: picture of lots of tools arranged on screen, ready to go

////

Psssss. That is the sound of your data decompressing from its airtight wrapper. Now what? What do you look for? And what tools do you use to get stuck in? We asked data journalists to tell us a bit about how they work with data. Here is what they said.

*Brian Boyer, Chicago Tribune*

____
Our tools of choice include Python and Django. For hacking, scraping and playing with data, and PostGIS, QGIS and the MapBox toolkit for building crazy web maps. R and NumPy + MatPlotLib are currently battling for supremacy as our kit of choice for exploratory data analysis, though our favorite data tool of late is homegrown: CSVKit. More or less everything we do is deployed in the cloud.
____

=== Using Data Visualisation to Find Insights in Data (Gregor Aisch, Open Knowledge Foundation) ===

[quote, William S. Cleveland, Visualising Data]
____
Visualization is critical to data analysis. It provides a front line of attack, revealing intricate structure in data that cannot be absorbed in any other way. We discover unimagined effects, and we challenge imagined ones.
____

*Everything is Visualised*

Data by itself, consisting of bits and bytes stored in a file on a computer hard-drive, is invisible. In order to be able to see and make any sense of data, we need to visualise it. In this chapter I'm going to use a broader understanding of the term visualising, that includes even pure textual representations of data. For instance, just loading a dataset into a spreadsheet software can be considered as data visualisation. The invisible data suddenly turns into a visible 'picture' on our screen. Thus, the questions should not be whether journalists need to visualise data or not, but which kind of visualisation may be the most useful in which situation. In other words: when does it makes sense to go beyond the table visualisation.

The short answer is: almost always. Tables alone are definitely not sufficient to give us an overview of a dataset. And tables alone don't allow us to immediately identify patterns within the data. The most common example here are geographical patterns which can only be observed after visualising data on a map. But there are also other kinds of patterns which we will see later in this chapter.

*Using Visualisation to Discover Insights*

It is unrealistic to expect that data visualisation tools and techniques will unleash a barrage of ready-made stories from datasets. There are no rules, no `protocol' that will guarantee us a story. Instead, I think it makes more sense to look for `insights', which can be artfully woven into stories in the hands of a good journalist.

Every new visualisation is likely to give us some insights into our data. Some of those insights might be already known (but perhaps not yet proven) while other insights might be completely new or even surprising to us. Some new insights might mean the beginning of a story, while others could just be the result of errors in the data.

In order to make the finding of insights in data more effective, I find the following process very helpful:

image::Figures/05-BB.png[width=600]

Each of these steps will be discussed further in this section.

*Visualise Data*

Visualising data is the central point of this process. Each visualisation provides a unique perspective on the dataset. Types of visualisations can include:

  * Tables - Displaying the raw numbers.
  * Charts - See http://www.flickr.com/photos/amit-agarwal/3196386402/sizes/l/[this diagram] for some common examples.
  * Maps - Displaying geographical context.
  * Graphs - Displaying relations in networks.

*Analyse and interpret what you see*

Once you have visualised your data, the next step is to learn something from the picture you created. You could ask yourself:

  * What can I see in this image? Are there any patterns?
  * What does this mean in the context of the data?

Sometimes you might end up with visualisation that, in spite of its beauty, might seem to tell you nothing of interest about your data. But there is almost always _something_ that you can learn from any visualisation, however trivial.

*Document Your Insights and Steps*

If you think of this process as a journey through the dataset, the documentation is your travel diary. It will tell you where you have traveled to, what you have seen there and how you made your decisions for your next steps. You can even start your documentation before taking your first look at the data.

In most cases when we start to work with a previously unseen dataset we are already full of expectations and assumptions about the data. Usually there is a reason why we are interested that dataset that we are looking at. It's a good idea to start the documentation by writing down these initial thoughts. This helps us to identify our bias and reduces the risk of mis-interpretation of the data by just finding what we originally wanted to find.

I really think that the documentation is the most important step of the process; and it is also the one we're most likely to tend to skip. As you will see in the example below, the described process involves a lot of plotting and data wrangling. Looking at a set of 15 charts you created might be very confusing, especially after some time has passed. In fact, those charts are only valuable (to you or any other person you want to communicate your findings) if presented in the context in which they have been created. Hence you should take the time to make some notes on things like

  * Why have I created this chart?
  * What have I done to the data to create it?
  * What does this chart tell me?

*Transform Data*

Naturally, with the insights that you have gathered from the last visualisation you might have an idea of what you want to see next. You might have found some interesting pattern in the dataset which you now want to inspect in more detail.

Possible transformations are:

  * zoom into a. (e.g. limit to a certain time span)
  * filter
  * aggregate
  * outlier removal

Now we can really get going. The good news is that we've already solved the hardest question: which direction should we move in? Remember that insight you got from the last visualisation? The one you've written down in you story documentation? The next step is trying to remove this insight from our data. The theory behind this is that every insight in your data might prevent us from seeing some other insights. Removing something we already know is no problem, as long as we're keeping everything for our records. And this is what I call the transform step.

*Which Tools to Use*

Every data visualisation tool available is good at something. However, here are a few requirements for choosing the right tools:

  * Visualisation and data wrangling should be easy and cheap. If changing parameters of the visualisations takes you hours, you won't experiment that much. That doesn't necessarily mean that you don't need to learn how to use the tool. But once you learned it, it should be really efficient.
  * It often makes a lot of sense to choose a tool that covers both the data wrangling and the data visualisation issues. Separating the tasks in different tools means that you have to import and export your data very often.

The sample visualisations in the next section were created using the R, which is kind of the swiss army knife of scientific data visualisation.

*Example: Making Sense of US Election Contribution Data*

Let us have look at the US Presidential Campaign Finance database which contains about 450,000 contributions to US Presidential candidates. The CSV file is 60 megabytes and way too big to handle easily in a programme like Excel.

In the first step I will explicitly write down my initial assumptions on the FEC contributions dataset:

  * Obama gets the most contributions (since he is the president and has the greatest popularity)
  * The number of donations increases as the time moves closer to election date.
  * Obama gets more small donations than Republican candidates.

To answer the first question we need to _transform_ the data. Instead of each single contributions we need to sum the total amounts contributed to each candidate. After _visualising_ the results in a sorted table we can confirm our assumption that Obama would raise the most money.

[options="header"]
|=======================
|Candidate | Amount ($)
|Obama, Barack | 72,453,620.39
|Romney, Mitt | 50,372,334.87
|Perry, Rick | 18,529,490.47
|Paul, Ron | 11,844,361.96
|Cain, Herman | 7,010,445.99
|Gingrich, Newt | 6,311,193.03
|Pawlenty, Timothy | 4,202,769.03
|Huntsman, Jon | 2,955,726.98
|Bachmann, Michelle | 2,607,916.06
|Santorum, Rick | 1,413,552.45
|Johnson, Gary Earl | 413,276.89
|Roemer, Charles E. 'Buddy' III | 291,218.80
|McCotter, Thaddeus G | 37,030.00
|=======================

But despite of showing the minimum and maximum amounts and the order the table does not tell very much about the underlying patterns in candidate ranking. Here is another view on the data, a chart type that is called _dot chart_ in which we can see everything that is shown in the table _plus_ the patterns within the field. For instance, the dot chart allows us to immediately compare the distance between Obama and Romney and Romney and Perry without needing to subtract values. (Footnote: The shown dot chart was created using R. You can find links to the source codes at the end of this chapter).

image::Figures/05-CC.png[width=600]

Now, let us proceed with a bigger picture of the dataset. As a first step I _visualised_ all contributed amounts over time in a simple plot. We can see that almost all donations are very very small compared to three really big outliers. Further investigation returns that these huge contribution are coming from the ``Obama Victory Fund 2012'' (also known as Super PAC) and were made on June 29th ($450k), September 29th ($1.5mio) and December 30th ($1.9mio).

image::Figures/05-DD.png[width=600]

While the contributions by Super PACs alone is undoubtedly the biggest story in the data, it might be also interesting to look beyond it. The point now is that these big contributions disturb our view on the smaller contributions coming from individuals, so we're going to remove them from the data. This transform is commonly known as outlier removal. After visualizing again, we can see that most of the donations are within the range of $10k and -$5k.

image::Figures/05-EE.png[width=600]

According to the contribution limits placed by the FECA individuals are not allowed to donate more than $2500 to each candidate. As we see in the plot, there are numerous donations made above that limit. In particular two big contributions in May attract our attention. It seems that they are 'mirrored' in negative amounts (refunds) June and July. Further investigation in the data reveals the following transactions:

  * On May 10 _Stephen James Davis_, San Francisco, employed at Banneker Partners (attorney), has donated *$25,800* to Obama.
  * On May 25 _Cynthia Murphy_, Little Rock, employed at the Murphy Group (public relations), has donated *$33,300* to Obama.
  * On June 15 the amount of *$30,800* was refunded to _Cynthia Murphy_, which reduced the donated amount to *$2500*.
  * On July 8 the amount *$25,800* was refunded to _Stephen James Davis_, which reduced the donated amount to $0.

What's interesting about these numbers? The $30,800 refunded to Cynthia Murphy equals the maximum amount individuals may give to national party committees per year. Maybe she just wanted to combine both donations in one transaction which was rejected. The $25,800 refunded to Stephen James Davis possibly equals the $30,800 minus $5000 (the contribution limit to any other political committee).

Another interesting finding in the last plot is a horizontal line pattern for contributions to Republican candidates at $5000 and -$2500. To see them in more detail, I visualised just the Republican donations. The resulting graphic is kind of the perfect example for patterns in data that would be invisible without data visualisation.

image::Figures/05-FF.png[width=600]

What we can see is that there are many $5000 donations to Republican candidates. In fact, a look up in the data returns that these are 1243 donations, which is only 0.3% of the total number of donations, but since those donations are evenly spread across time, the line appears. The interesting thing about the line is that donations by individuals were limited to $2500. Consequently, every dollar above that limited was refunded to the donors, which results in the second line pattern at -$2500. In contrast, the contributions to Barack Obama don't show a similar pattern.

image::Figures/05-GG.png[width=600]

So, it might be interesting to find out why thousands of Republican donors did not notice the donation limit for individuals. To further analyze this topic, we can have a look at the total number of $5k donations per candidate.

image::Figures/05-HH.png[width=600]

Of course, this is a rather distorted view since it does not consider the total amounts of donations received by each candidate. The next plot shows the percentage of $5k donations per candidate.

image::Figures/05-II.png[width=600]

*What To Learn From This*

Often, such a visual analysis of a new dataset feels like an exciting journey to an unknown country. You start as a foreigner with just the data and your assumptions, but with every step you make, with every chart you render, you get new insights about the topic. Based on those insights you make decisions for your next steps and what issues are worth further investigation. As you might have seen in this chapter, this process of visualizing, analyzing and transformation of data could be repeated nearly infinitely.

*Get the Source Code*

All of the charts shown in this chapter were created using the wonderful and powerful software R. Created mainly as a scientific visualization tool, it is hard to find any visualization or data wrangling technique that is not already built into R. For those who are interested in how to visualise and wrangle data using R, here's the source code of the charts generated in this chapter. Also, there is a wide range of books and tutorials available.

  * https://gist.github.com/1769733[dotchart: contributions per candidate]
  * https://gist.github.com/1816161[plot: all contributions over time]
  * https://gist.github.com/1816169[plot: contributions by authorized committees]