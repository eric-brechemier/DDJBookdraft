== 2. In The Newsroom ==

=== How the News Apps Team at Chicago Tribune Works (Brian Boyer, Chicago Tribune) ===

////

321 words

Comments:

Short and sweet.

No suggested changes, except minor formatting. Ready to send back to him to revise/approve.

Nice how it is, but there could be more detail, if Brian has anything to add. E.g. perhaps give a few examples of data-driven apps that CT has done, how these differed from other forms of reporting, what they brought to the newsroom that was new. Also could cover what other people in the newsroom originally thought about the apps team, how it got started, etc.

Todo:

ask Brian for a nice picture of Chicago Tribune hackers. Perhaps of 5 minute standup meeting - or of developers programming in pairs?

////

The news applications team at the Chicago Tribune is a band of happy hackers embedded in the newsroom. We work closely with editors and reporters to help: (1) research and report stories, (2) illustrate stories online and (3) build evergreen web resources for the fine people of Chicagoland.

It's important that we sit in the newsroom. We usually find work via face-to-face conversations with reporters. They know that we're happy to help write a screen scraper for a crummy government website, tear up a stack of PDFs, or otherwise turn non-data into something you can analyze. It's sort of our team's loss leader -- this way we find out about potential data projects at their outset.

Unlike many teams in this field, our team was founded by technologists for whom journalism was a career change. Some of us acquired a masters degree in journalism after several years coding for business purposes, and others were borrowed from the open government community.

We work in an agile fashion. To make sure we're always in sync, every morning begins with a 5-minute stand up meeting. We frequently program in pairs -- two developers at one keyboard are often more productive than two developers at two keyboards. Most projects don't take more than a week to produce, but on longer projects we work in week-long iterations, and show our work to stakeholders -- reporters and editors usually -- every week. "Fail fast" is the mantra. If you're doing it wrong, you need to know as soon as possible, especially when you're coding on a deadline!

There's a tremendous upside to hacking iteratively, on a deadline: We're always updating our toolkit. Every week we crank out an app or two, then, unlike normal software shops, we can put it to the back of our mind and move on to the next project. It's a joy we share with the reporters, every week we learn something new.

===  The ABC's Data Journalism Play (Wendy Carlisle, Australian Broadcasting Corporation) ===

////

1134 words

////


Now in its 70th year the Australian Broadcasting Corporation is Australia's national public broadcaster. Annual funding is around AUS$1bn which delivers seven radio networks, 60 local radio stations, three digital television services, a new international television service and an online platform to deliver this ever expanding offering of digital and user generated content. At last count there were in excess of 4,500 full time equivalent staff and nearly 70% of them make content.

We are a national broadcaster fiercely proud of our independence - because although funded by government - we are separated at arm's length through law. Our traditions are independent public service journalism. The ABC is regarded the most trusted news organisation in the country.

These are exciting times  and under a managing director - the former newspaper executive Mark Scott - content makers at the ABC have been encouraged to  as the corporate mantra puts it - be `agile'.

Of course that's easier said than done. 

But one initiative in recent times designed to encourage this has been  a competitive staff pitch for money to develop multi-platform projects.

This is how the ABC's first ever data journalism project was conceived.

Sometime early in 2010 I wandered into the pitch session to face with three senior `ideas' people with my proposal.

I'd been chewing it over for some time. Greedily lapping up the data journalism that the now legendary Guardian data journalism blog was offering, and that was just for starters.

It was my argument that no doubt within 5 years the ABC would have its own data journalism unit. It was inevitable, I opined. But the question was how are we going to get there, and whose going to start.

For those readers unfamiliar with the ABC  think of a vast bureaucracy built up over 70 years. Its primary offering was always radio and television. With the advent of online in the last decade this content offering unfurled into text, stills and a degree of interactivity previously unimagined. The web space was forcing the ABC to rethink how it cut the cake (money) and rethink what kind of cake it was baking (content).

It is of course a work in progress. 

But something else was happening with data journalism. Government 2.0 (which as we discovered is largely observed in the breach in Australia) was starting to offer new ways of telling stories that were hitherto buried in the zero's and dots.

All this I said to the folk during my pitch. I also said we needed to identify new skills sets, train journalists in new tools. We needed a project to hit play. 

And they gave me the money.

On the 24th of November 2011 the ABC's multiplatform project and ABC News Online went live with http://www.abc.net.au/news/specials/coal-seam-gas-by-the-numbers/promise/[`Coal Seam Gas by the Numbers'].

image::Figures/02-01.png[width=600]

It was five pages of interactive maps, data visualisations and text. 

It wasn't exclusively data journalism - but a hybrid of journalisms that was born of the mix of people on the team and the story, which to put in context is raging as one of the hottest issues in Australia.  

The jewel was an interactive map showing coal seam gas wells and leases in Australia. Users could search by location and switch between modes to show leases or wells. By zooming in users could see who the explorer was, the status of the well and its drill date. Another map showed the location of coal Seam gas activity compared to the location of groundwater systems in Australia. 

image::Figures/02-02.png[width=600]

We had data visualisations which specifically addressed this issue of waste salt and water production that would be produced depending on the scenario that emerged.   

Another section of the project investigated the release of chemicals into a local river system

*Our team*

  * A web developer and designer. 
  * A lead journalist  
  * A part time researcher with expertise in data extraction, excel spread sheets and data cleaning
  * A part time junior journalist.
  * A consultant executive producer
  * A academic consultant with expertise in data mining, graphic visualisation and advanced research skills.
  * The services of a project manager and the administrative assistance of the ABC's multi-platform unit.
  * Importantly we also had a reference group of journalists and others whom we consulted on a needs basis.

*Where did we get the data from?*

The data for the interactive maps were scrapped from shapefiles (a common kind of file for geospatial data) downloaded from government websites. 

Other data on salt and water were taken from a variety of reports.

The data on chemical releases was taken from Environmental permits issued by the government. 

*What did we learn?*

`Coal Seam Gas by the Numbers' was an ambitious in content and scale.  Uppermost in my mind was what did we learn and how might we do it differently next time?

The data journalism project brought a lot of people into the room who do not normally meet at the ABC. In lay terms - the hacks and the hackers. Many of us did not speak the same language or even appreciate what the other does.  Data journalism is disruptive!   

The practical things:

  * Co-location of the team is vital. Our developer and designer were off-site and came in for meetings. This is definitely not optimal! Place in the same room as the journalists. 
  * Our consultant EP was also on another level of the building. We needed to be much closer, just for the drop-by factor
  * Choose a story that is solely data driven.

**The big picture: some ideas**

  * Big media organisations need to engage in capacity building to meet the challenges of data journalism. My hunch is there are a lot of geeks and hackers hiding in media technical departments desperate to get out. So -we need `hack and hacker meets' workshops where the secret geeks, younger journalists, web developers and designers come out to play with more experienced journalists for skill sharing and mentoring. Task: download this data set and go for it! 
  * Ipso facto Data journalism is interdisciplinary. Data journalism teams are made of people who would not in the past have worked together. The digital space has blurred the boundaries.
  * We live in a fractured, distrustful body politic.  The digital space makes everyone a content maker: we can all be journalists now, right?  Wrong. Journalists need to reassert themselves as ethical, trustworthy, honest story tellers. Data journalism offers the opportunity to turn  the signal to noise ratio into something we can all understand. Journalists are going to need to be literate in data journalism tools to
  * Data journalism is still all about story telling.
  * Increasingly the journalists of tomorrow will be data journalists.
  * Australia is behind Europe and the United States in data journalism. Why? That's another discussion.

////
Wendy Carlisle has been an ABC journalist for 20 years and is primarily an investigative reporter working with radio's investigative program 'Background Briefing' and the Four Corners program on ABC TV.  She was the lead journalist on 'Coal Seam gas: by the Numbers' 
Background Briefing http://www.abc.net.au/radionational/programs/backgroundbriefing/
Four Corners http://www.abc.net.au/4corners/
////

=== Data Journalism at the Zeit Online (Sascha Venohr, Zeit Online) ===

////

364 words

Comments:

This is good, but this is currently a bit too focused on one example to fit seamlessly into this section. Need more on how data journalism fits 'in the newsroom' of Zeit Online as a whole.

Could we have a bit more detail on things like:

  * Backround - what made Zeit Online interested in data journalism, who pushed for it, how they got started, where the budget came from, how others at Zeit Online felt about this
  * Mention of other data journalism projects at Zeit Online
  * Current status of data journalism within Zeit Online
  * Other comments/tips that could be useful/relevant to other news organisations getting started with data journalism, e.g. on how to pitch this, what its value is, why/how to pursue it.

////

image::Figures/02-03.png[width=600]

The http://opendata.zeit.de/pisa-wohlstands-vergleich/visualisierung.php#/en/DEU-OECD[PISA based Wealth Comparison] project is an interactive visualisation that enables comparison of standards of living in different countries. The interactive uses data from the OECD's comprehensive world education ranking report, http://en.wikipedia.org/wiki/Programme_for_International_Student_Assessment[PISA 2009], published in December 2010. The report is based on a questionnaire which asks fifteen-year-old pupils about their living situation at home.

The idea was to analyse and visualise this data to provide a unique way of comparing standards of living in different countries.

First of all our in-house editorial team decided which facts seemed to be useful to make living standards comparable and should be visualised, including:

  * Wealth (number of owned TVs, cars and available bathrooms at home)
  * Family-situation (are there grandparents living with the family together, percentage share of families with only one child, unemployment of parents and mother's job status)
  * Access to knowledge sources (internet at home, frequency of using e-mail and quantity of owned books)
  * Three additional indicators on the level of development of each country.

With the help of the internal design team these facts were translated into self-explanatory icons. A front end design was built to make comparison between the different countries like in a card-game possible.

Next we contacted people from the German http://opendata-network.org/[German Open Data Network] to find developers who could help with the project. This community of highly motivated people suggested Gregor Aisch, a very talented information designer, to code the application that would make our dreams come true (without using flash - which was very important to us!). Gregor created a very high quality and interactive visualisation with a beautiful bubble-style, based on the http://raphaeljs.com/[Raphaël-Javascript Library].

The result of our hand in hand work was a very successful interactive which got a lot of traffic. It is easy to compare any two countries, which makes it useful as a reference tool. This means that we can re-use it in our daily editorial work. For example if we are covering something related to the living situation in Indonesia, we can quickly and easily embed a graphic http://opendata.zeit.de/pisa-wohlstands-vergleich/visualisierung.php#/en/DEU-IDN[comparing the living situation in Indonesia and Germany]. The know-how transferred to our in house-team was a great investment for future projects.

=== Data Journalism at the Texas Tribune (TBC) ===

Waiting for a reply.

=== How to Hire a Hacker (Lucy Chambers, Open Knowledge Foundation) ===

////

430 words

////


One of the things that I am regularly asked by journalists is 'how do I get a coder to help me with my project?'. Don't be deceived into thinking this is a one-way process; civic-minded hackers and data-wranglers are often just as keen to get in touch with journalists. 

Journalists are power-users of data driven tools and services. From the perspective of developers: journalists think outside the box to use data tools in contexts developers haven't always considered before (feedback is invaluable!) they also help to build context and buzz around projects and help to make them relevant. It is a symbiotic relationship.

Fortunately, this means that whether you are looking to hire a hacker or looking for possible collaborations on a shoestring budget, there will more than likely be someone out there who is interested in helping you.

So how do you find them? Says Aron Pilhofer from the New York Times:


[quote]
____
You may find that your organisation already has people with all the skills you need, but they are not necessarily already in your newsroom. Wander around, visit the technology and IT departments and you are likely to strike gold. It is also important to appreciate coder culture, come across someone who has a computer that looks like this...

image::Figures/02-04.jpg[width=600]

...then you are probably onto a winner.

____

Here are a few more ideas:

  * *Post on job websites*. Identify and post to websites aimed at developers who work in different programming languages. For example, the http://www.python.org/community/jobs/[Python Job Board].
  * *Contact relevant mailing lists*. For example, the http://www.ire.org/resource-center/listservs/subscribe-nicar-l/[NICAR-L] and http://lists.okfn.org/mailman/listinfo/data-driven-journalism[Data Driven Journalism] mailing lists.
  * *Contact relevant organisations*. For example, if you want to clean up or scrape data from the web, you could contact an organisation such as https://scraperwiki.com/[Scraperwiki], who have a great address book of trusted and willing coders.
  * *Join relevant groups/networks*. Look out for initiatives such as http://hackshackers.com/[Hacks/Hackers] which bring journalists and techies together. Hacks/Hackers groups are now springing up all around the world. You could also try posting something to their http://hackshackers.com/blog/2012/02/25/subscribe-to-hackshackers-jobs-newsletter/[jobs newsletter].
  * *Local interest communities*. You could try doing a quick search for an area of expertise in your area (e.g. `javascript' + `london'). Sites such as Meetup.com can also be a great place to start.
  * *Hackathons and competitions*. Whether or not there is prize money available: app and visualisation competitions and development days are often fruitful ground for collaboration and making connections.
  * *Ask a nerd!* Nerds hang around with more nerds. Word of mouth is always a good way to find good people to work with.

=== Harnessing External Expertise Through Hackthons (Jerry Vermanen, De Stentor) ===

image::Figures/02-XY.jpg[width=600]

In March 2010, Utrecht based digital culture organisation SETUP put on an event called http://setup.nl/content/hacking-journalism[`Hacking Journalism']. The event was organised to encourage greater collaboration between developers and journalists.

`We organize hackathons to make cool applications, but we can't recognise interesting stories in data. What we build has no social relevance', said the programmers. `We recognize the importance of data journalism, but we don't have all the technical skills to build the things we want', said the journalists.

Working for a regional newspaper, there was no money or incentive to hire a programmer for the newsroom. Data journalism was still an unknown quantity for Dutch newspapers at that time.

The hackathon model was perfect. A relaxed environment for collaboration, with plenty of pizza and energy drinks. http://www.regiohack.nl/[RegioHack] was a hackathon organised by my employer, the regional newspaper http://www.destentor.nl/[De Stentor], our sister publication http://www.tctubantia.nl/[TC Tubantia] and http://saxion.nl/[Saxion Hogescholen Enschede], who provided the location for the event.

The setup was as following: everyone could enlist for a 30-hour hackathon. We provided the food and drink. We aimed for 30 participants, which we divided into 6 groups. These groups would focus on different topics, such as crime, health, transport, safety, ageing and power. For us, the three main objectives for this event were as follows:

*1. Find stories.* For us, data journalism is something new and unknown. The only way we can prove its use, is through well crafted stories. We planned to produce at least three data stories.

*2. Connect people.* We, the journalists, don't know how data journalism is done and we don't pretend to. By putting journalists, students and programmers in one room for 30 hours, we want them to share knowledge and insights.

*3. Host a social event.* Newspapers don't organise a lot of social events, let alone hackathons. We wanted to experience how such an event can yield results. In fact, the event could have been tense: 30 hours with strangers, lots of jargon, bashing your head against basic questions, working out of your comfort zone. By making it a social event - remember the pizza and energy drink? - we wanted to create an environment in which journalists and programmers could feel comfortable and collaborate effectively.

Before the event, TC Tubantia had an interview with the widow of a policeman who had written a book on her husband's working years. She also had a document with all registered murders in the eastern part of the Netherlands, maintained by her husband since 1945. Normally, we would publish this document on our website. This time, we made a http://www.tctubantia.nl/regio/9810350/Moord-en-doodslag-in-Twente.ece[dashboard using the Tableau software]. We also http://www.regiohack.nl/regiohack-blog/een-moord-voor-goede-gegevens/[blogged] about how this came together on our RegioHack site.

During the hackathon, one project group came up with the subject of development of schools and the ageing of our region. http://public.tableausoftware.com/views/Krimpleerlingaantalshrinkingnumberofstudents/Dashboard1?:embed=yes&:toolbar=yes&:tabs=yes[By making a visualisation of future projections], we understood which cities would get in trouble after a few years of decline in enrolments. With this insight, we made an article on how this would affect schools in our region.

We also started a very ambitious project, called De Tweehonderd van Twente (in English, The Two Hundred of Twente) to determine who had the most power in our region and build a database of the most influential people. Through a Google-ish calculation - who has the most ties with powerful organisations - a list of influential people will be composed. This could lead to a series of articles, but it's also a powerful tool for journalists. Who has connections with who? You can ask questions to this database and use it in our daily routine. Also, this database has cultural value. Artists already asked if they could use this database when finished to make interactive art installations.

image::Figures/02-YY.jpg[width=600]

After RegioHack, we noticed that journalists considered data journalism as a viable addition to traditional journalism. My colleagues continued to use and build on the techniques learned on the day to create more ambitious and technical projects such as a database of the administrative costs of housing. With this data, I made http://www.destentor.nl/regio/10168441/.ece[an interactive map in Fusion Tables]. We asked our readers to play around with the data and crowdsourced results (http://tjoadesign.nl/blog/?p=439[here], for example). After a lot of questions on how we made a map in Fusion Tables, I also recorded a http://www.jerryvermanen.nl/2012/01/tutorial-fusion-tables/[video tutorial].

What did we learn? We learned a lot, but we also came along a lot of obstacles. We recognized these four:

*1. Where to begin: question or data?* Almost all projects stalled when searching for information. What datasets are interesting? Where do we find those? And does this answer our research question? When when we found an interesting dataset, it mostly didn't answer the question asked.

*2. Uneven distribution of technical skills.* Technical knowledge was not distributed evenly among the participants. A small group of programmers carried the weight of doing the scraping and visualizing. We expected this in advance, so we assigned a central desk with technical skills (Tableau, custom visualisations, Excel, etc.). In addition to what knowledge every project group had, this central desk could help the groups out with problems.

*3. Datasets not being combined.* Participants didn't combine many datasets. Instead, they took one dataset, tried to visualize it and draw conclusions based on the results. This isn't wrong, but data journalism gets more interesting in the interconnections of datasets.

*4. Not always clear how to proceed.* What above all comes down to, is that there's no routine. The participants have some skills under their belt, but don't know how and when to use them. One journalist compared it with baking a cake. `We have all the ingredients: flour, eggs, milk, etcetera. Now we throw it all in a bag, shake it and hope a cake comes out of it.' Indeed, we have all the ingredients, but don't know what the recipe is.

What now? Our first experiences with data journalism could help other journalists or programmers aspiring the same field of work and we are working to produce a report.

Also, we are considering how to continue RegioHack in a hackathon form. We found it fun, educational and productive and a great introduction to data journalism.

But for data journalism to work, we have to integrate it in the newsroom. Journalists have to think in data, in addition to quotes, press releases, council meetings and so on. By doing RegioHack, we proved to our audience that data journalism isn't just hype. We can write better informed and more distinctive articles, while presenting our readers different articles in print and online.

=== Business Models for Data Journalism (Mirko Lorenz, Deutsche Welle) ===

Go into any newsroom that predominantly relies on traditional models and there is a sense of despair. ``Five years ago we had eight people on the local desk, now we are three and expected to do print plus parts of the online reporting''. There is no question that journalists have gone through ten years of web innovation, often even being early adopters, and still see revenue from digital media go down, down, down.

Can data journalism change that? Is it that big? While one must be very careful predicting the future, there are a few facts that should be considered when thinking about business models for data journalism and journalism as a whole.

**1. Attention is no longer a scarce resource**

This might be obvious, but still puzzles many, even very large and venerable media organizations. The ``value'' of attention has moved. The new electronic models, where the money goes, are based on providing highly targeted services. But that is not the business of newspapers or media companies, it's the kingdom of the big search and social tech companies.

So, what is left for media companies? In any economic process value is usually created by scarcity. Stuff that is available in abundance has a low price, stuff that is scarce will usually sell for higher prices.

Today, in this multi-channel world, attention can be generated in abundance, but _trust_ is an increasingly scarce resource. Don't take this lightly. While ``trust'' might seem something that is hard to measure and hard to monetize, it actually is relatively simple if the value of trust is understood. If you are not sure whether you can trust someone, you may feel cautious and wish to check the veracity of the information you receive from them. If you trust someone - perhaps because you've known them for a long time or you have evidence that they greatly value your trust and confidence - you may no longer feel the need to spend time verifying the information they give you. For example, if someone you trust tells you that a certain price vs. quality is the best deal, then you will often make a decision about the purchase very quickly. If an organisation you trust tells you that this financing plan is cool, you may be quicker to sign the dotted line.

Of course this is all oversimplified. But you get the idea, don't you? Data journalism plus an emphasis on building trust could propel media organizations - small and big, old and new - into a segment in society and the economy that is currently too small, while the attention-getters' segment is simply too big.

One could argue that trust is the basis for future data journalism related business models. In a world where many organisations go to great lengths to distort clarity, hide facts and maintain their market share by little tricks or fine print - going forward building media structures based on the core concept of trust could open up a really big market for journalism.

So, news organisations and individual journalists should think about where there will be demand for trusted information. If you can solve this riddle for a particular group or segment of the market, making money won't be your main concern in a relatively short time.

**2. Being first is not the goal**

Many media companies believe in having ``exclusive'' content. They hope that by being first they will get a greater share of attention from the audience which translates into more custom from advertisers and so on. 

But ultimately what matters is that the reporting is done well. Don't do it quick, do it right. To take an example, there was a huge amount of coverage of the Euro crisis, but it was often very difficult for members of the public to understand developments in context. This is where reporting is failing: journalists are often simply re-telling what they heard elsewhere. So, it's the Euro crisis in detail, but not in much depth. Imagine people could have a clear, comprehensible, and ideally customisable/interactive view of how this crisis developed, where it is now and which kinds of policy options could potentially really work. Very difficult indeed. But the lesson is that speed is not the only game in town.

**3. Data is the new black box and journalists can help unpack it**

Which brings us on to the main point of this section, which is this: while we cannot prove how much money could be made to finance journalism, it is clear that there is a great deal of potential in newsrooms being able to more effectively work with data.

To do that, they do not need multi-million databases or expensive IT. Instead they should invest in the knowledge and skills of journalists, giving them more ways to find the information that is really important.

Imagine we had thousands of highly experienced data journalists, providing information at different levels: locally, regionally, nationally and globally. There would of course be problems and mis-interpretations of data. But by and large we could hope that many investigations into data could help all of us get along better.

Sometimes displaying the results of data analysis becomes an art, but equally often being able to show a specific development in relatively simple charts can still come as a surprise. A prime example of that is the excellent http://www.nytimes.com/2011/07/24/opinion/sunday/24sun4.html["How the deficit got this big"] story from the New York Times, in which they compare the estimated and actual budgets of the US. Having a longer memory, being able to boil down something into one picture that puzzles everybody is the new scoop. Data journalism is a tool to do that. It's like switching on a light in a dark room.

**4. Money is no scarce resource**

In the world of journalism money is scarce. The hard work of writing and photographing is and has been paid low. One reason is that the while writing is pretty complex to master, it is very difficult if not impossible to attach a certain value tag to one well-written text. It's just hard. The text might move you to tears or laugh out loud - but no-one knows that prior to publication.

Using data as a basis for reporting, this might change. Data in today's interconnected world can help to get the big picture as well as the very detailed view how a certain development affects me and you - even telling us separately how much money out of our own pockets will go into the bail-out of a failed bank or country.

This is why stories based on data can be much more powerful than a typical article. This is not implying that one form is better than the other. But it says that being able to point people to data analysis if they ask why a certain conclusion is drawn is much more reliable than an opinion. There are more facts, more data points, the whole story can become much deeper and - using the data - can even be broken down to show the very individual effects of a development.

In the data world, which is right now owned by large institutions, banks and big companies, money is no scarce resource. In Germany, as of November 2011, there was a very peculiar case, where a "bad bank" which was set-up after the 2008 crisis, actually managed to "loose" 53 billion Euros, due to a software mistake. That money was gone for a few months before it surfaced again.

53 billion Euros though would define one of the very big media/advertising markets in a Western country. Using data, we should stop worrying about refinancing journalism. If the analysis is really, really good there a many options for future media companies to sell trustable services - such as an individual investment guide, reliable calculators to finance a house, buy a car, etc.
The point is: While this market is not there nor easy to create - who would argue against the need for trustable, reliable analysis that even extends to help you and me making good decisions?

Plus, if you look around from this angle, there actually are examples of companies using a combination of journalism and data with some success.
Statista is one. The Hamburg-based company takes data and helps users to find good charts. What they do is basically service, not journalism in the original concept - but the basic principle is intact. There are others, often in niches, constantly collecting data and then publishing the findings. Often these compaies are combining different areas such as consulting, research, publishing based on data. Often they are quite successful in this, even in slack economic periods. What media companies and journalists need to understand though is that these data-driven companies might not resemble what we identify as a media company today. So, a big need for further development of data-driven journalism is to understand and model sustainable data-driven newsrooms.

**5. Look for examples, then re-model**

It is said that changing media is incredible hard. Which is true as long as there are only the old models of making money with media content are applied. The sources of money for a typical media company are: Subscriptions, advertising and - so some extend - single items sales, e.g. like a book publisher. The relation of these major sources of income vary, but in most markets around the world will comprise the biggest source of revenue that is used to finance journalism.

It's not so much a sales problem, it's a product value problem. People - you, me, everybody - buy something all the time. And there is a huge unmet demand for reliable, trustable and understandable information. Examples of such information could be described for almost any area of life: Politics, economics, sports. Or even more concrete: An interactive guide with reliable calculators how to buy a house in a specific region. A guide to green energy, broken down to your particular needs. A guide how to get a car, where all the costs factors can be used to make a decision beyond the advertisers claims, such as cost of ownership, resale prices, etc. These are just examples, painted with a few brushes and far from complete. But in effect, producing reliable, individual packages of information is a potential market - with many ways how to combine it with journalism or using such end-products to finance journalism.

Building this market is challenge. To be successful, three fundamental aspects must be covered:
First, there must be a real value. The "services" must be so good, that they provide a real new option to make better decisions.
Secondly, the value should be higher than the price. This sounds abstract but isn't. Just imagine a data-driven media company offers a guide for house buying that goes far beyond what a bank would be willing to tell you, even comparing one bank versus the other. If the cost of such a service would 20 Euro, but the potential savings using it go into thousands of euros - this is when such services are actually marketable. 
Thirdly, and this is often overlooked, there must be a whole, working and accepted system for such payments. Right now, both journalists and potential customers do not even expect such services from most media companies. To get this to a high level we first need pioneers who proof that this is a market and then apply the models to newsrooms, old and new.

To give one example how such shifts and markets can be created, just take Google. The search company has masterfully automated a steady stream of income through it's AdWords concept. Around the world single people and large companies are booking AdWords, ranging from investments of 10, 20 Euros to millions of Euros. This is how Google makes money.

So, the best guess we can make at this moment goes like this: The future of media and journalism might still rely on subscriptions and sales of advertising space, but to a (hopefully big) extend data journalism can help to create novel services.
Data journalism offers an opportunity to provide insightful stories, which - once the data is understood - can be transformed into services, tools, apps and personalized information.

Right now, many if not most media companies are from having something like that. There was no incentive to cover topics in depth, instead the main incentive was to drum up attention.

But it is not impossible to master that shift. If you look more deeply there are quite a number of working models in this space. The list would range from larger media companies like Thompson Reuters to The Economist to many information providers who fill a niche and employ thousands of people based on looking into data. This would to some extend include companies like eMarketer, Marketing Sherpa - often with blurring lines to make the distinction whether these are primarily journalism companies or research companies. Effectively, we will have to watch this space and make descions whether to lean more towards one side or the other.

Two very good examples are Thompson Reuters and The Economist. In the case of Thompson, just look at the history of this company that has grown out of one regional newspaper in Canada into one of the big data providers for financial markets. Preparing for this Thompson relied on in-depth, professional information streams. Simplified they built a database, maintained by a number of editors to cover one market niche in depth and then selling this information once it was good enough. These models are often successful after a longer period of building expertise, one benefit being that they are pretty stable over long periods.

The Economist is another example. The magazine has built an excellent, influential brand on it's media side. At the same time the "Economist Intelligence Unit" is now more like a consultancy, reporting about relevant trends and forecasts for almost any country in the world. They are employing hundreds of journalists and claim to serve about 1,5 million customers worldwide.

Finally, we should be aware of the many examples for new offerings in all stages of data usage. An uncomplete here could include quite a number of new offerings which are all building blocks for a better use of data for the public. Technology today enables many new ways to extract value. Examples of companies and offerings making use of this would include: DataSift, Timetric, Junar, Needlebase, Flowing Data, The Guardian Data Blog and Data Store - just to provide some examples. And there is room to try out new approaches for many levels of data use. Take BuzzData, a site where datasets can be published and discussed in public.

*"This is not journalism"*

Yes, yes - at this point in discussions, journalists often exclaim that "this is not journalism". This is true sometimes, sometimes not. Good journalism goes beyond just being a commercial product or service, good journalism plays a role as a public service, too.

When inventing new ways to get both sides in line - the financing and the special role of journalism, we must think of all components, not just the newsroom. Selling advertising, which is a department in every commercial media organization is not journalism, too. But it is surely needed to make the work of the newsroom possible. A lot of the work done in media companies effectively has the role of a supporting role for the core business of journalism. 

When looking at new models and trying to come up with something really new we should accept that these models will need several components: Good (data-driven) reporting, but maybe other lines of business and services to enable the journalism part of what the company is doing.

This is just starting..
This whole shift is just starting. The assumptions presented here what might work surely have some big holes in them. But be patient, keep on asking the right questions. And help us finding the missing elements to make the case for data journalism complete and sturdy. Saving journalism is worth the effort.

Understanding/refining/inventing models is one of the big challenges in this space. While it is not very satisfactory to leave it like that, two things might be accepted: It is possible. But as of now we simply have not enough experience and examples to understand these new models. One data point towards a future where editors and reporters will use data consistently as a starting point and not as an after thought is a study conducted by McKinsey in 2010, which got cited on "Flowing Data": There is a growing need for data heads and data managers. People who know how to transform raw collections of numbers into words and stories.

Right now we as the journalists experiencing this shift from old to new models do not have information ourselves. But this is the big challenge we have to master: Finding new, sustainable models to run journalism projects and reliable media companies using data more and better. If we succeed a new perspective for journalism could open up: New offerings, new sources to finance good reporting and in the long run even more jobs in this space than before.

=== Our Stories Come In Code (Lorenz Matzat, OpenDataCity.de) ===

////

Insert: pictures from projects

////

http://www.opendatacity.de/[OpenDataCity] was founded towards the end of 2010. There was pretty much nothing that you could call data journalism happening in Germany at this time.

Why did we do this? Many times we heard people working for newspapers and broadcasters say: ``No, we are not ready to start a dedicated data journalism unit in our newsroom. But we would be happy to outsource this to someone else.''

As far as we know, we are the only company specialising exclusively in data journalism in Germany. There are currently three of us: two of us with a journalism background and one with a deep understanding of code and visualisation. We work with a handful of freelance hackers, designers and journalists.

In the last twelve month we have undertaken four data journalism projects with newspapers, and have offered training and consultancy to media workers, scientists and journalism schools. The first app we did was an http://www.taz.de/1/berlin/fluglaerm-bbi/[interactive tool on airport noise] around the the newly built airport in Berlin with TAZ. Our next notable project was an http://www.zeit.de/datenschutz/malte-spitz-data-retention[application about data retention] of the mobile phone usage of a German politician with ZEIT Online. For this we won a http://www.grimme-institut.de/html/index.php?id=1345[Grimme Online Award] and a Lead Award in Germany, and an Online Journalism Award by the http://journalists.org/2011/09/25/2011-online-journalism-award-winners-announced/[Online Journalism Association] in the US. At the time of writing, we are have several projects in the pipeline - ranging from simpler interactive infographics to full blown realtime data applications.

Of course, winning prizes helps to built a reputation. But when we talk to the publishers, who have to approve the projects, our argument for investing into data journalism is not about winning prizes. Rather it is about getting attention over a longer period in a sustainable way. Building things for their long term impact, not for the scoop, which often is forgotten after some days.

Here are three arguments which we have used to encourage publishers to undertake longer term projects:

1. *Data projects don't date*. Depending on their design, new material can be added to data journalism apps. And they are not just for the users, but can be used internally for reporting and analysis. If you're worried that this means that your competitors will also benefit from your investment, you could keep some features or some data for internal use only.

2. *You can build on your past work*. When undertaking a data project, you will often create bits of code which can be reused or updated. The next project might take half the time, because you know much better what to do (and what not to) and you have bits and pieces you can build on.

3. *Data journalism pays for itself*. Data driven projects are cheaper than traditional marketing campaigns. Online news outlets will often invest in things like Search Engine Optimisation (SEO) and Search Engine Marketing (SEM). A executed data project will normally generate a lot of clicks and buzz, and may go viral. Publishers will typically pay less for this then trying to generate the same attention by clicks and links through SEM.

Our work is not very different from other new media agencies: providing applications or services for news outlets. But maybe we differ in that we think of ourselves first and foremost as journalists. In our eyes the products we deliver are articles or stories - albeit ones which are provided not in words and pictures, audio or video, but in code. When we are talking about data journalism we have to talk about technology, software, devices and how to tell a story with them.

To give an example: we are currently working on an application, which pulls in real-time data via a scraper. The data is updated every minute or so. We started doing this some months ago, and have so far collected a huge dataset which grows every hour. By now it amounts to some hundreds of thousands of rows of data. We are creating a data-article which enables the user to explore this realtime data, and to do research in the archive of the foregone months. In the end the story we are telling will be significantly defined by the individual action of the users.

In traditional journalism, due to the linear character of written or broadcasted media, we have to think about a beginning, the end, the story arc and the length and angle of our piece. With data journalism things are different. There is a beginning, yes. People come to the website and get a first impression of the interface. But then they are on their own. Maybe they stay for a minute - or half an hour.

Our job as data journalists is to provide the framework or environment for this. As well as the coding and data management bits, we have to thing of clever ways to design experiences. The User Experience (UX) derives mostly from the (Graphical) User Interface (GUI). In the end this is the part which will make or break a project. You could have the best code working in the background handling an exiting dataset. But if the front-end sucks, nobody will care about it.

There is still a lot to learn about and to experiment with. But luckily there is the games industry, which has been innovating with respect to digital narratives, ecosystems and interfaces for several decades now. So when developing data journalism applications we should watch closely how game design works and how stories are told in games. Why are casual games like Tetris such fun? And what makes the open worlds of sandbox games like Grand Theft Auto or Skyrim rock?

=== Kaas og Mulvad: Semi-finished Content for Stakeholder Groups (Mark Lee Hunter and Luk N. Van Wassenhove) ===

////

Insert: pictures from projects

////

Stakeholder media often lack capacity even more cruelly than do the news media, and the function of Kaas og Mulvad, a for-profit Danish corporation, is to provide it. The firm originated in 2007 as a spinoff of the non-profit Danish Institute for Computer-Assisted Reporting (Dicar), which sold investigative reports to media and trained journalists in data analysis. Its founders, Tommy Kaas and Nils Mulvad, were previously reporters in the news industry. Their new firm offers what they call "data plus journalistic insight" to stakeholder media, which finalise the content and distribute it to news media, as well as through their own media.  Direct clients include government institutions, PR firms, labour unions and NGOs such as EU Transparency and the World Wildlife Fund. The NGO work includes monitoring farm and fishery subsidies, and regular updates on EU lobbyist activities generated through "scraping" of pertinent websites. Indirect clients include foundations that fund NGO projects. The firm also works with the news industry: a tabloid newspaper purchased a celebrity monitoring service.

**1. Processes: Innovative IT plus analysis**

The firm undertakes about 100 projects per year, ranging in duration from a few hours to a few months. It also continuously invests in projects that expand its capacity and offerings. The celebrity monitoring service was one such experiment. Another involved scraping the Internet for news of home foreclosures and creating maps of the events. The partners say that their first criteria for projects is whether they enjoy the work and learn from it; markets are sought after a new service is defined. They make it clear that in the news industry, they found it difficult to develop new methods and new business. Comments Mulvad:

----

We have no editors or bosses to decide which projects we can do, which software or hardware we can buy. We can buy the tools according to project needs - like the best solutions for text scraping and mining. Our goal is to be cutting edge in these areas. We try to get customers who are willing to pay, or if the project is fun we do it for a lower charge.

----

**2. Value created: Personal and firm brands and revenue**

Turnover in 2009 was approximately 2.5 million Danish kroner, or €336,000. The firm also sustains the partners' reputations as cutting edge journalists, which maintains demand for their teaching and speaking services. Their public appearances, in turn, support the firm's brand.

**3. Key insights of this example**

  * The news industry's crisis of declining capacity is also a crisis of under-utilisation of capacity. Kaas and Mulvad had to leave the news industry to do work they valued, and that pays. Nothing prevented a news organisation from capturing that value.
  * In at least some markets, there exists a profitable market for semi-finished content that serves the interests of stakeholder groups.
  * However, this opportunity raises the issue of how much control journalists can exercise over the presentation and use of their work by third parties. We recall that this issue already exists within the news industry (where editors can impose changes on a journalist's product), and it has existed within other media industries (such as the film industry, where conflicts between directors and studios over "final cuts" are hardly rare). It is not a particular moral hazard of stakeholder media.
  * From a revenue standpoint, a single product or service is not enough. Successful watchdog enterprises would do better to take a portfolio approach, in which consulting, teaching, speaking and other services bring in extra revenue, support the watchdog brand, and enrich the lifestyle of the operators.

=== When code pays for words by Clément Renaud (Sharism Lab?) ===

Example from OWNI
