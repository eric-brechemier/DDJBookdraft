== 4. Getting Data ==

=== Where to look? ===

////

Insert: picture from data catalogue
Refernce: data book on O'Reilly

////

  *    Overview: An overview of open data sources, what they contain, how to find them, how to search them, examples of open data being used by journalists
‚Äö√≥√£   Is data copyrightable? "In the United States, data will be protected by copyright only if they express creativity. Some databases will satisfy this condition, such as a database containing poetry or a wiki containing prose. Many databases, however, contain factual information that may have taken a great deal of effort to gather, such as the results of a series of complicated and creative experiments." http://sciencecommons.org/old/databases/
‚Äö√≥√£ 	What is Open Data?
                     ‚Äö√±‚Ä†http://en.wikipedia.org/wiki/Open_data, http://opendefinition.org/
                     ‚Äö√±‚Ä†Data published using Creative Commons tools and licenses: http://wiki.creativecommons.org/Data
                     ‚Äö√±‚Ä†Other open licenses for data: http://opendefinition.org/licenses/
                     ‚Äö√±‚Ä†National and local governments have begun publishing data on portals like http://www.data.gov/ and http://data.cityofchicago.org/
                     ‚Äö√±‚Ä†Commercial re-use clauses
  *    Authors: Jonathan Gray (Open Knowledge Foundation), Brian Boyer (Chicago Tribune), Jane Park (Creative Commons), John Keefe (WNYC), Chrys Wu (Hacks/Hackers),
  *    Editor: Lucy Chambers, Friedrich Lindenberg
  *  	Length: 1-3 pages (with links and examples)

Introduction

by Lucy Chambers
In this chapter, we aim to show you how to get your hands on the best data available to further your investigation. Sometimes you don't have to look far, the data may already be out there: it is a matter of knowing where to look. Other times, you may need to find the right people to ask. In particular, this chapter deals with:
  *  	Tips and tricks for where to look for data: before launching into FOI requests or embarking down a technical route such as scraping.
  *  	Search tips: How to make Google give you want you want, in the format you want it.
  *    Communities for data: useful forums and mailing lists to ask experts directly
  *  	Data houses: data catalogues, government portals, accessing research data and social data sites
Finally, we'll have a quick look at the legal implications of re-using data from web databases - while you may decide to break copyright to get your story out, it is still useful to know the boundaries in current law.

A Journalist's Guide to Finding Data -- Open or Otherwise!


I need to find data for my story. Where should I look?

**1. Google for 5 minutes.**

Often enough, databases on the web are included in Google - whether the publisher intended or not. When searching for data, make sure that you include both search terms relating to the content of the data you're trying to find as well as some information on the format or source that you would expect it to be in.
  *    A handy feature of google is that it allows you to limit the file types of the returned results. For example, you can look only for spreadsheets (filetype:XLS filetype:CSV), geodata (filetype:shp), or database extracts (filetype:MDB, filetype:SQL, filetype:DB). If you're into that sort of thing, you can even look for PDFs (filetype:pdf).
  *    Another option is searching by a part of a URL: (inurl:downloads filetype:xls) will try to find all Excel files that have "downloads" in their web address (if you find a single download, its often worth just checking what other results exist for the same folder on the web server). You can also limit your search to only those results on a single domain name: (site:agency.gov).
  *    Another popular trick is not to search for content directly, but for places where bulk data may be available. For example, (site:agency.gov Directory Listing) may give you some listings generated by the web server with easy access to raw files, while (site:agency.gov Database Download) will look for intentionally created listings.
          	
**2. Ask an Expert.**

Professors, government offices and industry folks often know where to look. Call 'em up! Show up at the office. Ask nicely. "I'm doing a story on X. Where would I find this?" "Do you know who has this?"

**3. Ask a mailing list.**

Mailing lists combine the wisdom of a whole community on a particular topic. For data journalists, http://www.ire.org/membership/subscribe/nicar-l.html/[NICAR-L] is an excellent starting point. This is a listserv of data-journalism geeks, working on all kinds of projects. Chances are that someone has done a story like yours, and would have an idea of where to start - if not a link to the data itself.

Other Mailing lists you could try:
  *    http://project-wombat.org/[Project Wombat] - "Project Wombat is a discussion list for difficult reference questions". Ask them anything. They'll likely be able to find you an answer with references
  *    Open Knowledge Foundation Lists: http://wiki.okfn.org/Mailing_Lists. In particular the http://lists.okfn.org/mailman/listinfo/data-driven-journalism[Data Driven Journalism List], which acts as a hub for many journalistic enquiries.
  *    http://theinfo.org/[theInfo] is an older set of mailing lists that has a lot of knowledgeable experts subscribed.

Topic Specific Lists:
  *    Transparency: http://groups.google.com/group/sunlightlabs?pli=1[Sunlight Labs Mailing List] - The Sunlight Foundation focuses their activities into promoting and facilitating government transparency.
  *    Mapping: http://www.nacis.org/index.cfm?x=1[North American Cartographic Information Society]
  *    Mapping: Society of Cartographers http://soc.org.uk/cartosoc.htm[CARTO-SoC]
  *    Mapping: MAPS-L: LISTSERV@LISTSERV.UGA.EDU - MAPS-L is an international discussion forum for anyone dealing with cartographic information, cartographers, remote sensors, geographers, and cartomaniacs of all types.
          	
**4. Use Forums.**

Search for existing answers or ask a question at http://getthedata.org/[Get The Data] or on http://www.quora.com/[Quora]. GetTheData is Q&A site where you can ask your data related questions, including where to find data relating to a particular issue, how to query or retrieve a particular data source, what tools to use to explore a data set in a visual way, how to cleanse data or get it into a format you can work with.

**5. Join Hacks/Hackers**

A rapidly expanding international grassroots journalism organization with dozens of chapters and thousands of members across four continents The mission is to create a network of journalists ("hacks") and technologists ("hackers") who rethink the future of news and information. With such a broad network - you stand a strong chance of someone knowing where to look for the thing you seek.

**6. Learn a bit about government IT.**

Understanding the technical and administrative context in which governments maintain their information is often helpful when trying to access data. Whether it's CORDIS, COINS or THOMAS - big-acronym databases often become most useful once you understand a bit about their intended purpose.

Find government org charts and look for departments/units with a cross-cutting function (e.g. reporting, IT services), then explore their web sites. A lot of data is kept in multiple departments and while for one, a particular database may be their crown jewels, another may give it to you freely.

Look out for dynamic infographics on government sites. These are often powered by structured data sources/APIs that can be used independently (e.g. flight tracking applets, weather forecast java apps).

**7. Google it again using phrases and improbable sets of words you've spotted since last time.**

**8. Write a FOI Request!**

If you believe that a government body has the data you need, a freedom of information request may be your best tool. See chapter 4.1.2 Asking for data, for more information.

=== Data Catalogues and Social Data (Friedrich Lindenberg, The Open Knowledge Foundation) ===

Over the last couple of years, a number of data portals have sprung up all over the web, that aim to help you get access to all kinds of information:

  *  	Government Data Portals: the government's willingness to release a given dataset will vary from country to country. A growing number of states are providing data portals (inspired by data.gov and data.gov.uk) to promite the civic and commercial re-use of government information. An up-to-date, global index of such sites can be found at datacatalogs.org. Another handy site is the http://www.guardian.co.uk/world-government-data[Guardian World Government Data], a meta search engine that includes many international government data catalogues.
  *  	http://thedatahub.org/[The DataHub] - a community-driven resource that makes it easy to find, share and reuse open content and data, especially in ways that are machine automatable.
  *    https://scraperwiki.com/[ScraperWiki] an online tool to make the process of extracting "useful bits of data easier so they can be reused in other apps, or rummaged through by journalists and researchers." Most of the scrapers and their databases are public and can be re-used.
  *    http://data.worldbank.org/[World Bank] and http://data.un.org/[United Nations] data portals provide high-level indicators for all countries, often for many years in the past.
  *  	A number of startups are emerging, that aim to build communities around data sharing and re-sale. This includes http://buzzdata.com/[Buzzdata] - a place to share and collaborate on private and public datasets - and data shops such as http://www.infochimps.com/[Infochimps] and http://datamarket.com/[DataMarket].
  *    http://datacouch.com/[DataCouch] - A place to upload, refine, share & visualize your data.
  *    An interesting Google subsidiary, http://www.freebase.com/[Freebase], provides "an entity graph of people, places and things, built by a community that loves open data."

### Research data ###

by Jonathan Gray
  *    Google Scholar
  *    Mendeley
  *    IBM Research - Technical paper search
  *    JSTOR - "With more than a thousand academic journals and over 1 million images, letters, and other primary sources, JSTOR is one of the world's most trusted sources for academic content."
  *    UK Data Archive - "The UK's largest collection of digital research data in the social sciences and humanities"

### Reference data ###

by Friedrich Lindenberg

It's often helpful to pull in data from code sheets, such as lists of countries, states, organizations etc. This kind of data - which is used to augment another collection - is called reference data. Other examples of reference data include code sheets to decipher classifications and taxonomies, such as the EU's procurement vocabulary: on most portals, only short identifiers are provided, while the actual term descriptions are quite helpful.

Some sources for reference data include:
  *    http://www.geonames.org/[GeoNames] and http://open.mapquestapi.com/nominatim/[Nominatim] for geographic names and coordinates.
  *    National statistics agencies
  *    Web sites of standards bodies, such as the ISO (e.g. country codes) or the http://iatistandard.org/[IATI Standard] site.
  *    International organisations such as the United Nations (e.g. classification of functions of government).

=== Doing the legal stuff ===

////

Insert traffic lights picture - mixed signals?

////

(Source: http://opendatahandbook.org/[The Open Data Manual], with input from Helen Darbishire, Access Info)

Most of the new Open Data portals being set up by governments now make clear that the data that is released can be used free of charge, same goes for information obtained through Freedom of Information Requests. If you have scraped the data from a database such as a public body's website, then you may be particularly liable for limitations on reuse. Sometimes there will be intellectual property limitations, but normally the only requirement is that you cite the source of the information.

When talking about databases and intellectual property we need to distinguish between the structure and the content of a database (when we use the term 'data' we shall mean the content of the database itself). Structural elements include things like the field names and a model for the data -- the organization of these fields and their inter-relation.

In many jurisdictions it is likely that the structural elements of a database will be covered by copyright (it depends somewhat on the level of 'creativity' involved in creating this structure).
 
The distinction between the "contents" of a database and the collection is especially crucial for factual databases since no jurisdiction grants a monopoly right in the individual facts (the "contents") even though it may grant right(s) in them as a collection.

To illustrate, consider the simple example of a database which lists the melting point of various substances. While the database as a whole might be protected by law so that one is not allow to access, reuse or redistribute it without permission this would never prevent you from stating the fact that substance Y melts at temperature Z.
 
You can read more about the situation your jurisdiction in the http://opendefinition.org/guide/data/[Guide to Open Data Licensing].

If you find you are having problems with your right to reuse information, then please let the campaign group Access Info Europe know (helpdesk@access-info.org). Access Info will help you with legal advice and will try to find lawyers in your country should that be necessary.
Success? What now...? Share the data...


When you publish your project, do the rest of us a favor and include the data you've collected! It would be a damned shame if all that beautiful data you dug up (cleaned up, formatted and augmented) went stale on your hard drive. Let others source-check your story and perhaps even find different stories in the data.

When you do publish data, include an explicit IP statement, in particular an open license like the Creative Commons Zero or Attribution terms or the Open Database License (ODbL). If the data is government information not covered by copyright, publish it using Creative Commons' PD Mark dedication to make it clear that the data will be available in the public domain forever and for others to reuse!

NOTE FOR JWYG: Much of the information in this chapter formerly related to submitting an FOI request, differences in legislation in different countries... It's interesting but not specific to data journalism. The overflow material is in this doc: FOI tipsheet (also linked from overflow materials) Thought perhaps we could turn the other sections into an FOI tipsheet and publish it on DDJnet and link to it

=== Asking for data ===

Authors: Helen Darbishire (Access Info), Fabrizio Scrollini (London School of Economics), Martin Rosenbaum (BBC)
Peer-reviewers: Martin Rosenbaum, BBC
Editor: Sam Leon (Open Knowledge Foundation), Lucy Chambers (Open Knowledge Foundation), Liliana Bounegru (European Journalism Centre)



### Your Right to Data ###

////

Insert: picture of FOI law

////

Information held by public bodies is one of the principle sources of data for the work of any journalist. The right of access to information (aka "freedom of information") is the right of anyone to access information held by public authorities. Many countries now have freedom of information laws to make this information available to the public. This chapter deals with:
FOIA for data - what to ask for, what format to request it in and how to ask for it
Case studies about stories based on successful data FOI requests.



For more detailed information on the process of submitting an FOI request, the difference in FOI legislation in different countries and appeal procedures see the FOI tipsheet.
Before FOI - Where to look

Before you delve into making a FOI request, see if the data is already available - or has already been requested by others. See 2.1 'Where Does Data Live' for more information on where you can look online for the data.
Privacy Concerns and FOI for databases


Privacy concerns: For data journalists trying to get access to a full data set, issue might come up because some of the information in the data set is confidential and the public authority uses this as a reason for not providing the entire database. This is not acceptable under the right of access to information: public bodies have to provide at least partial access.

For example, by removing personal data fields from the database so that the data journalist can see all the statistical and factual data the public authority would not be violating confidentiality. All exceptions must be justified by the public body and you have a right to appeal.
TIP: "Look at the 'disclosure logs' that some public authorities have on their websites, which list some or all of their responses to previous FOI requests. They are a good source of ideas about what kind of information you are actually likely to obtain." (Martin Rosenbaum - BBC)

FOI requests - useful tips

from the http://www.legalleaks.info/[Leaks Toolkit] (Access Info Europe and n-ost)
Tips by Martin Rosenbaum, BBC
Fact boxes - Helen & Fabrizio
Before you submit your request


**1. Plan ahead to save time.**

Think about submitting a formal access request whenever you set out to look for information. It's better not to wait until you have exhausted all other possibilities. You will save time by submitting a request at the beginning of your research and carrying out other investigations in parallel.

Tip: Be prepared for delay. FOI can be most useful where the information would still be of use to you weeks or even months later.

**2. Check the rules about fees**

Before you start submitting a request, check the rules about fees for either submitting requests or receiving information. That way, if a public official suddenly asks you for money, you will know what your rights are.

FACT BOX: You can ask for electronic documents to avoid copying and posting costs, mention in your request that you would prefer the information in electronic format. That way you will avoid paying a fee, unless of course the information is not available electronically, although these days it's usually possible to scan documents which are not already digitalised and then to send them as an attachment by e-mail.

Writing your FOI request:
Start out simple: In all countries, it is better to start with a simple request for information and then to add more questions once you get the initial information. That way you don't run the risk of the public institution applying an extension because it is a "complex request".

Tip: A request for information only held by one part of a public authority will probably be answered more quickly than one which requires a search across the entire authority. A request which involves the authority in consulting third parties (eg, a private company which supplied the information, another government which is affected by it) can take particularly long. Be persistent.

Tip: Think 'inside the filing cabinet'. Try to find out what data is collated. For example, if you get a blank copy of the form the police fill out after traffic accidents, you can then see what information they do or do not record about car crashes.
FACT BOX: Time Limits in freedom of information laws

Most freedom of information laws provide a time limit for authorities to reply to you. Globally, the range in most laws is from a few days to one month. Read more: (6)
Submit multiple requests: If you are unsure where to submit your request, there is nothing to stop you submitting the request with two, three or more bodies at the same time. In some cases, the various bodies will give you different answers, but this can actually be helpful in giving you a fuller picture of the information available on the subject you are investigating.

Tip: Especially important if you are planning to compare data from different public authorities. Is your request in any way ambiguous?

For example, if you ask for figures for 'the past three years', some authorities will send you information for the past three calendar years and others for the past three financial years, which you won't be able to directly compare.
Submit international requests: Increasingly requests can be submitted electronically, so it doesn't matter where you live. Alternatively, if you do not live in the country where you want to submit the request, you can sometimes send the request to the embassy and they should transfer it to the competent public body. You will need to check with the relevant embassy first if they are ready to do this - sometimes the embassy staff will not have been trained in the right to information and if this seems to be the case, it's safer to submit the request directly to the relevant public body.
Use appropriate language: use language and etiquette appropriate to any other professional communication in your country.

Tip: If you are planning to send the same request to many public authorities start by sending an initial draft of the request to a few authorities as a pilot exercise. This will show you whether you are using the right terminology to obtain the material you want and whether answering your questions is feasible, so that you can then revise the request if necessary before sending it to everyone.

Mention your right to information: Usually the law does not require that you mention the access to information law or freedom of information act, but this is recommended because it shows you know your legal rights and is likely to encourage correct processing of the requests according to the law. We note that for requests to the EU it's important to mention that it's an access to documents request and it's best to make a specific mention of Regulation 1049/2001.
Hide your request in a more general one: If you decide to hide your real request in a more general one, then you should make your request broad enough so that it captures the information you want but not so broad as to be unclear or discourage a response. Specific and clear requests tend to get faster and better answers.
Anticipate the exceptions: If you think that exceptions might be applied to your request, then, when preparing your questions, separate the question about the potentially sensitive information from the other information that common sense would say should not fall under an exception. Then split your question in two and submit the two requests separately.
Ask for access to the files#: If you live near where the information is held (e.g. in the capital where the documents are kept), you can also ask to inspect original documents. This can be helpful when researching information that might be held in a large number of documents that you'd like to have a look through. Such inspection should be free of charge and should be arranged at a time that is reasonable and convenient for you.

FACT BOX: Governments are not obliged to process data for you, but should give you all the data they have, and if it is data that they should have according to perform their legal competencies, they should certainly produce it for you.
Keep a record! Make your request in writing and save a copy or a record of it so that in the future you are able to demonstrate that your request was sent, in case you need to make an appeal against failure to answer. This also gives you evidence of submitting the request if you are planning to do a story on it.
Speed up answers by making it public that you submitted a request: If you write or broadcast a story that the request has been submitted, it can put pressure on the public institution to process and respond to the request. You can update the information as and when you get a response to the request - or if the deadline passes and there is no response you can make this into a news story as well. Doing this has the additional benefit of educating members of the public about the right of access to information and how it works in practice.
Involve your colleagues in using access to information: If your colleagues are sceptical about the value of access to information requests, one of the best ways to convince them is to write a story based on information you obtained using an access to information law. Mentioning in the final article or broadcast piece that you used the law is also recommended as a way of enforcing its value and raising public awareness of the right.
Data Specific FOI requests


What to ask for: 'Raw Data Now'

The focus of many access to information advocates has turned towards persuading governments to make more information available in formats you can use and reuse.

This reframing of the right to data seeks to make clear that:
Detailed information should be available in a "disaggregated" or "granular" format - this is sometimes also referred to as "raw data" before it has been "cooked" by public officials and statisticians.
Entire data sets or databases should be made available, free of charge and in an open source and machine readable format in order to permit re-use of the information.



In the following sections you will find a brief definition of what access to information means globally and nationally.

Read More: http://www.access-info.org/documents/Access_Docs/Advancing/Beyond_Access_7_January_2011_web.pdf[A recent report by the Open Knowledge Foundation and Access Info Europe] provides a thorough description of this evolving field.
Case Studies

=== Wobbing works. Use it! by Brigitte Alfter (Freelance Journalist) ===

http://www.alfter.dk/[www.alfter.dk], journalist / co-founder of the http://www.wobbing.eu/[www.Wobbing.eu network], co-founder of the http://www.farmsubsidy.org/[www.Farmsubsidy.org] network

Using freedom of information legislation - or wobbing, as the slang goes - is an excellent tool. But it requires method and often persistence. Three examples from Brigitte Alfter about the strength and challenges of Wobbing:


Case Study 1: The Farmsubsidy project

////

Insert: picture from project

////

These years the EU pays almost ‚Äö√á¬® 60 billion to farmers and the farming industry. Per year. This has been going on since late 1950ies and the political narrative was that the subsidies help our poorest farmers. However a first FOI breakthrough in Denmark in 2004 indicated: The story was just a narrative. The small farmers were struggling as they so often complained about in private and in public, and in reality most of the money went to a few large land owners and to the agro industry. So obviously I wanted to know: Is there a pattern throughout the EU?

In the summer of 2004 I asked the European Commission for the data. Every year in February the Commission receives data from the member states. Data show who applied for EU funding, how much beneficiaries got, and whether they got it for farming their land, developing their region or for exporting milkpowder. At the time, the Commission received the figures on CD-roms with csv files. A lot of data, but in principle easy to work with. If you got them out, that is.

In 2004 the Commission refused - the key argument being, that the data were uploaded into a database and couldn't be retrieved without a lot of work. An argument, that the European Ombudsmand called "maladministration". You can find all documents in this case on http://www.wobbing.eu/[www.wobbing.eu] http://www.wobbing.eu/news/eu-watchdog-criticises-commission-and-comments-access-databases[here]. Back in 2004 we did not have the time to be legal foodies. We wanted the data.

So we teamed up throughout Europe and went country by country to get the data. English, Swedish and Dutch colleagues got the data in 2005. Finland, Poland, Portugal, regions of Spain, Slovenia and other countries opened up in the too. Even in wob-difficult Germany I got a breakthrough and received some data in the province of North Rhine-Westfalia in 2007. I had to go to court to get the data - but it then resulted in some nice articles in news magazine http://www.stern.de/wirtschaft/news/unternehmen/agrarsubventionen-volle-toepfe-fuer-die-grossen-601794.html[Stern and Stern online].

Was it random, that Denmark and the UK were the first to open? Not necessarily. In a larger political picture the farm subsidies at the time had to be seen in the context of the WTO negotiations, where subsidies were under pressure. Denmark and the UK are among the more liberal countries in the EU, so there may well have been political winds blowing into the direction of transparency in those countries.

The story did not stop there, for more episodes and for the data go to www.farmsubsidy.org.

Lessons learnt: Go wob-shopping. We have a fabulous diversity of freedom of information laws in Europe, and different countries have different political interests at given points of time. This is an advantage to use. 

Case Study 2: Side effects

We are all guinea pigs when it comes to taking medicine. Drugs can have side-effects. We know, we consider it compared to the effect we wish for - and we make a decision. Unfortunately often this decision is not an informed decision.

When teenagers take a pill against pimples, they hope for a smooth skin and not for a bad mood. Yet exactly that happened with one drug, where the youngsters turned depressive and even suicidal. The danger of this particular side effect was not easily available - an obvious story for journalists.

There are data about side-effects. The producers regularly have to deliver information to the health authorities about observed side-effects. They are held by national or EU-authorities once a drug is allowed on the market.

The initial breakthrough again came on national level in Denmark. During a cross-border research by a Danish-Dutch-Belgian team, also the Netherlands opened. Another example of wob-shopping: This time it helped to point out to the Dutch authorities, that the data were accessible in Denmark.

But the story was true, in Europe there were suicidal young people and sadly also suicides in several countries. Journalists, university researchers, the family of a young victim were all pushing hard to get access to this information. The EU-ombudsman helped push the transparency at the European Medicines Agency - and it http://www.ombudsman.europa.eu/press/release.faces/en/5498/html.bookmark[looks, as if he succeeded]. So now the task is upon journalists to get out data and analyse the material thoroughly. Are we - as one researcher put it - all guinea pigs, or are the control mechanisms sound?

Lessons learnt: Don't take no for an answer when it's about transparency, be persistent and follow a story over time. Things may well change and allow better reporting based upon better access at a later point.

Case Study 3: Smuggling death


Recent history can be utterly painful for entire populations, particularly after wars and in times of transition. So how can journalists obtain hard data to investigate, whether - for example - last decades war profiteers are in power now? This was the task two Slovenian, a Croatian and a Bosnian journalist set out, the Slovene team being the driving force.

In their home country Slovenia parliamentary commissions had set out to look into the question of profiting from the Balkan wars, but never reached a conclusion. Yet there was a highly valuable trail of declassified documents and data. 6000 pages which the Slovene team obtained through a freedom of information request.

In this case the data had to be extracted from the documents and sorted in databases - for example transports had to be traced by vessel number in ports and license plates of trucks. Thanks to the data and in combination with further data, analysis and research, they were able to map numerous of the routes of the http://www.kaasogmulvad.dk/unv/kiev/Arms%20smuggling%20-%20TRILOGY%20In%20the%20Name%20of%20the%20State%20-%20Matej%20Surc,%20Blaz%20Zgaga%20-%20Slovenia.pdf[illegal weapon trade].

But the team succeeded and the results are http://www.journalismfund.eu/index.php?page=10&detail=154&be785f3421ff8f5ed50dfda00382b66049da3f53=a499c267af42d8392057f24e3fa5b9ca[unique] and have already brought the http://www.journalismfund.eu/index.php?page=9&detail=155[first award] to the team. But more importantly the story matters for the entire region and may well be picked up by journalists in other countries along the routes of the deadly material.

Lessons learnt: Get out good raw material even if you find it in unexpected places and combine with already accessible public data.

Useful resources
For further case studies and tools to facilitate making FOI requests, please see Further Resources. 

=== Getting data from the web by Friedrich Lindenberg (Open Knowledge Foundation) ===

////

Insert: picture of scraperwiki in action

////

Note for JWYG: see also original scraping chapter and import useful bits: https://docs.google.com/document/d/1idnhY0XSFeLSRlRfMU2nm_dnjN-8iKLWP7W_8-WqVi4/edit - old scraping version

You've tried everything else, and you haven't managed to get your hands on the data you want. You've found the data on the web, but, alas - no download options are available and copy-paste has failed you. Fear not, there may still be a way to get the data out. This chapter is about the technical solutions to acquiring machine-readable information. Some examples include:
Getting data from web-based APIs, such as interfaces provided by online databases and many modern web applications (including Twitter, Facebook and many others#). This is a fantastic way to access government or commercial data, as well as data from social media sites.
Extracting data from PDFs. This is very difficult, as PDF is a language for printers and does not retain much information on the structure of the data that is displayed within a document. Extracting information from PDFs is beyond the scope of this book, but there are some tools# and tutorials that may help you do it.
Screen scraping web sites. During screen scraping, you're extracting structured content from a normal web page with the help of a scraping utility or by writing a small piece of code. While this method is very powerful and can be used in many places, it requires a bit of understanding about how the web works.



With all those great technical options, don't forget the simple options: often it is worth to spend some time searching for a file with machine-readable data or to call the institution which is holding the data you want.
What is machine-readable data?

The goal for most of these methods is to get access to machine-readable data. Machine readable data is created for automatic processing by a computer, instead of the presentation to a human user. The structure of such data relates to contained information, and not the way it is displayed eventually. Examples of easily machine-readable formats include CSV, XML, JSON and Excel files, while formats like Word documents, HTML pages and PDF files are more concerned with the visual layout of the information. PDF for example is a language which talks directly to your printer, it's concerned with position of lines and dots on a page, rather than distinguishable characters.
Scraping web sites: what for?

Everyone has done this: you go to a web site, see an interesting table and try to copy it over to Excel so you can add some numbers up or store it for later. Yet this often does not really work, or the information you want is spread across a large number of web sites. Copying by hand can quickly become very tedious, so it makes sense to use a bit of code to do it.

The advantage of scraping is that you can do it with virtually any web site - from weather forecasts to government spending, even if that site does not have an API for raw data access.

What you can and cannot scrape

There are, of course, limits to what can be scraped. Some factors that make it harder to scrape a site include:

Badly formatted HTML code with little or no structural information e.g. older government websites.
Authentication systems that are supposed to prevent automatic access e.g. CAPTCHA codes and paywalls.
Session-based systems that use browser cookies to keep track of what the user has been doing.
A lack of complete item listings and possibilities for wildcard search.
Blocking of bulk access by the server administrators.

Another set of limitations are legal barriers: some countries recognize database rights, which may limit your right to re-use information that has been published online. Sometimes, you can choose to ignore the license and do it anyway - depending on your jurisdiction, you may have special rights as a journalist. Politically, for Government data you'll be fine too. Commercial organizations - and certain NGOs - react with less tolerance and may try to claim that you're "sabotaging" their systems. Other information may infringe the privacy of individuals and thereby violate data privacy laws or professional ethics.

Tools that help you scrape

There are many programs that can be used to extract bulk information from a web site, including browser extensions and some web services. Depending on your browser, tools like Readability (which helps extract text from a page) or DownThemAll (which allows you to download many files at once) will help you automate some tedious tasks, while Chrome's Scraper extension is explicitly built to extract tables from a web site. Developer extensions like FireBug (for Firefox, the same thing is already included in Chrome, Safari and IE) let you track exactly how a web site is structured and what communications happen between your browser and the server.

ScraperWiki is a web site that allows you to code scrapers in a number of different programming languages, including Python, Ruby and PHP. If you want to get started with scraping without the hassle of setting up a programming environment on your computer, this is the way to go. Other web services, such as Google Spreadsheets and Yahoo! Pipes also allow you to perform some extraction from other web sites.

How does a web scraper work?

Web scrapers are usually small pieces of code written in a programming language such as Python, Ruby or PHP. Choosing the right language is largely a question of which community you have access to: if there is someone in your newsroom or city already working with one of these languages, then it makes sense to adopt the same language.

While some of the click-and-point scraping tools mentioned before may be helpful to get started, the real complexity involved in scraping a web site is in addressing the right pages and the right elements within these pages to extract the desired information. These tasks aren't about programming, but understanding the structure of the web site and database.

When displaying a web site, your browser will almost always make use of two technologies: HTTP is a way for it to communicate with the server and to request specific resource, such as documents, images or videos. HTML is the language in which web sites are composed.

The anatomy of a web page

Any HTML page is structured as a hierarchy of boxes: a large box (or "tag") will contain many smaller ones - for example a table that has many smaller divisions: rows and cells. There are many types of tags that perform different functions - some produce boxes, others tables, images or links. Tags can also additional properties such as unique identifiers and belong to groups called 'classes', which makes it possible to target and capture individual elements within a document. Selecting the appropriate elements this way and extracting their content is the key to writing a scraper.
Viewing the elements in a web page: everything can be broken up into boxes within boxes.
To scrape web pages, you'll need to learn a bit about the different types of elements that can be in an HTML document - for example, the <table> element wraps a whole table, which has <tr> elements for its rows, which in turn contain <td> for each cell. The most common element type you will encounter is <div>, which can basically mean anything#. The easiest way to get a feel for these elements is by using the http://skypoetsworld.blogspot.com/2008/01/browser-debugging-tools.html[developer toolbar] in your browser: they will allow you to hover over any part of a web page and see what the underlying code is.

Tags work like book ends, marking the start and the end of a unit. For example <strong> signifies the start of a bold piece of text </strong> signifies the end of that section. Easy.

An Example: Nuclear incidents

http://www-news.iaea.org/EventList.aspx[NEWS] is the International Atomic Energy Agency's (IAEA) portal on world-wide radiation incidents and a strong contender for membership in the Weird Title Forum. The web page lists incidents in a simple, blog-like site that can be easily scraped. To start, create a new Python scraper on https://scraperwiki.com/[ScraperWiki] and you will be presented with a text area that is mostly empty, except for some scaffolding code. In another browser window, open the http://www-news.iaea.org/EventList.aspx[IAEA site] and open the developer toolbar in your browser. In the "Elements" view, try to find the HTML element for one of the news item titles.
Your browsers developer toolbar helps you connect elements on the web page with the underlying HTML code.

Investigating this page will reveal that the titles are <h4> elements within a <table>. Each event is a <tr> row, which also contains a description and a date. If we want to extract the titles of all events, we should find a way to select each row in the table sequentially, while fetching all the text within the title elements.

In order to turn this process into code, we need to make ourselves aware of all the steps involved. To get a feeling for the kind of steps required, let's play a simple game: In your ScraperWiki window, try to write up individual instructions for yourself, for each thing you are going to do while writing this scraper like in a recipe (prefix each line with a hash sign to tell Python that this not real computer code). E.g.:

# Look for all rows in the table
# Unicorn must not overflow on left side.  

Try to be as precise as you can and don't assume that the program knows anything about the page you're attempting to scrape.

Once you've written down some pseudo-code, let's compare this to the essential code for our first scraper:

----
1. import scraperwiki
2. from lxml import html
----

In this first section, we're importing existing functionality from libraries - snippets of pre-written code. "scraperwiki" will give us the ability to download web sites, while "lxml" is a tool for the structured analysis of HTML documents. Good news: if you are writing a Python scraper with ScraperWiki, these two lines will always be the same.

----
3. url = "http://www-news.iaea.org/EventList.aspx"
4. doc_text = scraperwiki.scrape(url)
5. doc = html.fromstring(doc_text)
----

Next, (Line 3) we're making a name (variable): "url" - and assign the URL of the IAEA page as its value. This tells the scraper that this thing exists and we want to pay attention to it. Note that the URL itself is in quotes as it is not part of the program code but a string - a sequence of characters.

Line 4. We then use the "url" variable as input to a function, scraperwiki.scrape. A function will provide some defined job - in this case it'll download a web page. When it's finished, it'll assign its output to another variable, "doc_text". "doc_text" will now hold the actual text of the website - not the visual form you see in your browser, but the source code, including all the tags. Since this form is not very easy to parse, we'll use another function, html.fromstring, to generate a special representation where we can easily address elements - the so-called document object model (DOM).

----
6. for row in doc.cssselect("#tblEvents tr"):
7.	link_in_header = row.cssselect("h4 a").pop()
8.	event_title = link_in_header.text
9. 	print event_title
----

Lines 6-9. In this final step, we use the DOM to find each row in our table and extract the event's title from its header. Two new concepts are used: the for loop and element selection (.cssselect). The for loop essentially does what its name implies - it will traverse a list of items, assigning each a temporary alias ("row" in this case) and then run any indented instructions for each item.

The other new concept, element selection, is making use of a special language to find elements in the document. CSS selectors are normally used to add layout information to HTML elements and can be used to precisely pick an element out of a page. In this case (Line. 6) we're selecting "#tblEvents tr" which will match each <tr> within the table element with the ID "tblEvents" (the hash simply signifies ID). Note that this will return a list of <tr> elements.

As can be seen on the next line (Line. 7), where we're applying another selector to find any <a> (which is a hyperlink) within a <h4> (a title). Here we only want to look at a single element (there's just one title per row), so we have to "pop" it off the top of the list returned by our selector with the .pop() function.

Note that some elements in the DOM contain actual text, i.e. text that is not part of any markup language, which we can access using the "[element].text" syntax seen on line 8. Finally, in line 9, we're printing that text to the ScraperWiki console. If you hit run in your scraper, the smaller window should now start listing the event's names from the IAEA web site.

=== Crowdsourcing data by Marianne Bouchart (Data Journalism Blog) ===

////

Insert: picture relevant to this section

////

This sub-chapter will show what is data crowdsourcing an dhow it can be used both as a collection tool and a helping hand to sort out existing datasets. We will be examining the examples of  two organisations, Buzz Data and the Guardian, and how they use data crowdsourcing in their work and make it easier for the audience and other journalists to get involved and have a go at it.
definition of crowdsourcing data for journalism

- Crowdsourcing is the act of sourcing tasks traditionally performed by specific individuals to a group of people or community (crowd) through an open call.

Jeff Howe from Wired Magazine established that the concept of crowdsourcing depends essentially on the fact that because it is an open call to a group of people, it gathers those who are most fit to perform tasks, solve complex problems and contribute with the most relevant and fresh ideas.

It has been used by journalists to turn readers into contributors for a few years now, some more successfully than others.

'When you have tones of files, statistics, reports to go through, like with the MPs expenses or the wikileaks cables, crowdsourcing is a really good way to get things done,' argues Simon Rogers, editor of the Guardian's Data Blog in an interview conducted for this handbook.

Data crowdsourcing can be helpful both for collecting data and sorting out big datasets you already have but that are too big for you to sort out alone. It's also a very good way to get different perspectives on a subject you work on as the audience will often point out details that you didn't notice on your own. 
Although this trend to ask the audience to become contributors in the analysis of big data has been a real success in projects such as the MPs expenses in the UK or the Wikileaks cables worldwide, the actual 'collection' of data via crowdsourcing stays a problem as journalists and editors still don't know how to make sure the data people contribute is reliable.
To crowdsource data, journalists can use surveys, polls, or comment fields. They can also ask their audience to contribute to a Google Doc or a spreadsheet. 

Online resources like the Open Data Cook Book gives you help on how to collect data using Google forms.

'Google forms (part of the Google Docs online office suite) allow you to collect and share data, the wesbite says.

'This recipe isn't about using an existing source of data, but instead allows you to create your own data. You might want to run a survey, or log events that occur. With Google forms you can easily share the data you collect as a spreadsheet.'

Go to the Open Data cook book website to follow their tutorial.

Other platforms such as Help Me Investigate, Crowdsourcing.com or GetTheData.org offer good guidance on how to get the crowd involved in your data gathering. 

(intro to buzz data)

Video interview with Nick Edouard from Buzz Data: Data crowdsourcing meets social media http://vimeo.com/36859634 

Nick: 'I am the executive vice president of business development and marketing at Buzz Data. I got into data through Buzz Data CTO Pete Forde who I've known for a while and who I think is a visionary in this space. Pete introduced me to open data and data in general. I have worked with a lot of data in previous jobs but it was a privilege to come on board at Buzz Data. 

Q: What's Buzz Data and how was it created?

'Buzz Data makes data sharing and collaboration on both private and public datasets very easy. It's important that we are not just about open data. We love open data, but we are really focused on how people interact with data of all type whether they are public or private. So what we've done is create essentially a platform that makes it very easy to add context, visualisations, articles, links around a dataset to create a story rather than just a dateset per say. An other major feature is the degree of user engagement that you can get. Not only have we incorporated some of the best features of social networks into Buzz Data, but at the same time, we've made it very easy for you to find collaborators to work with on a dataset. Whether as I said, it's in a public fashion or indeed privately, just between you and me for example.'

Q: So do you consider yourself as the social medium of data?

'I guess there are a lot of ways in which Buzz Data can be described: it's a social network for data, it's a gate hump for data, etc. I think what we are doing is unique. I can't see anything in the market that is the same thing. We really think about what data needs to be to become meaningful, to become useful, to add insight. And that is really two things: the context and the engagement.'

Q: How do you deal with truthfulness and trust on Buzz Data? How can people make sure the data they crowdsource is reliable?

'From a truthfulness prospective, obviously, we are looking to the community tu curate itself, I guess in that degree, our model is somewhat similar to Wikipedia: we are expecting people to engage, to point out errors and inacuracies in the datasets. I very much believe that we gonna find that communities of interest around topics are very engaged and as a result will self police.'

Q: So you don't have a webmaster whose role is to verify the data that is on the site?

'We gonna be doing that in the future, we gonna verify that the people that are publishing say who they are. At the moment we have The Economist Intelligence Unit, the Globe and Mail, a Canadian national newspaper, who are publishing data. We know who they are, but that is certainly something that we are working on and we will be able to verify who the data publisher is so that as a data user, you gonna have confidence in the veracity of the data. The other thing as well is that there's a virtual control on Buzz Data so if somebody post an original dataset, you can download their data, play around with it and if there's something you don't like or might not agree with some of the data points in it, you can push that back, upload that and point out innacuracies in the original datasets. I think this is a place for data to live, it is not overwritten it will very much evolve.' 

Q: Are people able to share their activity on Buzz Data with other sociel networks like Twitter or Facebook?

'Yes, without a doubt. We have already done the overall integration so at the moment you can see who is already on Buzz Data from existing from existing social networks (Linked'in, Twitter and Facebook). But we are working on it so you can see more easily your activity on Buzz Data through your other social networks. Hand in hand with that we are working on some features for data publishers so that they can widgetize a lot of the Buzz Data functionalities. They are going to be able to display what is happening on Buzz Data with their datasets within their own landing pages and normal websites, etc.'

Q: How useful can Buzz Data be to data journalists?

'I think it can be enormously useful for data journalists. We have our own in-house data journalist Momoko Price who is our communication director. There is a number of ways [our site acn be useful to data journalists], 
Buzz Data can be used privately to collaborate on datasets. Let's say a group of two or three investigative journalists could use Buzz Data to essentially work out the story to get it to the point where they are ready to publish. 
Once you are ready to publish then obviously the story can go to print, you can link potentially to the underlining dataset it is based on, you can publish that on Buzz Data openly and then see what the community does to it. I think it's going to be really interesting to see how people take that story forward. We've already seen this first-hand on Buzz Data in a couple of occasions where the community has become engaged with a dataset, they've taken it in a different direction to the guy that originally written the story an published it and as a result there is a follow up story and not only that but the community is far more engaged with the story than it would have been otherwise.'


Q: What do you think about data journalism in general and the way it is evolving today?

'I think it is massively important. I think it is very important for the future of open data and open data full stop actually. As someone described, data journalists are essentially data engineers. You guys are going to be taking data as a raw ingredient and turn it into something that's useful. And I think thats the oil in the machine essentialy. One thing that the open data mouvement need to address full stop is when data is published, something has to happen with it otherwise people will be less inclined to publish it. Great things do happen when data is shared and published so I think data journalists' job is gonna be key to that.'

Q: The Open Data movement campaigns for more openess from governments and big organisations to publish more data online. How do you see this evolve in the future?

'I hope we'll stop talking about it as needing to evolve, I think it's gonna become part of the course sooner rather than later. I am really encouraged by the activity of some governments, the UK for example is very good in this regard. Is there any room to be better? Without a shutter of a doubt. I think part of that too is adding the context around the dataset. So thinking about how things become published, the more information you can wrap around it is obviously the better, the more useful it becomes. I don't think it's just about open data too. I think open data tends to be too much focused on kind of governments', etc. I think a lot of organisations, a lot of private companies are starting to realise the value in sharing data. Obvisouly there are types of data that will never be made public, I don't want my credit card information to ever be public, and at Buzz Data we fully understand that there is information that will always be confidential to companies. I think the open data movement will continue and I think thrive. I am excited about that but I think there's also gonna be more and more private organisations that will be looking in-house and thinking ‚Äòwhat should we be sharing with our consumers? And how do we benefit from that?'

Q: So what's next for Buzz Data?

' The way I see it is that Buzz Data is good and it's only gonna get better from hereon in. We have some very exciting things on the road map, we are working on things like smarter recommendation algorythms, and better around discoverabilitty, producing widgets for data publishers, how do you embed functionality on your website. 

We are working on improving the collaboration side of it, it is far better that anything I can see out there at the moment but I think we can still make it better. A lot that I have been doing in the past couple of weeks is actually talking to epole that have been using the service, getting their feedback and then we'll build that in the product road map. So we are listening to the people who are using the service at the moment, we gonna make it better based on their feedback. It's already jolly good but I think we can make it better.'

Q: Three golden rules to data?

'Three golden rules to data? 
I think firstly data needs context. So if you are thinking about sharing or publishing data, think about what else you can provide like ‚Äòthis heading in this spreadsheet is actually composed of the following three or four factors'. Because that's gonna enable your user community, fondamentally people who want to play with your data to actually know where they should take it. You gonna be able to give them more of a steer. So context is important..
Secondly is to think about engagement. How best to stimulate it within your community so rather than just publishing and like the Duke of Wellington 'being damned', do not just sit back. Imagine stuff that's gonna happen, I think you do need to think ‚Äòwho do we need to tell that we've done this?' That's one marking that needs to be put in place and then stimulate that activity. 
I guess the third would be ‚ÄòPublish! Publish! Publish!' The more the merrier, I think great things happen when data is shared. You just have to see the NASA dark matter competition last year, the Netflix recommendation algorythm, a contest from a while back... It's not just advances in science, there are also great things commercially that happened and I hope Buzz Data is going to be a big part of those stories over the next couple of years. 

Something great is going to happen on Buzz Data sooner rather than later and I am excited for that.' 
How to collect data from content already published in social media 

using Twitter hashtags (examples: emergency hashtags used in the event of an earthquake or huricane in the US)
using Tools such as:

People Browser
Social mention (media monitoring tools)
Disqus

Keyword search, what is discussed about specific topics on the web? 
Look for the right keywords. Certain topics have different words in different countries (football, soccer, etc.). 
Make your search as specified or as wide as possible, as adapted as possible to your search. 
consider language, country, target groups 
gather information from different platforms to get a common opinion
cross check to make sure the information is reliable and consistent 

Data Crowdsourcing at the Guardian - Getting the readers involved


While there is already a huge amount of data available online, some media organisations are going one step further by getting their readers involved in the process of collecting and analysing data. 

'Crowdsourcing' has become a common practice in the world of data journalism and the Guardian is now considered as a reference in the field. 

The Guardian's Data Blog won the 2011 Newspaper Awards prize for Best use of New Media and the Guardian's DataStore was honoured at the Knight Batten awards for innovation in journalism in 2011.

We went to the Guardian's offices in London to talk to their data journalism team about data crowdsourcing and the different projects they have recently set up.

Simon Rogers is the editor of the Data Blog. He told us how crowdsourcing can help when other ways of collecting and analysing data are not enough... 

SIMON ROGERS, Editor of the Data Blog:

' I suppose the thing about crowdsourcing is that when you have tones of stuff to go through, like with the MPs expenses where there were 40,000 pages to go through, that's impossible for one person to do. Also, when you have got all this stuff that is inaccessible or in a bad format, that's really where crowdsourcing can help. 

One thing the Guardian has got is lots of readers, lots of pairs of eyes, and if there is something interesting to do then it can really work. That's what we did with the MPs expenses. We had 450,000 documents and very little time to do anything. So what better way than open up to readership? 

But what we found with the MPs expenses is that what we generated was tip-offs, stories more than data. So although it was remarquably successful in terms of traffic, people really liked it, it wasn't in terms of actual raw data. It was hard to know who we could trust.

We are doing something at the moment with MixMag on drug use and that has been phenomenal as well, it looks like it is going to be bigger than the British crime survey in terms of how many people come back to it. It's brilliant. 

I think what those things have in common is that they are things people care about so they are willing to spend the time. A lot of the crowdsource we have done mostly relies on the obsessives. So with the MPs expenses we had a massive amount of traffic at the beginning and it really died down. But what we still got are people that are obsessively going through every page looking for that story. One person has done 30,000 pages. They know a lot of stuff.

So in terms of generating stories it worked really well and people really liked it, it made the Guardian ‚Äòlook good', but I think in terms of generating raw data, not so much. 

Then we did it with the Sarah Palin papers, that was again a good way of looking for stories, scouring the raw information for stories. 

But it also seems to me that some of the [crowdsourcing projects] that we've done that worked really well have been more like ‚Äòall fashioned' surveys. When you are asking people about their experience, about their lives, about what they've done, they work very well because people aren't as likely to make that up. They will say what they feel. When we asked people to kind of do our job for us, you have to find a framework for people to produce the data in a way you can trust them.'

Q: You were talking about reliability, how do you deal with that on a daily basis? How do you trust people to contribute? 

 'I think the approach that Old Weather have got, where they get ten people to do each one, is a really good test on that. It is an old fashioned computer science technique to get a warm up as a train starter. I think that's a very good way to do it. 

With the MPs expenses, we tried to minimise the risk of MPs going online and editing their own records to make themselves look better. But you can't permanently guard that site, you can only look out for certain urls or if it's coming from the SW1 area of London. So that's a bit trickier. That's where I felt out of it because I felt like the data we were getting out was not reliable. Even though stories were great, it wasn't producing raw numbers that we could confidently use.'

Q: Could you give a piece of advice to journalists who want to use crowdsourcing to collect data online?

'I would say:
making it something personal to people really helps,
otherwise make it about what the people care about, or are gonna care about after, even when the news dies down, where they will still want to get involved.
also, what is really worth it is when you make it more like a game. Second time around when we did the expenses story, it was much more like a game, with individual tasks for people to do. It really help to give people specific tasks and things to do. That made a big difference because I think if you just present people with the mountain of information to go through, it's a hard work, that's what we get paid for. So I think making it fun is really important.'


In October 2011 The Data Blog set up a crowdsourced map of the Occupy movement. 

Asking people to map where protests were taking place around the world, the Guardian data team managed to get hundreds of contributors and over 6000 Facebook shares by simply using a Google form. 

The result is a very straight forward map of the Occupy movement worldwide which is still being updated today.

The Guardian's data journalism team has done many of these projects in the past few years, each of them proved to be very successful. So we asked them for tips on how to use crowdsourcing in data journalism and we asked Data journalist James Ball to tell us about the Guardian's most successful crowdsourcing projects...

JAMES BALL, data journalist at the Guardian

'I think the one that got the biggest response was something that we did on olympic ticketing. Thousands of people in the UK tried to get tickets for this year's Olympics and there was a lot of fury that people hadn't got them. People had ordered hundreds of pounds worth and been told they'll get nothing. But no one really knew if it was just some people complaining quite loudly while actually most people were happy. 

So we tried to work out a way to find out. We decided the best thing we could really do, in the lack of any good data, was to ask people. And we thought we'd have to treat it as a light thing because it wasn't a balanced sample. 

We created a Google form and asked very specific questions. It was actually a long form, it asked how much in value people had ordered their tickets, how much their card had been debited for, which events they went for, this kind of thing. 

We put it up as a small picture on the front of the site but it was shared around really rapidly. I think this is one of the key things, you can't just think ‚Äòwhat do I want to know' for my story', you have to think ‚Äòwhat are people wanting to tell me right now'. And it's only when you tap into what are people wanting to talk about that crowdsourcing is going to be successful. 

But the volume of responses on this, which is one of the earliest times we have ever tried to do this approach, was huge. We had a thousand responses in less than an hour and seven thousands by the end of that day. 

So obviously we took presenting the results a bit more seriously at this point, we had no idea how well it would do. So we put on some cavy hats you know, Guardian readers may be more wealthy than other people, also people who got less than they expected might be more willing to talk to us. 

So we didn't know how much value the results would have. So we did that and cut it down and we had a good seven thousand records to base it on and we found about half the people who'd asked for tickets had got nothing, how many people had ordered and got. We ran all of this stuff, and obviously because so many people had taken part the day before, there was a lot of interest in the results. 

A few weeks later, the official summary report came out, and our numbers were shockingly close, they were almost exactly spot on. I think partly through luck but also because we got just so many people.

Generally though, I tend to find that crowdsourcing is more useful for the content around the data story or for getting people to follow one up. So if we publish a story based on data that we do internally and I do quite a lot of work on lobying, I publish the full register or the whole thing that I based it on because sometimes you will get someone who doesn't like a particular MP or Lord who will say ‚ÄòOh, that research doesn't declare anything but I know that they work for here'. Now you love to get that as an email rather than on your open tools that you usually use but that's fine. You know, it's all about learning how to follow up. So while it is less frequent that it helps you compile data, it often lets you do more with the data than you could yourself. You can only know so many people.

Q: Can you talk me through the process of going through all the data you collected, you had about 7000 responses, how did you deal with this huge amount of data?

'We'd preempted that we wanted to do analysis on it, which helped. If you start something in a comment thread, it is often too late to turn it into a usable way. So you have to set out at the start and think ‚Äòwhat's the best tool for what I want to know?' Is it a comment thread, is it building an app, and if it is building an app, you have to think ‚ÄòIs this worth the wait? And is it worth the resource to do it?' 

In this case we thought of Google forms and if someone fills that out it turns into a row on a spreadsheet. So it meant that straight away, even if it was still updating, I could open the spreadsheet and see all of the results. 

I could have tried to do the work in Google but I downloaded it into Microsoft Excel and then did things like sort it from low to high and found the people who decided to write in instead of putting digits on how much they spent and fixed all of those. I decided not to exclude as little as I could. So rather than taking only valid responses, I tried to fix other ones. People had used foreign currencies so I converted them to sterling, all of which was a bit painstaking. 

But the whole analysis was done in a few hours, then I knocked out the obviously silly entries. A lot of people decided to fill it out pointing out they spent nothing on tickets, that's a bit facetious but fine, I mean, that was less than a hundred out of seven thousands several hundreds. 

And then a few dozens who put obviously fake high amounts to try and destort it. I am talking things like ten million pounds in there, I wasn't jumping to assumptions. So that left me with a set that I could use with the normal data principles we use every day. I did what's called a ‚Äòpivot table', I did some averaging, that kind of thing. 

So because we planned it ahead, it meant it was quite simple, if we had stumble into it just through a comment thread, it would have been harder. It shows the value of thinking about what you might do with the results of crowdsourcing in advance.'

Q: There was a lot of manual work to sort out the data, how many people did you have on this?

'Just me. We hadn't really any idea it would get the momentum it did so I worked with the Sports blog editor, we put our heads together and thought this might be a fun project. We did it, start to finish, in 24 hours. We had the idea, we put something up at lunch time, we put it on the front of the site, we saw it was proving quite popular, we kept it on the front of the site for the rest of the day and we presented the results online the next morning.'

Q: What made you choose Google Doc for this, compared to other poll or survey tools that are available online?

'It's because it gives complete control over the results. I don't have to use anyone else's analytic tools. I can put it easily into a database software or into spreadsheets which was enough for that job. When you start using specialist polling softwares, you are often going through their tools and their restrictions. I think if the information we'd been asking for was particularly sensitive, we might have hesitated before using Google and thought about doing something ‚Äòin house'. But generally, the ease of dropping a Google Form into a Guardian page and it's virtually visible to the user that we are using one, is so convenient. 

Q: What advice would you give to other journalists who want to give a go to data crowdsourcing, in terms of what questions to ask the audience or how to tackle the subject?

'- You have to have very specific things you want to know, and as much as possible, ask things that get multiple choice responses.
Try to get some basic demographics of who you are talking to so you can see if your sample might be biased.
If you are asking for amounts and things like this, try in the guidance to specify that it's in digits, that they have to use a specific currency and things like that. A lot won't, but the more you hold their hand through, the better.
and always, always put on a comment box because a lot of people will fill out the other things but what they really want is to give you their opinion on the story, specially on a consumer story or an outrage. You might actually be interested in only a few of the controlled fields but as long as they have got a chance to comment and as long as you use and reflects on some of the interesting ones, you get a much greater degree of response.'

trust/ethical issues around data crowdsourcing/ limitation


There is still a lot to be done to make crowdsourcing data more reliable.
As Nick Davies said in a discussion at the Frontline Club: 'There's some kind of flaw in the theory that crowdsourcing is a realistic way of converting data into information and stories, because it doesn't seem to be happening.' Nick Davies: Data, crowdsourcing and the ‚Äòimmeasurable confusion' around Julian Assange

To engage better with their audience and to try and develop a more reliable approach to crowdsourcing, some organisations create a ranking/reward system to establish trust within the community that contributes to their data (The Guardian, Citizen side, etc.)

Most importantly, webmasters are needed to verify and clean the data along the way.

In the data crowdsourcing process, journalists need to give precise guidelines to their 'crowd', asking people to give a source link to verify the information they contribute.

 

END

Relevant sources for other chapters? :

For the getting data chapter:
Are those new resources mentioned? --> Ocean.data.gov, Geo.data.gov (which joins Energy.data.gov, health.data.goc and data.gov/Law)

I will be interviewing Jeanne Holm, who is:
- Co-Chair, eGovernment Interest Group at W3C
- Data.gov, Evangelist at GSA
- Chief Knowledge Architect at Jet Propulsion Laboratory
- Chair, Knowledge Management at NASA
- Instructor at UCLA
- Member at International Academy of Astronautics
- Chair, IAF Web and KM for Space at International Astronautical Federation
(yes, she is all that ;D)
I can hurry the interview to this week if you think it would be useful to the handbook. Just let me know.

I also interviewed David Stevenson, open data consultant in the US and author of Data Dynamite about open public data if that could be of any interest, let me know... I attached the audio file to this email so you can see whether it would be interesting or not to integrate that in the handbook. It is quite long but I was thinking of maybe using just some parts of it like:
 7m56sec Some people, some journalists even, think that open data is not for them because they are scared of numbers. What would you tell them?
11m41sec on data visualisation tools
14min23sec What do you think about data journalism and data visualizations, how important do you think they will be in the future?
15min 09sec Stephenson's theory about the 2008 collapse and how data sharing could have prevented it.
22min 05sec The impact that dat.gov has had in the US
25min 05sec Is the US leading the way in terms of data liberation?
39min 32sec The risks of real-time open data
41min51sec 3 golden rules to access data online
----
The UNECE Making Data Meaningful guides

http://www.unece.org/stats/documents/writing.html 

Making Data Meaningful - Part 1: A guide to writing stories about numbers, by the United Nations Economic Commission for Europe http://www.unece.org/fileadmin/DAM/stats/documents/writing/MDM_Part1_English.pdf

Making Data Meaningful - Part2: A guide to presenting statistics by the United Nations Economic Commission for Europe http://www.unece.org/fileadmin/DAM/stats/documents/writing/MDM_Part2_English.pdf 
-----



You can now see a basic scraper operating: it downloads the web page, transforms it into the DOM form and then allows you to pick and extract certain content. Given this skeleton, you can try and solve some of the remaining problems using the ScraperWiki and Python documentation:
Can you find the address for the link in each event's title?
Can you select the small box that contains the date and place by using its CSS class name and extract the element's text?
ScraperWiki offers a small database to each scraper so you can store the results - copy the relevant example from their docs and adapt it so it will save the event titles, links and dates.
The event list has many pages - can you scrape multiple pages to get historic events as well?



As you're trying to solve these challenges, have a look around ScraperWiki: there are many useful examples in the existing scrapers - and quite often, the data is pretty exciting, too. This way, you don't need to start off your scraper from scratch: just choose one that is similar, fork it and adapt to your problem.
Further Reading

Other Tutorials:
Scraperwiki screencast for non-programmers: http://blog.scraperwiki.com/2011/08/15/scraperwiki-tutorial-screencast-for-non-programmers/
ProPublica tutorial: http://www.propublica.org/nerds/item/scraping-websites



Handling PDF:
PDFs generated from Text files (Bad): http://www.propublica.org/nerds/item/turning-pdfs-to-text-doc-dollars-guide
PDFs generated from Images (Worse): http://www.propublica.org/nerds/item/image-to-text-ocr-and-imagemagick



Working with APIs:
http://www.poynter.org/how-tos/digital-strategies/138211/beginners-guide-for-journalists-who-want-to-understand-api-documentation/[Beginner's guide for journalists who want to understand API documentation]
Working with APIs: http://chronicle.com/blogs/profhacker/working-with-apis-part-1/22674[http://chronicle.com/blogs/profhacker/working-with-apis-part-1/22674]
http://code.google.com/apis/chart/image/docs/making_charts.html[Getting Started With Google Charts] and Interactive Charts. There are also samples in the http://code.google.com/apis/chart/interactive/docs/gallery.html[Google Visualization API Gallery].
http://onlinejournalismblog.com/2011/07/22/how-to-grab-useful-political-data-with-the-they-work-for-you-api/[How to grab useful political data with the They Work For You API]





**3.4 Crowdsourcing Expert Opinions: Annotating and analysing datasets**

Overview: How to enable people to annotate and comment on datasets (particularly, harnessing expert opinions)
Thinking about your audience - how do you engage the right type of people?
Do's and don'ts - lessons learnt
Tools & Technical considerations
Write-access - can people damage your data?
File-sharing
Getting the data out
Authors: Ted Han?
Length: 1 page
