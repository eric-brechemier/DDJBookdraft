== 3. Case studies ==

////

So you have the data... what next? Where do you start and where could you end?

In this section, top data-journalists take us through useful tips and tricks for how they have produced their data-powered stories before diving deeper into what made them determined to work on a particular project, why they chose the methods they did, the barriers they encountered, lessons learned and the eventual impact the stories made....

More case studies

http://www.guardian.co.uk/politics/2009/may/15/mps-expenses-heather-brooke-foi[MPs expenses scandal] in the UK is an example of how a simple Freedom of Information (FOI) request can trigger a large scale investigation. An FOI request uncovered some interesting stories about how elected officials spend taxpayers' money.
http://blogs.lanacion.com.ar/projects/data/subsidies-for-the-bus-transportation-system-datajournalism-project-in-argentina-la-nacion/[Subsidies for the Bus Transportation System]: Angelica Peralta Ramos , LA NACION (Argentina)

case study 1

You've poured hours into tidying up and working with your data and got your story. What do you do with the data now? In this chapter, we take a look at cases where journalists have decided to open up their datasets and publish them along with their stories: What role did the datasets play in the original story? What was the motivation for opening them up? What did others choose to do with the data?
////

=== Data Stories by Martin Rosenbaum (BBC) ===

Data journalism can sometimes give the impression that it is mainly about presentation of data - such as visualisations which quickly and powerfully convey an understanding of an aspect of the figures, or interactive searchable databases which allow individuals to look up say their own local street or hospital. All this can be very valuable, but like other forms of journalism, data journalism should also be about stories. So what are the kinds of stories you can find in data? Based on my experience at the BBC, I have drawn up a list or `typology' of different kinds of data stories.

I think it helps to bear this list below in mind, not only when you are analysing data, but also at the stage before that, when you are collecting it (whether looking for publicly available datasets or compiling freedom of information requests).

*1. Measurement*

The simplest story - counting or totalling something.

`Local councils across the country spent a total of £x billion on paper clips last year'.

- but it's often difficult to know if that's a lot or a little. For that, you need context - which can be
provided by:

*2. Proportion*

`Last year local councils spent two-thirds of their stationery budget on paper clips'

- or

*3. Internal comparison*

`Local councils spend more on paper clips than on providing meals-on-wheels for the elderly'

- or

*4. External comparison*

`Council spending on paper clips last year was twice the nation's overseas aid budget'

- or there are other ways of exploring the data in a contextual or comparative way:

*5. Change over time*

`Council spending on paper clips has trebled in the past four years'

- or

*6. `League tables'*

(these are often geographical or by institution, and you must make sure the basis for comparison is fair, eg taking into account the size of the local population)

`Borsetshire Council spends more on paper clips for each member of staff than any other local authority,at a rate four times the national average'

- or you can divide the data subjects into groups:

*7. Analysis by categories*

`Councils run by the Purple Party spend 50% more on paper clips than those controlled by the Yellow Party'

- or you can relate factors numerically

*8. Association*

`Councils run by politicians who have received donations from stationery companies spend more on paper clips, with spending increasing on average by £100 for each pound donated'

(but, of course, always remember that correlation and causation are not the same thing).

So if you're investigating paper clip spending, are you also getting the following figures:
- total spending to provide context?
- geographical/historical/other breakdowns to provide comparative data?
- the additional data you need to ensure comparisons are fair, such as population size?
- other data which might provide interesting analysis to compare or relate the spending to?

=== Hospital billing by Steve Doig (Walter Cronkite School of Journalism and Mass Communication) ===

////

Insert: picture from project

////

Investigative reporters at http://californiawatch.org/[CaliforniaWatch] had gotten tips from sources that a large chain of hospitals in California might be systematically gaming the federal Medicare program that pays for the costs of medical treatments of Americans aged 65 or older. The particular scam that was alleged is called "upcoding", which means reporting patients having more complicated conditions - worth higher reimbursement - than actually existed. But a key source was a union that was fighting with the hospital chain's management, and the CaliforniaWatch team knew that independent verification was necessary for the story to have credibility.

Luckily, California's department of health has public records that give very detailed information about each case treated in all the state's hospitals. The 128 variables include up to 25 diagnosis codes from the "International Statistical Classification of Diseases and Related Health Problems" manual (commonly known as ICD-9) published by the World Health Organization. While patients aren't identified by name in the data, other variables tell the age of the patient, how the costs are paid and which hospital treated him or her. The reporters realized that with these records, they could see if the hospitals owned by the chain were reporting certain unusual conditions at significantly higher rates than were being seen at other hospitals.

The data sets were large - nearly 4 million records per year, and the reporters wanted to study six years worth of records in order to see how patterns changed over time. They ordered the data from the state agency; it arrived on CD-ROMs that were easily copied into a desktop computer. The reporter doing the actual data analysis used a system called http://www.sas.com/[SAS] to work with the data. SAS is very powerful (allowing analysis of many millions of records) and is used by many government agencies, including the California health department, but it is expensive - the same kind of analysis could have been done using any of a variety of other database tools, such as Microsoft Access or the open-source http://www.mysql.com/[MySQL].

With the data in hand and the programs written to study it, finding suspicious patterns was relatively simple. For example, one allegation was that the chain was reporting various degrees of malnutrition at much higher rates than were seen at other hospitals. Using SAS, the data analyst extracted frequency tables that showed the numbers of malnutrition cases being reported each year by each of California's more than 300 acute care hospitals. The raw frequency tables then were imported into Microsoft Excel for closer inspection of the patterns for each hospital; Excel's ability to sort, filter and calculate rates from the raw numbers made seeing the patterns easy.

Particularly striking were reports of a condition called Kwashiorkor, a protein deficiency syndrome that almost exclusively is seen in starving infants in famine-afflicted developing countries. Yet the chain was reporting its hospitals were diagnosing Kwashiorkor among elderly Californians at rates as much as 70 times higher than the state average of all hospitals. http://californiawatch.org/health-and-welfare/hospital-chain-already-under-scrutiny-reports-high-malnutrition-rates-8786[Read the story].

For other stories, the analysis used similar techniques to examine the reported rates of conditions like septicemia, encephalopathy, malignant hypertension and autonomic nerve disorder. Read the story. And another analysis looked at allegations that the chain was admitting from its emergency rooms into hospital care unusually high percentages of Medicare patients, whose source of payment for hospital care is more certain than is the case for many other emergency room patients. http://californiawatch.org/health-and-welfare/prime-healthcares-treatment-rare-ailments-stands-out-13021[Read the story].

To summarize, stories like these become possible when you use data to produce evidence to test independently allegations being made by sources who may have their own agendas. These stories also are a good example of the necessity for strong public records laws; the reason the government requires hospitals to report this data is so that these kinds of analyses can be done, whether by government, academics, investigators or even citizen journalists. The subject of these stories is important because it examines whether millions of dollars of public money is being spent properly.

=== Care home crisis by Cynthia O'Murchu (Financial Times) ===

////

Insert: picture from project

////

A Financial Times investigation into the quality of care provided in care homes began with a simple question: is care provided by private providers better than that offered by the non-profit sector or the government?

The analysis was timely, because the financial problems of Southern Cross, then the country's largest care home operator, were coming to a head. The government had for decades promoted a privatization drive in the care sector and continued to tout the private sector for its astute business practices.

Our inquiry began with analyzing data we obtained from the UK regulator in charge of inspecting care homes. The information was public, but it required a lot of persistence to get the data in a form that was usable.

The data included ratings (now defunct) on individual homes' performance and a breakdown of whether they were private, government-owned or non-profit. The Care Quality Commission (CQC), up to June 2010, rated care homes on quality ranging from a verdict of "zero stars - poor" to "three stars - excellent".

The first step required extensive data cleaning, as the data provided by the Care Quality Commission for example contained categorizations that were not uniform. This was primarily done using Excel. We also determined - through desk and phone research - whether particular homes were owned by private-equity groups. Before the financial crisis, the care home sector was a magnet for private equity and property investors, but several - such as Southern Cross - had begun to face serious financial difficulties. We wanted to establish what effect, if any, private equity ownership had on quality of care.

A relatively straight forward set of Excel calculations enabled us to establish that the non-profit and government-run homes on average performed significantly better than the private sector. Some private equity-owned care home groups performed well over average, and others well below average.

Paired with on-the-ground reporting, case studies of neglect, an in-depth look at the failures in regulatory policies as well as other data on levels of pay, turnover rates etc, our analysis was able to paint a picture of the true state of elderly care.

Some tips:

  * Make sure you keep notes on how you manipulate the original data.
  * Keep a copy of the original data and never change the original.
  * Check and double check the data. Do the analysis several times (if need be from scratch).
  * If you mention particular companies or individuals, give them a right to reply.

=== A 9 month investigation into European Structural Funds by Cynthia O'Murchu (Financial Times) ===

////

Insert: picture from project

////

In 2010, the Financial Times and the Bureau of Investigative Journalism (BIJ) joined forces to investigate European Structural Funds. The intention was to review who the beneficiaries of European Structural Funds are and check whether the money was put to good use. At ‚Äö√á¬®347bn over seven years Structural Funds is the second largest subsidy programme in the EU. The programme has existed for decades, but apart from broad, generalised overviews, there was little transparency about who the beneficiaries are. As part of a rule change in the current funding round, authorities are obliged to make public a list of beneficiaries, including project description and amount of EU and national funding received.

The project was done with up to 12 journalists and one coder and took nine months. Data gathering alone took many months. The project resulted in five days of coverage in the Financial Times and the BIJ, a BBC radio documentary, and several TV documentaries.

Before you tackle a project of this level of effort, you have to be certain that the findings are original, and that you will end up with good stories nobody else has.

The process was broken up into a number of distinct steps:

*1. Identify who keeps the data and how it is kept*

The EU Commission's Directorate General for the Regions have a http://ec.europa.eu/regional_policy/index_en.cfm[portal] to the websites of regional authorities that publish the data. We believed that the EU commission would have an overarching database of project data that we could either access directly, or which we could obtain through a Freedom of Information request. No such database exists to the level of detail we required. We quickly realised that many of the links the Commission provided were faulty and that most of the authorities published the data in PDF format, rather than analysis-friendly formats such as CSV or XML.

A team of up to 12 people worked on identifying the latest data and collating the links into one large spreadsheet we used for collaboration. Since the data fields were not uniform (for example headers were in different languages, some data sets used different currencies, some included breakdowns of EU and National Funding) we needed to be as precise as possible in translating and describing the data fields available in each data set.

*2. Download and prepare the data*

The next step consisted of downloading all the spreadsheets, PDFs and, in some cases ,web scraping the original data.

Each data set had to then be standardised. Our biggest task was extracting data out of PDFs, some hundreds of pages long. Much of this was done using UnPDF and ABBYY FineReader, which allow data to be extracted to formats such as CSV or Excel.

It also involved checking and double checking that the PDF extraction tools had captured the data correctly. This was done using filtering, sorting and summing up totals (to ensure it corresponded with what was printed on the PDFs).

*3. Create a database*

The team's coder set up a SQL database. Each of the files prepared were then used as a building block for the overall SQL database. A once a day process would upload all the individual data files into one large SQL database, which could be queried on the fly through its front end by using queries. 

*4. Double-checking and analysis*

The team analysed the data in two main ways:

a. via the database front end. This entailed typing particular keywords of interest (e.g. "tobacco", "hotel", "company A" in to the search engine. With help of the google translate function, which was plugged into the search functionality of our database, those keywords would be translated into 21 languages and would return appropriate results. These could be downloaded and reporters could do further research on the individual projects of interest.

b. macro-analysis using the whole database. Occasionally, we would download a full data set, which could then be analysed for example using keywords, or aggregating data by country, region, type of expenditure, number of projects by beneficiary etc.

Our story lines were informed by both these analyses, but also through on the ground and desk research.

Double-checking the intergrity of the data (by aggregating and checking against what authorities said had been allocated) took a substantial amount of time. One of the main problems was that authorities would for the most part only divulge the amount of "EU and national funding". Under EU rules, each programme is allowed to fund a certain percentage of the total cost using EU funding. The level of EU funding is determined, at programme level, by the so-called co-financing rate. Each programme (e.g. regional competitiveness) is made up of numerous projects. At the project levels, technically one project could receive 100 per cent EU funding, and another none at all, as long as grouped together, the amount of EU funding at the programme level is not more than the approved co-financing rate.

This meant that we needed to check each EU amount of funding we cited in our stories with the beneficiary company in question.

Read the story:

  * http://www.ft.com/intl/eu-funds[Financial times coverage]
  * http://www.thebureauinvestigates.com/category/projects/europes-hidden-billions/[The Bureau of Investigative Journalism Coverage]

=== The tell-all telephone by Sascha Venohr (Zeit Online) ===

////

Insert: picture from project

////

Most people's understanding of what can actually be done with the data provided by our mobile phones is theoretical; there were few real-world examples. That is why Malte Spitz from the German Green party decided to publish his own data. To access the information, he had to file a suit against telecommunications giant Deutsche Telekom. The data is the basis for ZEIT ONLINE's accompanying interactive map, were contained in a massive Excel document. Each of the 35,831 rows of the spreadsheet represent an instance when Spitz's mobile phone transferred information over a half-year period.

Seen individually, the pieces of data are mostly harmless. But taken together, they provide what investigators call a profile - a clear picture of a person's habits and preferences, and indeed, of his or her life. This profile reveals when Spitz walked down the street, when he took a train, when he was in an plane. It shows that he mainly works in Berlin an which cities he visited. It shows when he was awake and when he slept.

To illustrate just how much detail from someone's life can be mined from this stored data, ZEIT ONLINE has "augmented" Spitz's information with records that anyone can access: the politician's tweets and blog entries were added to the information on his movements. It is the kind of process that any good investigator would likely use to profile a person under observation. ZEIT ONLINE decided to keep one part of Spitz's data record private, namely, whom he called and who called him. That kind of information would not only infringe on the privacy of many other people in his life, it would also, even if the numbers were encrypted, reveal much too much about Spitz (but government agents in the real world would have access to this information).

We were very happy to work with Lorenz Matzat and Michael Kreil from Open Data City to find a solution how to understand and extract the geolocation from the dataset. Every connection of Spitz' mobile phone has to be triangulated to the positions of the antenna pole. Every pole has three antennas, each covering 120¬¨‚àû. The two programmers found out, that the saved position indicated the direction from the pole Spitz' mobile phone was connecting.

Matching this with the positions of the poles-map of the http://emf2.bundesnetzagentur.de/karte.html[state-controlled agency] agency gave us the possibility to get his position for each of the 260,640 minutes during the 181 days and put it via API on a Google Map. Together with the in-house graphics and design team we created a great interface to navigate: By pushing the play button, you will set off on a trip through Malte Spitz's life.

After a very successful launch of the Project in Germany, we recognized a great traffic from outside Germany and decided to translate the app in an English version. After earning the German Grimme Online Award, the project was honoured with an ONA award in September 2011, the first time for a German news website.

See the data
Read the story

=== Citizen data reporters by Amanda Rossi (Friends of Januária) ===

////

Insert: picture from project

////

Coordinator of the http://amigosdejanuaria.wordpress.com/[Friends of Januária] citizen media project, Brasil

Large newsrooms are not the only ones that can work on data-powered stories. Even those that may be considered as citizen reporters have the opportunity to use data in their stories. The same skills that are useful for data journalists can also help ordinary citizens access data about their local reality, and turn them into stories.

This was the primary assumption that led to the citizen media outreach project Friends of Januária, which began in September of 2011. The project received a grant from http://rising.globalvoicesonline.org/[Rising Voices], the outreach arm of http://globalvoicesonline.org/[Global Voices Online], and received additional support from the organization Article 19. Between September and October 2011, a group of young residents of a small town in Brazil were trained in basic journalism techniques and budget monitoring. They also learned how to make Freedom of Information requests and access publicly available information from official databases on the Internet.

The pilot project took place in Januária, a small town of approximately 65,000 residents located in north of the state of Minas Gerais, which is one of the poorest regions of Brazil. The town is also renowned for the failure of its local politicians. In just six years (2004-2010), Januária had seven different mayors. Almost all of them were removed from office due to wrongdoing in their public administrations, including charges of corruption.

Small towns like Januária often fail to attract attention from the Brazilian media, which tends to focus on larger cities and state capitals. However, there is an opportunity for residents of small towns to become a potential ally in the monitoring of the public administration because they know the daily challenges facing their local communities better than anyone. With the Internet as another important ally, residents can now better access information such as budget information and other data about their towns.

After taking part in twelve workshops, some of the new citizen reporters from Januária began to demonstrate how this concept of accessing publicly available data in small towns can be put into practice. For example, Soraia Amorim, a 22 year-old citizen journalist, wrote a story about the number of doctors that are on the city payroll according to Federal Government data. However, she found that the official number did not match with the town's reality. To write this story, Soraia had access to health data about her town, which is available online at the http://tabnet.datasus.gov.br/tabdata/cadernos/cadernosmap.htm[website of the Sistema Único de Saúde (SUS - Unique Health System)], a federal program that provides free medical assistance to the Brazilian population. According to SUS data, Januária should have 71 doctors in various health specialities. See the table below:

The number of doctors indicated by SUS data did not match what she knew about the local reality. Taking into account that the town's residents were always complaining about the lack of doctors and that even some patients had to travel to neighboring towns to see a doctor. Later, Soraia interviewed a woman that had recently been in a motorcycle accident and could not find medical assistance at Januária's hospital because no doctor was available. She also talked to the town's Health Secretary, who admitted that there were less doctors in town than the number published by SUS.

These initial findings raise many questions about reasons for this difference between the official information published online and the town's reality. One of them is that the federal data may be wrong, which would mean that there is an important lack of health information in Brazil. Another possibility may be that the town is incorrectly reporting the information to SUS. Both of these possibilities should lead to a deeper investigation to find the definitive answer. However, Soraia's story is an important part of this chain because it highlights an inconsistency and may also encourage others to look more closely about this issue. "Access to data is a tool to provide information so that citizens can do something and fight for their rights," says Soraia.

This was an important first step, and only after a few short trainings, Soraia was better equipped to search for data, make a Freedom of Information request, and write a story. "I used to live in the countryside, and finished high school with a lot of difficulty. When people asked me what I wanted to do with my life, I always told them that I wanted to be a journalist. But I imagined that it was almost impossible due to the world I lived in." After taking part in the Friends of Januária training. Soraia believes that access to information and data is an important tool to change the reality of her town. "I feel capable of changing my town, my country, the world. And I will pursue the access to information in my daily life," adds Soraia. See below for an excerpt from her story:

"According to the Health Handbook of SUS, Januária has approximately one doctor per one thousand residents (71 doctors). (...) Despite this number, many people are not assisted. That is what happened with Geiciane (who had a motorcycle accident and could not be assisted at the town's hospital). (...) The town's Health Secretary, André Rocha, does not confirm the amount of doctors mentioned by SUS".

Another citizen journalist from the project is Alysson Montiériton, 20 years-old, who also used data for his final article. It was during the project's first class, when the citizen reporters walked around the city to look for subjects that could become stories, that Alysson decided to write about a broken traffic light located in a very important intersection, which had remained broken since the beginning of the year. After learning how to look for data on the Internet, Alysson searched for the number of vehicles that exists in town and the amount of taxes paid by those who own vehicles. See below an extract of his report:

"The situation in Januária gets worse because of the high number of vehicles in town. According to IBGE (the most important statistics research institute in Brazil), Januária had 13,771 vehicles (among which 7,979 were motorcycles) in 2010. (...) The town's residents believe that the delay in fixing the traffic light is not a result of lack of resources. According to the Treasury Secretary of Minas Gerais state, the town received 470 thousand reais in vehicle taxes in 2010".

By having access to data, Alysson was able to show that Januária has many vehicles (almost one for every five residents) and that a broken traffic light could put a lot of people in danger. Besides, he was able to tell his audience the amount of funds received by the town from taxes paid by vehicle owners and, based on that, to question whether this money would not be enough to repair the traffic light to provide safe conditions to drivers and pedestrians. See the graph below:

"At the http://www.ibge.gov.br/home/[IBGE site] site , I clicked on "Cities", looked for Januária, and saw all the information that is available about the town. Then, I selected the data on the number of vehicles that exists in Januária. Besides, I looked for the amount of money that Januária receives from IPVA (vehicle tax). The town receives a good amount of funds and should provide a better maintenance for equipment, like traffic lights and so many others public equipment that citizens demand on a daily basis", explains Alysson.

Although these two reports, written by Soraia and Alysson, are very simple, they show that data can be used by citizen reporters. You don't need to be in a large newsroom, with the need for specialists, to use data in your reports. After twelve workshops, Soraia and Alysson, with no journalism background, were able to work on data powered stories and write interesting pieces about their local reality. In addition, their articles show that data itself can be useful even on a small scale. In other words, that there is also valuable information in small datasets and tables - not only in huge spreadsheets.

"I was able to learn how to look for data about my town on the Internet. I invite my readers to look for data about your own town on the Internet. I searched about my town and wrote a piece about it", says Alysson. If you like his example, accept his challenging invitation.

=== Which car model? MOT failure rates by Martin Rosenbaum (BBC) ===

////

Insert: picture from project

////

In January 2010 the BBC obtained data about the MOT pass and fail rates for different makes and models of cars. This is the test which assesses whether a car is safe and roadworthy - any car over three years old has to have an MOT test annually.

We obtained the data under freedom of information following an 18-month battle with VOSA, the Department for Transport agency which oversees the MOT system. VOSA turned down our FOI request for these figures on the grounds that it would breach commercial confidentiality. It argued that it could be 'commercially damaging' to vehicle manufacturers with high failure rates. However, we then appealed to the Information Commissioner, who ruled that disclosure of the information would be in the public interest. VOSA then released the data, 18 months after we asked for it.

We analysed the figures, focusing on the most popular models and comparing cars of the same age. This showed wide discrepancies. For example, among three year old cars, 28% of Renault Méganes failed their MOT in contrast to only 11% of Toyota Corollas. The figures were reported on television, radio and online.

The data was given to us in the form of a 1,200 page PDF document, which we then had to convert into a spreadsheet to do the analysis. As well as reporting our conclusions, we published this Excel spreadsheet (with over 14,000 lines of data) on the BBC News website along with our story. This gave everyone else access to the data in a usable form.

The result was that others then used this data for their own analyses which we did not have time to do in the rush to get the story out quickly or in some cases would have stretched our technical capabilities at the time. This included examining the failure rates for cars of other ages, comparing the records of manufacturers rather than individual models, and creating searchable databases for looking up the results of individuals models. We added links to these sites to our online news story, so our readers could get the benefit of this work.

This illustrated some advantages of releasing the raw data to accompany a data-driven story. There may be exceptions (for example, if you are planning to use the data for other follow-up stories later and want to keep it to yourself meanwhile), but on the whole publishing the data has several important benefits.

1. Your job is to find things out and tell people about them. If you've gone to the trouble of obtaining all the data, it's part of your job to pass it on.

2. Other people may spot points of significant interest which you've missed, or simply details that matter to them even if they weren't important enough to feature in your story.

3. Others can build on your work with further, more detailed analysis of the data, or different techniques for presenting or visualising the figures, using their own ideas or technical skills which may probe the data productively in alternative ways.

4. It's part of incorporating accountability and transparency into the journalistic process. Others can understand your methods and check your working if they want to.

=== Finnish Parliamentary elections and campaign funding by Esa M‚àö¬ßkinen (Helsingin Sanomat) ===

////

Insert: picture from project

////

In the spring of 2012, there are many ongoing trials about the campaign election funding of Finnish general elections of 2007.

After the elections in 2007, the press found out that the laws on publicising campaign funding had no effect on politicians. Basically, campaign funding has been used to buy favours from politicians and they had not declared their funding as mandated by Finnish law.

After these incidents, the laws became stricter. After the general election in March 2011, Helsingin Sanomat decided that we should dig very carefully all the available data on campaign funding. The new law stipulates, that election funding must be declared and only donations below 1500 euros may be anonymous.

First step was to get the data and find interested developers.

Helsingin Sanomat has organized HS Open hackathons since march 2011. We invite Finnish coders, journalists and graphic designers to the basement of Sanoma building. Participants are divided into groups of three, and they are encouraged to develop applications and visualizations. We have had about 60 participants in each of our three events so far.

We decided that campaign funding data should be the focus of HS Open #2, May 2011.

The National Audit Office of Finland is the authority that keeps records of campaign funding. That was the easy part. Chief information Officer, Jaakko Hamunen, built a website that provides real time access to their campaign funding database. The Audit Office made this in just two months after our request.

http://www.vaalirahoitus.fi/[Vaalirahoitus.fi] site will provide the press and public information on campaign funding on every elections from now on.

Second step was to get ideas: what should we do with the data?

The participants of HS Open 2 came up with twenty different prototypes about what to do with the data. You can find all the prototypes http://blogit.hs.fi/hsnext/hs-open-2-tuotti-parikymmenta-prototyyppia[here] (text in Finnish).

Researcher of bioinformatics, Janne Peltola, noted that campaign funding data looked like gene data they research in terms of containing many interdependencies. Moreover, they have an open source tool to map out these interdependencies. It's called http://www.cytoscape.org/[Cytoscape]. So we ran the data through Cytoscape, and had a very interesting prototype.

Third step was to implement the idea on paper and on our site, HS.fi.

The law on campaign funding states, that elected members of parliament must declare their funding two months after the elections. In practise, this meant that we got the real data in mid-June. In HS Open, we had data only from the MPs who had filed their report before the deadline.

There was also a problem with the data format. National Audit Office provided the data as two CSV files. First contained the total budget of campaigns, the other listed all the donors. We had to combine these two; we made a file that contained three columns: donor, receiver and amount. If the politicians had used their own money, in our data format looked like Politician A donated x euros to Politician A. Counter-intuitive, but it worked for Cytoscape.

When the data was cleaned and reformatted, we just ran it through Cytoscape. Then our graphics department made a page of it.

Second part was to make a visualization for our web page. You can find the result http://www.vaaliraha.com/[here].

The idea of this was not network analysis. We wanted to show people in easy format how much campaign funding there is and who gives it. The first view shows the distribution of funding between MPs. When you click on one MP, you get the breakdown on his or her funding. You can also vote on whether this particular donor is good or not. The visualization was made by Juha Rouvinen and Jukka Kokko, from ad agency Satumaa.

The web version of campaign funding visualization uses the same data as the network analysis.

Fourth step was to publish the data.

Of course, the National Audit Office already publishes the data, so there was no need to republish. But, as we had a better data structure, we decided to publish it. We give out our data with http://creativecommons.org/choose/[Creative Commons licence]. And there have been several visualizations made by independent developers that have used our data subsequently. We have also published some of these.

The tools we used for the project were: Excel and Google Refine for data cleaning and analysis. Cytoscape for network analysis. Illustrator and Flash for the visualizations. The Flash should have been HTML5, but we ran out of time.

What did we learn? Perhaps the most important thing was, that data structures can be very hard. If the original data is not in suitable format, recalculating and converting it will take a lot of time. 

=== Illinois school report cards by Brian Boyer (Chicago Tribune) ===

http://schools.chicagotribune.com/
2011 Illinois school report cards

Each year, the Illinois State Board of Education releases school "report cards" -- data on the demographics and performance of all the public schools Illinois. It's a massive dataset, this year's drop was ~9,500 *columns* wide.

The problem with that much data is choosing what to present. (As with any software project, the hard part is not *building* the software, but building the *right* software.)

We worked with the reporters and editor from the education team to choose the interesting data. (There's a lot of data out there that seems interesting but which a reporter will tell you is actually flawed or misleading.)

We also surveyed and interviewed folks with school-age kids in our newsroom. We did this because of an empathy gap -- nobody on the news apps team has school-age kids. Along the way, we learned much about our users and much about the usability (or lack thereof!) of the previous version of our schools site.

We aimed to design for a couple specific users and use cases: 1) parents with a child in school who want to know how their school measures up, and 2) parents who're trying to sort out where to live, since school quality often has a major impact on that decision.

The first time around, the schools site was about a six week, two developer project. Our 2011 update was a four-week, two developer project. (There were actually three people actively working on the recent project, but none were full-time, so it adds up to about two.)

A key piece of this project was information design. Although we present far less data than is available, it's still a *lot* of data, and making it digestible was a challenge. Luckily, we got to borrow someone from our graphics desk -- a designer who specializes in presenting complicated information. He taught us much about chart design and, in general, guided us to a presentation that is readable, but does not underestimate the reader's ability or desire to understand the numbers.

The site was built in Python and Django. The data is housed in MongoDB -- the schools data is heterogeneous and hierarchical, making it a poor fit for a relational database. (Otherwise we probably would have used PostgreSQL.)

We experimented for the first time with Twitter's Bootstrap user interface framework on this project, and were happy with the results. The charts are drawn with Flot.

The app is also home to the many stories about school performance that we've written. It acts as sort of a portal in that way -- when there's a new school performance story, we put it at the top of the app, alongside lists of schools relevant to the story. (And when a new story hits, readers of chicagotribune.com are directed to the app, not the story.)

Early reports are that readers love the schools app. The feedback we've received has been largely positive (or at least constructive!), and page views are through the roof. As a bonus, this data will remain interesting for a full year, so although we expect the hits to tail off as the schools stories fade from the homepage, our past experience is that readers have sought out this application year-round.

A few key ideas we took away from this project are:

1. The graphics desk is your friend. They're good at making complex information digestible.
2. Ask the newsroom for help. This is the second project for which we've conducted a newsroom-wide survey and interviews, and it's a great way to get the opinion of thoughtful people who, like our audience, are diverse in background and generally uncomfortable with computers. :)
3. Show your work! Much of our feedback has been requests for the data that the application. We've made a lot of the data publicly available via an API, and we will shortly release the stuff that we didn't think to include initially.

===  Mapa76 Hackathon (Mariano Blejman, Hacks/Hackers) ===

////

Insert picture from Mapa76 hackathon

////

We opened the chapter of Hacks/Hackers Buenos Aires in April 2011. We hosted two initial meetups to publicize the idea of greater collaboration between journalists and software developers, with between 120 and 150 people at each event. For a third meeting we had a 30-hour hackathon with eight people at a digital journalism conference in the city of Rosario, 300 kilometers from Buenos Aires.

A recurring theme in these meetings was the desire to scrape large volumes of data from the web, and then to represent it visually. To help with this a project called Mapa76.info was born, which helps users to extract data, and then to display it using maps and timelines. Not an easy task.

Why Mapa76? On March 24, 1976 there was a coup in Argentina, which lasted until 1983. In that period there were an estimated 30,000 disappeared people, thousands of deaths, and 500 children born in captivity appropriated for the military dictatorship. Over 30 years later, in Argentina the number of persons convicted of crimes against humanity committed during the dictatorship, amounts to 262 people (September 2011). Currently there are 14 ongoing trials and 7 with definite date for its inception. As is known publicly that there are 802 peoples processed in various court cases that are open. Of these, 412 have one or more causes elevated to trial, while another 102 have the upliftment order to trial by the prosecution.

Such prosecutions generate large volumes of information that make hard the work of researchers, journalists, human rights organizations, judges, prosecutors, etc. - Is complex when collecting and analyzing data that can help to clarify facts. Much more information is produced in a dispersed manner, which researchers can process. Basically the problem is that investigators doesn't take advantage of IT tools applied to solving such problems. In most cases, both from the journalistic perspective and from the judiciary, the survey data was done manually on the basis of records already generated (some digital, some not). These practices represent the universe of analysis, to overlook certain facts and ultimately significantly inhibit exploration of the facts, and the construction of conjectures and / or global findings by researchers newspaper. Mapa76 should be considered as a tool of investigative journalism, historical and judicial open access through the Web. But also include private layers.

To reach the hackathon, we worked before in conducting a platform for programmers and journalists could work the day of the meeting. Martin Sarsale developed the basics algorithms to extract information from simple text documents. Some libraries were also used from DocumentCloud.org project, but not many. We came with a basic platform, where the user could work with a document on the site, this document was automatically analyzed and mined there names, dates and places automatically. Once extracted this information, "operator" take the case to tell his life story dates manually linking with "actions" (birth, place of arrest, the alleged place of disappearance, etc.)

The goal is to provide a platform for automatic extraction of data on the judgments of the military dictatorship in Argentina. The aim is to achieve automatic (or semiautomatic) display of life histories of the period 1976-1983 based on written evidence, arguments and judgments about dictatorship age. This platform aims to provide tools to analyze a document database (a set of documents) and help identify finding relationships difficult in manual way. The extracted data (names, places and dates) are collected, stored and can be refined by the researcher, in addition to viewing on a geolocated, timelines and maps of relationships between people. We found existing projects related to the investigation and we had several meetings with investigators, some of them post-hackathon.

* Federal Network Places of Memory

This systematization meets the highest amount of information about State terrorism nationally and can be easily accessed through a search engine navigable online, allowing multiple combinations of data and have photographs and mapping applications to performfixed and interactive maps based on Geographical Information Systems (GIS). The project also contemplates the use of the tool, constant updating and cooperative construction of information among all users of the system, after collation and validation by the National Archives of Memory. The aim is to establish itself as a substantial contribution to the investigations of crimes against humanity, especially those related to judicial proceedings.

* Provincial Archives of Memory in Cordoba
It is primarily a task for *file*, digitize and preserve documents. They currently have over 100,000 pages scanned from testimony given in the different cases of crimes against humanity that took place at the Juzgado Federal No. 2, police memos, rulings, judgments and so on. Systematized in collections using library software Greenstone Digital Library (http://www.greenstone.org/).

* National Prosecuting Authority - Pablo Parenti
Coordination and Monitoring Unit of the Causes of Human Rights of the Attorney General's Office, Research Tools (Excalibur)

* Asociación Nunca Más + Gabriel Acquistapace (Drupal)

* No time contacted the Forensic Anthropology Team and Memoria Abierta, or the work of Inés Caridi, Faculty of Sciences of the UBA, who made a mathematical model for Forensic Anthropology. Each of these projects have different types of quality, quantity and availability of public or private documents, but more or less similar purposes.

What could serve mapa76?
Both journalists and investigators, prosecutors, witnesses, can use it to establish relationships between disappeared, places, dates and conditions of detention.
It will allow to follow the story of a person's life and how was his course during his captivity and subsequent disappearance or release. In the case of not having information about a person, journalist, lawyer or prosecutor, can access the platform Mapa76 and "comb" a vast amount of documents for new information. This could allow identification of new witnesses, inquiring about test criteria, reaffirming that there was a systematic plan for the appropriation of children by armed forces, etc.

How does Mapa76 work?
Mapa76 is now a developing prototype.
It has three basic modules:
(A) the automatic extraction of data-names, dates and places;
(B) consultation and treatment data in context, and
(C) the display of the selected data on maps and timelines.
Using defined search patterns (which may be: personal names, organizations, places and dates), this software1- programmed in Ruby recognizes the words in the document that match the preset and stored in a database.

This base can be refined by the researcher and each finding can be viewed in context. Then in a second step, the data extracted / purified / selected can be viewed on a map or geo referenced in a timeline, and they in turn provide contextual information regarding the consultation.

To develop this platform we will work with layers in order to filter specific information regarding allegations, convictions, political-military group, a clandestine detention center, names, etc.

One way to understand the behavior patterns of Mapa76.info would be: the user uploads a document. This is analyzed by the software of automatic data extraction. Then the user chooses a person and paragraphs in which choose the name mentioned, the relevant dates and places related to the time.

Call for hackathon
We make a public announcement through the page Hacks/Hackers Buenos Aires http://meetupba.hackshackers.com At that time we had about 200 enrolled, there are currently about 540. We also called Human Rights associations and made a formal request for the hackathon in Tecnopolis, a vast exhibition organized by the national government dedicated to science in Argentina, which occurred place within the former Batallon 601, the base of military operations in the dictatorship government. The meeting was attended by about forty people including journalists, militant organizations, developers and designers. Other participants of hackathon were Junar makers of a platform for "streaming" data to organize the information automatically and export it to other websites. We received sponsorship from Mozilla Argentina and Tecnópolis.

During the hackathon
We searched for isolated tasks, so that participants could move independently

Programmers, Designers: Make an interface that combines timeline + map /
Designers
Incorporate the platform components: filter by organizations, agencies, for testimonials, etc..

Developers:
Try other ways to extract data
  *  names, addresses, dates
  *  Organizations (currently done with regexes + a Bayesian filter for names).
  *  misspelled names, etc.
  *  alias
  *  disambiguations
  *  different ways of referring to the same person
  *  "Jorge Julio Lopez"
  *  "Julio Lopez"
  *  "Lopez"
  *  Work on the API data mapa76 exposing

Journalists:
Find use cases:
  *  Who was who?
  *  Follow the story of a person. What happened?
  *  Compare two life stories
  *  Compare the story depending on version
  *  Combing documents to try to tell a story based on documents
  *  Incorporate other sources such as newspapers, databases, etc..

Identify key words:
  *  Kidnapping
  *  Transfer
  *  Survivor
  *  Captivity
  *  Identity Theft

Sort documents and relationships between documents:
  *  Testimony
  *  Expertise
  *  Case
  *  Judgment
  *  Fundamentals

Future
Improve the charging interface
  *  Smarter, faster
  *  Make a prototype query interface
  *  timelines (visualizations)
- Per person
- For detention center
- Per organization
  *  Consultation
- Who was with who, where, etc..
  *  Layers own info / private
Newly transcribed testimony

Later hackathon problems

Probably the main problem we had after the hackathon was that short-term objectives set were very demanding, very ambitious expectations, and the spread of ideas and the model of volunteer work hard to manage. All people involved and participating in the project are qualified with day jobs of high complexity, and could not move forward with the necessary regularity and systemization. In part, because the more involved people also participated in other activities planned by Hacks/Hackers Buenos Aires in 2011, the year we made nine events: four hackatones, four meetups, a web seminar and a lecture on a Free Software conference.

What are we doing?

  *  Generate technical documentation of the prototype.
  *  Define the functional scope of version 1.0, modular development plan, quantify the cost and size the equipment.
  *  Develop a Project Plan for version 1.0 and do a benchmarking for development funds.
  *  To survey the existence of functional modules to integrate into the platform Mapa76. For example, visualization of relationships between entities (people) to managing versions of a work product of the consultation on the system, etc.
  *  Improve data loading interface to make it more efficient and faster in the searches.
  *  Polish the query interface to improve the timelines per person per centerdetention.
  *  Set up a database query to establish relationships between people. By example, who was with who, where the disappearance occurred, how was your hostage situation, when it happened, and so on.
  *  Generate data layers of information for public and private consultation, to allow
store them "stories" at the user level, etc.

Current Status

At present, there is a group of four people directly involved in the project trying to get to prototype mapa76.info: Martin Sarsale (developer), Mark Vanetta (programmer), Andrew Snitcovsky (graphic designer), Mariana Berruezo (product concept) and Mariano Blejman (journalist). And may be 15 "satelite" collaborators.

* Mapa76.info is under development through mapa76-dev@googlegroups.com (Ruby, JQuery, MySQL) For developers who want to join the group, contact "Martin Sarsale" <martin.sarsale@gmail.com>
* Code https://github.com/mapa76/
* Project manager https://www.pivotaltracker.com/projects/344053
* In early March 2012 we will have a hackathon for visualizations timelines.

Who we are
Mapa76.info is an initiative of the Buenos Aires chapter of Hacks / Hackers, an area of
meeting comprised of journalists, software programmers and designers who come together to help build the future of media. The organizers of Hacks / Hackers Buenos Aires is made up Mariano Blejman (Página/12), Martin Sarsale (Sumavisos), Guillermo Movia (Mozilla Argentina), Cesar Miquel (EasyTech), Mariana Berruezo, Sorin Sergio and Ezequiel Clerici.


=== Electoral Hack in real-time by Hacks/Hackers Buenos Aires ===

With input from Andy Tow, Sergio Sorin, Mariana Berruezo, Martin Sarsale and Mariano Blejman

Electoral Hack http://elecciones.hhba.info is a Web environment of political analysis that illustrates real-time data from the provisional ballot results of the elections of October 23, 2011, in Argentina and provides several levels of analysis. The system also features a selection of information from previous elections and socio-demographic statistics from across the country. Electoral Hack was an initiative of Hacks/Hackers Buenos Aires with the political analyst Andy Tow, and was the result of joint volunteer work by members of the local chapter of Hacks/Hackers (journalists, programmers, designers, analysts, political scientists, etc.). Thematic maps of the primary elections of 2011and the general elections of 2007, as well as sociodemographic variables, are also available. Information was updated in real time with information from the provisional ballot count of the national elections of 2011 in Argentina and gave summaries of the presidential and vice-presidential elections as well as the renewal of Congress.

What data did we use?
All information posted came from official sources. The National Electoral Bureau provided access to data of the provisional count by Indra; the Department of the Interior provided information about elected posts and candidates from different political parties; the university project yoquierosaber.org provided biographical information and the policy platforms of each presidential ticket; while socio-demographic information came from the 2001 National Census of Population and Housing (INDEC), except for the total population which corresponded to the 2010 Census (INDEC), and from the Ministry of Health of the Nation.

How was it developed?
The application was generated during the 2011 Election Hackathon by Hacks/Hackers Buenos Aires the day before the election on October 23, 2011.  The hackathon was a day of work and programming with collaborations from 30 volunteer specialists. Electoral Hack was developed as an open platform for anyone interested in introducing improvements over time.  As technological tools we utilized developments that integrated Google Fusion Tables, maps, and vector graphics libraries, etc.


We worked on the construction of polygons for displaying geographic mapping and electoral demographics. Combining polygons in GIS software and geometries from public tables in Google Fusion Tables we generated tables with keys corresponding to the electoral database of the Ministry of Interior‚ÄîIndra and sociodemographic data from INDEC, creating from this data visualizations in Google Maps.

Using the GoogleMaps API we put several thematic maps online representing the spatial distribution of voting with different tones of color, where the gradation of intensity expressed different intervals of the percentage of votes for the various presidential tickets in different administrative departments and polling stations, with particular emphasis on major urban centers: the City of Buenos Aires, the 24 districts of Greater Buenos Aires, the City of Cordoba, and Rosario.

We used the same technique to generate thematic maps of previous elections, namely the presidential primaries of 2011 and the election of 2007, as well as of the distribution of sociodemographic data, such as for poverty, child mortality, and living conditions, allowing for analysis and comparison. We also put a display online showing the spatial distribution of the differences in percentage of votes obtained by each ticket in the general election of October compared to the August primary election.

Later, using partial data from the provisional ballot counts, we created an animated map depicting the anatomy of the count, in which the progress of the vote count can be seen in accelerated time from the closing of the local polls until the following morning.

Pros:

* We set out to find data and articulate it and we were able to do that.  At hand we had the Unicef's database of child sociodemographics, http://infoargentina.unicef.org.ar/, as well as the database of candidates created by the group yoquierosaber.org of Torcuato Di Tella University.  During the hackathon we gathered a large volume of additional data that we did not end up including.

* It was clear that the journalistic and programming work was enriched by scholarship. Without the contribution of Andy Tow and Hilario Moreno Campos, the project would have been impossible to pull off.

Cons:

* We ran into several problems: that the sociodemographic data we could use (indices of basic needs unmet, population, housing, etc.) were not up-to-date (most were from the 2001 census), and that their systematization was not publicly available or did not go to the local level (local average GDP, main economic activity, education level, number of schools, doctors per capita, and a long list of possibilities that would been great to have).

* On the other hand, the system was intended as a tool, one that others could use to recombine and display whatever data they wanted, so that the press could insert into their sites the information that interested them.  But it remained for future development.

* As participation was voluntary and collaborative, and as we had such a short time to plan, there was the practical impossibility of finishing what he had imagined as the final development. But we showed we were on the right track and we were able to develop a massive project.

* For the same reason, all the collaborative work of 30 people ended up condensed into a single programmer when the data offered by the government began to appear, and we ran into some problems regarding the importation of date in real time that were solved within hours.

Implications
Electoral Hack's platform had a big impact at the media, with television coverage, radio, graphics and online sites; its maps were used by several media platforms during the elections and in subsequent days. As the days went by, the maps and visualizations were updated, increasing traffic even more. On Election Day, the site created that very day received about 20 thousand unique visitors and its maps were reproduced by the newspaper P√°gina/12 on its cover page for two consecutive days as well as in articles in La Nacion; some of its maps appeared in the print edition of the newspaper Clar√≠n. It was the first time that an interactive display of real-time maps had been used in the history of Argentine journalism. In the central maps one could clearly see the overwhelming victory of Cristina Fernandez de Kirchner by 54 percent of the vote, broken up by color saturation. It also served to help understand very specific individual cases where local candidates swept the provincial votes.

What is Hacks/Hackers Buenos Aires?
In April 2011, a local chapter of Hacks / Hackers opened in Buenos Aires, a meeting space comprising journalists, software developers, and IT professionals coming together to help build the future of media. The organizers of Hacks / Hackers Buenos Aires are Mariano Blejman (P√°gina/12), Martin Sarsale (Sumavisos), Guillermo Movia (Mozilla Argentina), Cesar Miquel (EasyTech), Mariana Berruezo, Sorin Sergio Clerici, and Ezekiel. In 2011 there were four meetups, four hackathons, a conference, and a webinar. We currently have a base of 540 people, including journalists, programmers, designers, and entrepreneurs.

For this project, the Buenos Aires Chapter of Hacks/Hackers joined with the blogger and political analyst Andy Tow Hilario and political scientist Moreno-Field - of the PR consulting firm Dicen - to develop  with journalists, social scientists, programmers and graphic designers an innovative information system. Those who worked on the  idea of Sergio Sorin's included: Andy Tow - Andres Snitcofsky - Catalina Marino - Cesar Miquel - Damian Janowski - Ezequiel Clerici - Federico Ricciardi - Fernanda Roux - Florence Coelho - Guido Labonia - Guillermo Movia - Hilario Moreno Campos - Juan Pablo Renzi - Kevin Hanna - Lucas Aznar - Luis E. Guardiola - Mariana Berruezo - Mariano Mancuso - Martin Sarsale - Momi Peralta Ramos - Nicholas Cisco - Oscar Guindzberg - Sebastian Alvarez - Sebastian Melendez - Soledad Gherardi - Tania Wassaf - Valeria Bula and Mariano Blejman.

Twitter @ HacksHackersBA
Mail ba@hackshackers.com
Web http://meetupba.hackshackers.com
Blog http://www.hackshackers.com
