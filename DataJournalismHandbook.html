<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
    "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="application/xhtml+xml; charset=UTF-8" />
<meta name="generator" content="AsciiDoc 8.6.6" />
<title>The Data Journalism Handbook</title>
<style type="text/css">
/* Shared CSS for AsciiDoc xhtml11 and html5 backends */

/* Default font. */
body {
  font-family: Georgia,serif;
}

/* Title font. */
h1, h2, h3, h4, h5, h6,
div.title, caption.title,
thead, p.table.header,
#toctitle,
#author, #revnumber, #revdate, #revremark,
#footer {
  font-family: Arial,Helvetica,sans-serif;
}

body {
  margin: 1em 5% 1em 5%;
}

a {
  color: blue;
  text-decoration: underline;
}
a:visited {
  color: fuchsia;
}

em {
  font-style: italic;
  color: navy;
}

strong {
  font-weight: bold;
  color: #083194;
}

h1, h2, h3, h4, h5, h6 {
  color: #527bbd;
  margin-top: 1.2em;
  margin-bottom: 0.5em;
  line-height: 1.3;
}

h1, h2, h3 {
  border-bottom: 2px solid silver;
}
h2 {
  padding-top: 0.5em;
}
h3 {
  float: left;
}
h3 + * {
  clear: left;
}
h5 {
  font-size: 1.0em;
}

div.sectionbody {
  margin-left: 0;
}

hr {
  border: 1px solid silver;
}

p {
  margin-top: 0.5em;
  margin-bottom: 0.5em;
}

ul, ol, li > p {
  margin-top: 0;
}
ul > li     { color: #aaa; }
ul > li > * { color: black; }

pre {
  padding: 0;
  margin: 0;
}

#author {
  color: #527bbd;
  font-weight: bold;
  font-size: 1.1em;
}
#email {
}
#revnumber, #revdate, #revremark {
}

#footer {
  font-size: small;
  border-top: 2px solid silver;
  padding-top: 0.5em;
  margin-top: 4.0em;
}
#footer-text {
  float: left;
  padding-bottom: 0.5em;
}
#footer-badges {
  float: right;
  padding-bottom: 0.5em;
}

#preamble {
  margin-top: 1.5em;
  margin-bottom: 1.5em;
}
div.imageblock, div.exampleblock, div.verseblock,
div.quoteblock, div.literalblock, div.listingblock, div.sidebarblock,
div.admonitionblock {
  margin-top: 1.0em;
  margin-bottom: 1.5em;
}
div.admonitionblock {
  margin-top: 2.0em;
  margin-bottom: 2.0em;
  margin-right: 10%;
  color: #606060;
}

div.content { /* Block element content. */
  padding: 0;
}

/* Block element titles. */
div.title, caption.title {
  color: #527bbd;
  font-weight: bold;
  text-align: left;
  margin-top: 1.0em;
  margin-bottom: 0.5em;
}
div.title + * {
  margin-top: 0;
}

td div.title:first-child {
  margin-top: 0.0em;
}
div.content div.title:first-child {
  margin-top: 0.0em;
}
div.content + div.title {
  margin-top: 0.0em;
}

div.sidebarblock > div.content {
  background: #ffffee;
  border: 1px solid #dddddd;
  border-left: 4px solid #f0f0f0;
  padding: 0.5em;
}

div.listingblock > div.content {
  border: 1px solid #dddddd;
  border-left: 5px solid #f0f0f0;
  background: #f8f8f8;
  padding: 0.5em;
}

div.quoteblock, div.verseblock {
  padding-left: 1.0em;
  margin-left: 1.0em;
  margin-right: 10%;
  border-left: 5px solid #f0f0f0;
  color: #888;
}

div.quoteblock > div.attribution {
  padding-top: 0.5em;
  text-align: right;
}

div.verseblock > pre.content {
  font-family: inherit;
  font-size: inherit;
}
div.verseblock > div.attribution {
  padding-top: 0.75em;
  text-align: left;
}
/* DEPRECATED: Pre version 8.2.7 verse style literal block. */
div.verseblock + div.attribution {
  text-align: left;
}

div.admonitionblock .icon {
  vertical-align: top;
  font-size: 1.1em;
  font-weight: bold;
  text-decoration: underline;
  color: #527bbd;
  padding-right: 0.5em;
}
div.admonitionblock td.content {
  padding-left: 0.5em;
  border-left: 3px solid #dddddd;
}

div.exampleblock > div.content {
  border-left: 3px solid #dddddd;
  padding-left: 0.5em;
}

div.imageblock div.content { padding-left: 0; }
span.image img { border-style: none; }
a.image:visited { color: white; }

dl {
  margin-top: 0.8em;
  margin-bottom: 0.8em;
}
dt {
  margin-top: 0.5em;
  margin-bottom: 0;
  font-style: normal;
  color: navy;
}
dd > *:first-child {
  margin-top: 0.1em;
}

ul, ol {
    list-style-position: outside;
}
ol.arabic {
  list-style-type: decimal;
}
ol.loweralpha {
  list-style-type: lower-alpha;
}
ol.upperalpha {
  list-style-type: upper-alpha;
}
ol.lowerroman {
  list-style-type: lower-roman;
}
ol.upperroman {
  list-style-type: upper-roman;
}

div.compact ul, div.compact ol,
div.compact p, div.compact p,
div.compact div, div.compact div {
  margin-top: 0.1em;
  margin-bottom: 0.1em;
}

tfoot {
  font-weight: bold;
}
td > div.verse {
  white-space: pre;
}

div.hdlist {
  margin-top: 0.8em;
  margin-bottom: 0.8em;
}
div.hdlist tr {
  padding-bottom: 15px;
}
dt.hdlist1.strong, td.hdlist1.strong {
  font-weight: bold;
}
td.hdlist1 {
  vertical-align: top;
  font-style: normal;
  padding-right: 0.8em;
  color: navy;
}
td.hdlist2 {
  vertical-align: top;
}
div.hdlist.compact tr {
  margin: 0;
  padding-bottom: 0;
}

.comment {
  background: yellow;
}

.footnote, .footnoteref {
  font-size: 0.8em;
}

span.footnote, span.footnoteref {
  vertical-align: super;
}

#footnotes {
  margin: 20px 0 20px 0;
  padding: 7px 0 0 0;
}

#footnotes div.footnote {
  margin: 0 0 5px 0;
}

#footnotes hr {
  border: none;
  border-top: 1px solid silver;
  height: 1px;
  text-align: left;
  margin-left: 0;
  width: 20%;
  min-width: 100px;
}

div.colist td {
  padding-right: 0.5em;
  padding-bottom: 0.3em;
  vertical-align: top;
}
div.colist td img {
  margin-top: 0.3em;
}

@media print {
  #footer-badges { display: none; }
}

#toc {
  margin-bottom: 2.5em;
}

#toctitle {
  color: #527bbd;
  font-size: 1.1em;
  font-weight: bold;
  margin-top: 1.0em;
  margin-bottom: 0.1em;
}

div.toclevel1, div.toclevel2, div.toclevel3, div.toclevel4 {
  margin-top: 0;
  margin-bottom: 0;
}
div.toclevel2 {
  margin-left: 2em;
  font-size: 0.9em;
}
div.toclevel3 {
  margin-left: 4em;
  font-size: 0.9em;
}
div.toclevel4 {
  margin-left: 6em;
  font-size: 0.9em;
}

span.aqua { color: aqua; }
span.black { color: black; }
span.blue { color: blue; }
span.fuchsia { color: fuchsia; }
span.gray { color: gray; }
span.green { color: green; }
span.lime { color: lime; }
span.maroon { color: maroon; }
span.navy { color: navy; }
span.olive { color: olive; }
span.purple { color: purple; }
span.red { color: red; }
span.silver { color: silver; }
span.teal { color: teal; }
span.white { color: white; }
span.yellow { color: yellow; }

span.aqua-background { background: aqua; }
span.black-background { background: black; }
span.blue-background { background: blue; }
span.fuchsia-background { background: fuchsia; }
span.gray-background { background: gray; }
span.green-background { background: green; }
span.lime-background { background: lime; }
span.maroon-background { background: maroon; }
span.navy-background { background: navy; }
span.olive-background { background: olive; }
span.purple-background { background: purple; }
span.red-background { background: red; }
span.silver-background { background: silver; }
span.teal-background { background: teal; }
span.white-background { background: white; }
span.yellow-background { background: yellow; }

span.big { font-size: 2em; }
span.small { font-size: 0.6em; }

span.underline { text-decoration: underline; }
span.overline { text-decoration: overline; }
span.line-through { text-decoration: line-through; }


/*
 * xhtml11 specific
 *
 * */

tt {
  font-family: monospace;
  font-size: inherit;
  color: navy;
}

div.tableblock {
  margin-top: 1.0em;
  margin-bottom: 1.5em;
}
div.tableblock > table {
  border: 3px solid #527bbd;
}
thead, p.table.header {
  font-weight: bold;
  color: #527bbd;
}
p.table {
  margin-top: 0;
}
/* Because the table frame attribute is overriden by CSS in most browsers. */
div.tableblock > table[frame="void"] {
  border-style: none;
}
div.tableblock > table[frame="hsides"] {
  border-left-style: none;
  border-right-style: none;
}
div.tableblock > table[frame="vsides"] {
  border-top-style: none;
  border-bottom-style: none;
}


/*
 * html5 specific
 *
 * */

.monospaced {
  font-family: monospace;
  font-size: inherit;
  color: navy;
}

table.tableblock {
  margin-top: 1.0em;
  margin-bottom: 1.5em;
}
thead, p.tableblock.header {
  font-weight: bold;
  color: #527bbd;
}
p.tableblock {
  margin-top: 0;
}
table.tableblock {
  border-width: 3px;
  border-spacing: 0px;
  border-style: solid;
  border-color: #527bbd;
  border-collapse: collapse;
}
th.tableblock, td.tableblock {
  border-width: 1px;
  padding: 4px;
  border-style: solid;
  border-color: #527bbd;
}

table.tableblock.frame-topbot {
  border-left-style: hidden;
  border-right-style: hidden;
}
table.tableblock.frame-sides {
  border-top-style: hidden;
  border-bottom-style: hidden;
}
table.tableblock.frame-none {
  border-style: hidden;
}

th.tableblock.halign-left, td.tableblock.halign-left {
  text-align: left;
}
th.tableblock.halign-center, td.tableblock.halign-center {
  text-align: center;
}
th.tableblock.halign-right, td.tableblock.halign-right {
  text-align: right;
}

th.tableblock.valign-top, td.tableblock.valign-top {
  vertical-align: top;
}
th.tableblock.valign-middle, td.tableblock.valign-middle {
  vertical-align: middle;
}
th.tableblock.valign-bottom, td.tableblock.valign-bottom {
  vertical-align: bottom;
}


/*
 * manpage specific
 *
 * */

body.manpage h1 {
  padding-top: 0.5em;
  padding-bottom: 0.5em;
  border-top: 2px solid silver;
  border-bottom: 2px solid silver;
}
body.manpage h2 {
  border-style: none;
}
body.manpage div.sectionbody {
  margin-left: 3em;
}

@media print {
  body.manpage div#toc { display: none; }
}
</style>
<script type="text/javascript">
/*<![CDATA[*/
var asciidoc = {  // Namespace.

/////////////////////////////////////////////////////////////////////
// Table Of Contents generator
/////////////////////////////////////////////////////////////////////

/* Author: Mihai Bazon, September 2002
 * http://students.infoiasi.ro/~mishoo
 *
 * Table Of Content generator
 * Version: 0.4
 *
 * Feel free to use this script under the terms of the GNU General Public
 * License, as long as you do not remove or alter this notice.
 */

 /* modified by Troy D. Hanson, September 2006. License: GPL */
 /* modified by Stuart Rackham, 2006, 2009. License: GPL */

// toclevels = 1..4.
toc: function (toclevels) {

  function getText(el) {
    var text = "";
    for (var i = el.firstChild; i != null; i = i.nextSibling) {
      if (i.nodeType == 3 /* Node.TEXT_NODE */) // IE doesn't speak constants.
        text += i.data;
      else if (i.firstChild != null)
        text += getText(i);
    }
    return text;
  }

  function TocEntry(el, text, toclevel) {
    this.element = el;
    this.text = text;
    this.toclevel = toclevel;
  }

  function tocEntries(el, toclevels) {
    var result = new Array;
    var re = new RegExp('[hH]([2-'+(toclevels+1)+'])');
    // Function that scans the DOM tree for header elements (the DOM2
    // nodeIterator API would be a better technique but not supported by all
    // browsers).
    var iterate = function (el) {
      for (var i = el.firstChild; i != null; i = i.nextSibling) {
        if (i.nodeType == 1 /* Node.ELEMENT_NODE */) {
          var mo = re.exec(i.tagName);
          if (mo && (i.getAttribute("class") || i.getAttribute("className")) != "float") {
            result[result.length] = new TocEntry(i, getText(i), mo[1]-1);
          }
          iterate(i);
        }
      }
    }
    iterate(el);
    return result;
  }

  var toc = document.getElementById("toc");
  if (!toc) {
    return;
  }

  // Delete existing TOC entries in case we're reloading the TOC.
  var tocEntriesToRemove = [];
  var i;
  for (i = 0; i < toc.childNodes.length; i++) {
    var entry = toc.childNodes[i];
    if (entry.nodeName == 'div'
     && entry.getAttribute("class")
     && entry.getAttribute("class").match(/^toclevel/))
      tocEntriesToRemove.push(entry);
  }
  for (i = 0; i < tocEntriesToRemove.length; i++) {
    toc.removeChild(tocEntriesToRemove[i]);
  }

  // Rebuild TOC entries.
  var entries = tocEntries(document.getElementById("content"), toclevels);
  for (var i = 0; i < entries.length; ++i) {
    var entry = entries[i];
    if (entry.element.id == "")
      entry.element.id = "_toc_" + i;
    var a = document.createElement("a");
    a.href = "#" + entry.element.id;
    a.appendChild(document.createTextNode(entry.text));
    var div = document.createElement("div");
    div.appendChild(a);
    div.className = "toclevel" + entry.toclevel;
    toc.appendChild(div);
  }
  if (entries.length == 0)
    toc.parentNode.removeChild(toc);
},


/////////////////////////////////////////////////////////////////////
// Footnotes generator
/////////////////////////////////////////////////////////////////////

/* Based on footnote generation code from:
 * http://www.brandspankingnew.net/archive/2005/07/format_footnote.html
 */

footnotes: function () {
  // Delete existing footnote entries in case we're reloading the footnodes.
  var i;
  var noteholder = document.getElementById("footnotes");
  if (!noteholder) {
    return;
  }
  var entriesToRemove = [];
  for (i = 0; i < noteholder.childNodes.length; i++) {
    var entry = noteholder.childNodes[i];
    if (entry.nodeName == 'div' && entry.getAttribute("class") == "footnote")
      entriesToRemove.push(entry);
  }
  for (i = 0; i < entriesToRemove.length; i++) {
    noteholder.removeChild(entriesToRemove[i]);
  }

  // Rebuild footnote entries.
  var cont = document.getElementById("content");
  var spans = cont.getElementsByTagName("span");
  var refs = {};
  var n = 0;
  for (i=0; i<spans.length; i++) {
    if (spans[i].className == "footnote") {
      n++;
      var note = spans[i].getAttribute("data-note");
      if (!note) {
        // Use [\s\S] in place of . so multi-line matches work.
        // Because JavaScript has no s (dotall) regex flag.
        note = spans[i].innerHTML.match(/\s*\[([\s\S]*)]\s*/)[1];
        spans[i].innerHTML =
          "[<a id='_footnoteref_" + n + "' href='#_footnote_" + n +
          "' title='View footnote' class='footnote'>" + n + "</a>]";
        spans[i].setAttribute("data-note", note);
      }
      noteholder.innerHTML +=
        "<div class='footnote' id='_footnote_" + n + "'>" +
        "<a href='#_footnoteref_" + n + "' title='Return to text'>" +
        n + "</a>. " + note + "</div>";
      var id =spans[i].getAttribute("id");
      if (id != null) refs["#"+id] = n;
    }
  }
  if (n == 0)
    noteholder.parentNode.removeChild(noteholder);
  else {
    // Process footnoterefs.
    for (i=0; i<spans.length; i++) {
      if (spans[i].className == "footnoteref") {
        var href = spans[i].getElementsByTagName("a")[0].getAttribute("href");
        href = href.match(/#.*/)[0];  // Because IE return full URL.
        n = refs[href];
        spans[i].innerHTML =
          "[<a href='#_footnote_" + n +
          "' title='View footnote' class='footnote'>" + n + "</a>]";
      }
    }
  }
},

install: function(toclevels) {
  var timerId;

  function reinstall() {
    asciidoc.footnotes();
    if (toclevels) {
      asciidoc.toc(toclevels);
    }
  }

  function reinstallAndRemoveTimer() {
    clearInterval(timerId);
    reinstall();
  }

  timerId = setInterval(reinstall, 500);
  if (document.addEventListener)
    document.addEventListener("DOMContentLoaded", reinstallAndRemoveTimer, false);
  else
    window.onload = reinstallAndRemoveTimer;
}

}
asciidoc.install(2);
/*]]>*/
</script>
</head>
<body class="article" style="max-width:55em">
<div id="header">
<h1>The Data Journalism Handbook</h1>
<div id="toc">
  <div id="toctitle">Table of Contents</div>
  <noscript><p><b>JavaScript must be enabled in your browser to display the table of contents.</b></p></noscript>
</div>
</div>
<div id="content">
<div class="sect1">
<h2 id="_credits">Credits</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_for_the_great_unnamed">For the great unnamed</h3>
<div class="paragraph"><p>The Data Journalism Handbook was born at a 48 hour workshop at MozFest 2011 in London. It subsequently spilled over into an international, collaborative effort involving dozens of data journalism&#8217;s leading advocates and best practitioners.</p></div>
<div class="paragraph"><p>In the 6 months that passed between the book&#8217;s inception to its first full release, hundreds of people have been contributed in various ways. While we have done our best to keep track of them all, we have had our fair share of anonymous, pseudonymous and untraceable edits.</p></div>
<div class="paragraph"><p>To all of those people who have contributed and are not listed below, we say two things. Firstly, thank you. Secondly, can please tell us who you are so that we can give credit where credit is due.</p></div>
</div>
<div class="sect2">
<h3 id="_contributor_list">Contributor list</h3>
<div class="ulist"><ul>
<li>
<p>
Gregor Aisch, Open Knowledge Foundation
</p>
</li>
<li>
<p>
Brigitte Alfter, Freelance Journalist
</p>
</li>
<li>
<p>
David Banisar, Article 19
</p>
</li>
<li>
<p>
Caelainn Barr, EU Data Journalist
</p>
</li>
<li>
<p>
Michael Blastland, BBC
</p>
</li>
<li>
<p>
Mariano Blejman, Hacks/Hackers
</p>
</li>
<li>
<p>
John Bones, Verdens Gang
</p>
</li>
<li>
<p>
Marianne Bouchart, Data Journalism Blog
</p>
</li>
<li>
<p>
Liliana Bounegru, European Journalism Centre
</p>
</li>
<li>
<p>
Brian Boyer, Chicago Tribune
</p>
</li>
<li>
<p>
Jane Park, Creative Commons
</p>
</li>
<li>
<p>
Paul Bradshaw, Birmingham City University, City University London
</p>
</li>
<li>
<p>
Wendy Carlisle, ABC News
</p>
</li>
<li>
<p>
Lucy Chambers, Open Knowledge Foundation
</p>
</li>
<li>
<p>
Helen Darbishire, Access Info Europe
</p>
</li>
<li>
<p>
Steve Doig, Cronkite School of Journalism
</p>
</li>
<li>
<p>
David Erwin, New York Times
</p>
</li>
<li>
<p>
Lisa Evans, Guardian Datablog
</p>
</li>
<li>
<p>
Tom Fries, Bertelsmann Stiftung
</p>
</li>
<li>
<p>
Duncan Geere, Wired.co.uk
</p>
</li>
<li>
<p>
Rich Gordon, Northwestern University
</p>
</li>
<li>
<p>
Jonathan Gray, Open Knowledge Foundation
</p>
</li>
<li>
<p>
Ted Han, DocumentCloud
</p>
</li>
<li>
<p>
Kate Hudson, Open Journalism
</p>
</li>
<li>
<p>
Mark Lee Hunter (Affiliation?)
</p>
</li>
<li>
<p>
Mark Irungu, Internews Network
</p>
</li>
<li>
<p>
Francis Irving, ScraperWiki
</p>
</li>
<li>
<p>
Lizzie Jackson, Ravensbourne College
</p>
</li>
<li>
<p>
Nicolas Kayser-Bril, J++
</p>
</li>
<li>
<p>
John Keefe, New York Public Radio
</p>
</li>
<li>
<p>
Friedrich Lindenberg, Open Knowledge Foundation
</p>
</li>
<li>
<p>
Mirko Lorenz, Deutsche Welle
</p>
</li>
<li>
<p>
Esa M√§kinen, Helsingin Sanomat
</p>
</li>
<li>
<p>
Lorenz Matzat, OpenDataCity
</p>
</li>
<li>
<p>
David McCandless, Information Is Beautiful
</p>
</li>
<li>
<p>
Geoff McGhee, Stanford University
</p>
</li>
<li>
<p>
Aidan McGuire, ScraperWiki
</p>
</li>
<li>
<p>
Philip Meyer, University of North Carolina at Chapel Hill
</p>
</li>
<li>
<p>
Claire Miller, WalesOnline
</p>
</li>
<li>
<p>
Cynthia O&#8217;Murchu, Financial Times
</p>
</li>
<li>
<p>
Aron Pilhofer, New York Times
</p>
</li>
<li>
<p>
Cl√©ment Renaud, Sharism Lab?
</p>
</li>
<li>
<p>
Anthony Reuben, BBC
</p>
</li>
<li>
<p>
Simon Rogers, Guardian Datablog
</p>
</li>
<li>
<p>
Martin Rosenbaum, BBC
</p>
</li>
<li>
<p>
Amanda Rossi, Freelance journalist
</p>
</li>
<li>
<p>
Fabrizio Scrollini, London School of Economics
</p>
</li>
<li>
<p>
Adam Thomas, Source Fabric
</p>
</li>
<li>
<p>
Jack Thurston, FarmSubsidy (TBC)
</p>
</li>
<li>
<p>
Andrew Vande Moere, infosthetics.com
</p>
</li>
<li>
<p>
Sascha Venohr, Zeit Online
</p>
</li>
<li>
<p>
Jerry Vermanen, De Stentor
</p>
</li>
<li>
<p>
César Viana, Estacio de Sa University
</p>
</li>
<li>
<p>
Farida Vis, University of Leicester
</p>
</li>
<li>
<p>
Luk N. Van Wassenhove (Affiliation?)
</p>
</li>
</ul></div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_preface">Preface</h2>
<div class="sectionbody">
<div class="paragraph"><p>This book is intended to be a useful resource for anyone who thinks that they might be interested in becoming a data journalist, or dabbling in data journalism.</p></div>
<div class="paragraph"><p>Lots of people have contributed to writing it, and through our editorial we have tried to let their different voices and views shine through. We hope that it reads like a rich and informative conversation about what data journalism is, why it is important, and how to do it.</p></div>
<div class="paragraph"><p>Lamentably the act of reading this book will not supply you with a comprehensive repertoire of all if the knowledge and skills you need to become a data journalist. This would require a vast library manned by hundreds of experts able to help answer questions on hundreds of topics. Luckily this library exists and it is called the internet. Instead we hope this book will give you a sense of how to get started and where to look if you want to go further. Examples and tutorials serve to be illustrative rather than exhaustive.</p></div>
<div class="paragraph"><p>The Data Journalism Handbook is a work in progress. If you think there is anything which needs to be amended or is conspicuously absent, then please flag it for inclusion in the next version. It is also freely available under a Creative Commons Attribution Sharealike license, and we strongly encourage you to share it with anyone that you think might be interested in reading it.</p></div>
</div>
</div>
<div class="sect1">
<h2 id="_1_introduction">1. Introduction</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_what_is_data_journalism_by_paul_bradshaw_city_university">What is data journalism? by Paul Bradshaw (City University)</h3>
<div class="paragraph"><p>What is data journalism? I could answer, simply, that it is journalism done with data. But that doesn&#8217;t help much.</p></div>
<div class="paragraph"><p>Both &#8216;data&#8217; and &#8216;journalism&#8217; are troublesome terms. Some people think of &#8216;data&#8217; as any collection of numbers, most likely gathered on a spreadsheet. 20 years ago, that was pretty much the only sort of data that journalists dealt with. But we live in a digital world now, a world in which almost anything can be - and almost everything is - described with numbers.</p></div>
<div class="paragraph"><p>Your career history; 300,000 confidential documents; who knows who in your circle of friends can all be - and are - described with just two numbers: zeroes, and ones. Photos, video and audio are all described with the same two numbers: zeroes and ones. Murders, disease, political votes, corruption and lies: zeroes and ones.</p></div>
<div class="paragraph"><p>What makes data journalism different to the rest of journalism? Perhaps it is the new possibilities that open up when you combine the traditional &#8216;nose for news&#8217; and ability to tell a compelling story, with the sheer scale and range of digital information now available.</p></div>
<div class="paragraph"><p>And those possibilities can come at any stage of the journalist&#8217;s process: using programming to automate the process of gathering and combining information from across the web, as Adrian Holovaty did with ChicagoCrime and then EveryBlock.</p></div>
<div class="paragraph"><p>Or using software to find connections between hundreds of thousands of documents, as The Telegraph did with MPs' expenses.</p></div>
<div class="paragraph"><p>Data journalism can help a journalist tell a complex story through engaging infographics - such as Hans Rosling&#8217;s talks on world poverty or David McCandless&#8217;s visualisation work. Or it can help explain how a story relates to an individual, as the BBC and FT now routinely do with their budget interactives. And it can open up the newsgathering process itself, as The Guardian do so successfully with their DataBlog.</p></div>
<div class="paragraph"><p>Data can be the source of data journalism, or it can be the tool with which the story is told - or it can be both. Like any source, it should be treated with scepticism; and like any tool, we should be conscious of how it can shape and restrict the stories that are created with it.</p></div>
</div>
<div class="sect2">
<h3 id="_why_journalists_should_use_data_by_mirko_lorenz_deutsche_welle">Why journalists should use data by Mirko Lorenz (Deutsche Welle)</h3>
<div class="paragraph"><p>Journalism is under siege. In the past we relied on being the only ones operating a technology to multiply and distribute what had happened over night. The printing press served as a gateway, if anybody wanted reach the people of a city or region the next morning, they would turn to newspapers.</p></div>
<div class="paragraph"><p>This is over.</p></div>
<div class="paragraph"><p>Today news stories are flowing in as they happen, from multiple sources, eye-witnesses, blogs and what has happened is filtered through a vast network of social connections, being ranked, commented and more often than not: ignored.</p></div>
<div class="paragraph"><p>This is why data journalism is so important. Gathering, filtering and visualizing what is happening beyond what the eye can see has a growing value. The orange juice you drink in the morning, the coffee you brew - in today&#8217;s global economy there are invisible connections between things, networks and people. The language of this network is data: Little points of information that are often not relevant in a single instance, but massively important when viewed from the right angle.</p></div>
<div class="paragraph"><p>Right now, a few pioneering journalists already demonstrate how data can be used to create revealing, surprising and deeper insights into what is happening around us and how it might affect us. All of us.</p></div>
<div class="paragraph"><p>Data analysis can reveal "a story&#8217;s shape" (Sarah Cohen), provides a "new camera" (David McCandless). To get to that point the data investigation must be done in a step by step process that is in some ways similar to uncovering a lost city from the dirt by slowly brushing away all the stuff that keeps us from seeing its structure.</p></div>
<div class="paragraph"><p>Why journalists should use data</p></div>
<div class="paragraph"><p>But why should - of all professions - journalists be more interested in learning how to work with data when doing research for stories? There are several reasons: First, journalists are a group in society taking care of distributing what is relevant. At least if they are good. How this is done might sometimes be to the point and very inspiring, at other times sloppy, rushed and even misconstruing the facts. But, quite simply, in doing this job journalists (insert: writers, photographers, film makers) strive to help us understand, and this is important.</p></div>
<div class="paragraph"><p>Using data the job of journalists shifts its main focus from being the first ones to report to being the ones telling us what a certain development might actually mean. The range of topics can be far and wide. The next financial crisis that is in the making. The economics behind the products we use. The misuse of funds or political blundery.</p></div>
<div class="paragraph"><p>There is one barrier keeping journalists from using this potential: training in order to learn how to work with data through all the steps from a first question to a big data-driven scoop.</p></div>
<div class="paragraph"><p>Working with data is like stepping into vast, unknown territory. At first look, raw data is puzzling the eyes and the mind. Data as such is unwieldy, it is quite hard to shape it correctly for visualization. It needs experienced journalists, who have the stamina to look at often confusing, often boring raw data and "see" the hidden stories in there.</p></div>
<div class="paragraph"><p>On the other side: What choice do we as journalists have? Working with data can provide us with deep, original ways to report about what is going on. Once the data is "speaking" to us, all of sudden a very rich source to tell stories has opened up.</p></div>
<div class="paragraph"><p>And, increasingly, journalists see that particular opportunity. Judging on the results of a survey we conducted to find out more about training needs for data journalism, there is a big willingness to get out of the comfort zone of traditional journalism and invest time to master the new skills. The results from the survey showed us that journalists are "seeing" the opportunity, but need a bit of support to cut through the initial problems keeping them from working with data. There is a confidence, that should data journalism get more adopted, the work flows, the tools and the results will improve quite quickly. The pacemakers, such as the Multimedia team of the New York Times, The Guardian Data Blog, the Texas Tribune, and Die Zeit play an important role by being willing to publish such stories.</p></div>
<div class="paragraph"><p>As of now, we should not be too confident this data journalism will be more than a passing trend. There is potential, but as of now single journalists and newsrooms as a whole need to overcome the barriers keeping us from telling stories out of the data.</p></div>
<div class="paragraph"><p>This handbook provides one step towards that goal. It might not provide the total information needed to master data journalism, but it shows the enthusiasm, the power and the hope associated with it.</p></div>
<div class="paragraph"><p>Come join us.</p></div>
</div>
<div class="sect2">
<h3 id="_what_is_it_good_for_notes_from_the_field">What is it good for? Notes from the field</h3>
<div class="paragraph"><p><strong>Filtering the flow of data</strong></p></div>
<div class="quoteblock">
<div class="content">
<div class="paragraph"><p>When information was scarce, most of our efforts were devoted to hunting and gathering. Now that information is abundant, processing is more important. We process at two levels: (1) analysis to bring sense and structure out of the never-ending flow of data and (2) presentation to get what&#8217;s important and relevant into the consumer&#8217;s head. Like science, data journalism discloses its methods and presents its findings in a way that can be verified by replication.</p></div>
</div>
<div class="attribution">
&#8212; Philip Meyer (University of North Carolina at Chapel Hill)
</div></div>
<div class="paragraph"><p><strong>New approaches to storytelling</strong></p></div>
<div class="quoteblock">
<div class="content">
<div class="paragraph"><p>Data journalism is an umbrella term that, to my mind, encompasses an ever-growing set of tools, techniques and approaches to storytelling. It can include everything from traditional computer-assisted reporting (using data as a <em>source</em>) to the most cutting edge data visualization and news applications. The unifying goal is a journalistic one: providing information and analysis to help inform us all about important issues of the day.</p></div>
</div>
<div class="attribution">
&#8212; Aron Pilhofer (New York Times)
</div></div>
<div class="paragraph"><p><strong>Like photo journalism with a laptop</strong></p></div>
<div class="quoteblock">
<div class="content">
<div class="paragraph"><p>&#8216;Data journalism&#8217; only differs from &#8216;words journalism&#8217; in that we use a different kit. We all sniff out, report, and relate stories for a living. It&#8217;s like <em>photo journalism</em> - just swap the camera for a laptop.</p></div>
</div>
<div class="attribution">
&#8212; Brian Boyer (Chicago Tribune)
</div></div>
<div class="paragraph"><p><strong>Enhancing traditional journalistic practices</strong></p></div>
<div class="quoteblock">
<div class="content">
<div class="paragraph"><p>Broadly speaking, data journalism involves using data sources to augment or enhance traditional journalistic practices. This might include things like: finding new ways of telling stories, giving readers new angles or insights into the news, putting issues into context, and giving readers access to underlying information sources via new digital applications and services.</p></div>
</div>
<div class="attribution">
&#8212; Jonathan Gray (Open Knowledge Foundation)
</div></div>
<div class="paragraph"><p><strong>Number-crunching meets word-smithing</strong></p></div>
<div class="quoteblock">
<div class="content">
<div class="paragraph"><p>Data journalism is bridging the gap between stat technicians and wordsmiths. Locating outliers and identifying trends that are not just statistically significant, but relevant to de-compiling the inherently complex world of today.</p></div>
</div>
<div class="attribution">
&#8212; David Anderton (Freelance Journalist)
</div></div>
<div class="paragraph"><p><strong>Updating a journalist&#8217;s skills set</strong></p></div>
<div class="quoteblock">
<div class="content">
<div class="paragraph"><p>A new set of skills for searching, understanding and visualising digital sources in a time that basic skills from traditional journalism just aren&#8217;t enough. It&#8217;s not a replacement of traditional journalism, but an addition to it.</p></div>
</div>
<div class="attribution">
&#8212; Jerry Vermanen (NU.nl)
</div></div>
<div class="paragraph"><p><strong>As a remedy for information asymmetry</strong></p></div>
<div class="quoteblock">
<div class="content">
<div class="paragraph"><p>Information asymmetry - not the lack of information, but the inability to take in and process it with the speed and volume that it comes to us - is one of the most significant problems that citizens face in making choices about how to live their lives. Information taken in from print, visual and audio media influence citizens' choices and actions. Good data journalism helps to combat information asymmetry.</p></div>
</div>
<div class="attribution">
&#8212; Tom Fries (Bertelsmann Foundation)
</div></div>
<div class="paragraph"><p><strong>As an answer to data-driven PR</strong></p></div>
<div class="quoteblock">
<div class="content">
<div class="paragraph"><p>The availability of measurement tools and their decreasing prices, in a self-sustaining combination with a focus on performance and efficiency in all aspects of society, have led decision-makers to quantify the progresses of their policies, monitor trends and identify opportunities.</p></div>
<div class="paragraph"><p>Companies keep coming up with new metrics showing how well they perform. Politicians love to brag about reductions in unemployment numbers and increases in GDP. The lack of journalistic insight in the Enron, Worldcom, Madoff or Solyndra affairs is proof of many a journalist&#8217;s inability to clearly see through numbers. Figures are more likely to be taken at face value than other facts as they carry an aura of seriousness, even when they are entirely fabricated.</p></div>
<div class="paragraph"><p>Fluency with data will help journalists sharpen their critical sense when faced with numbers and will hopefully help them gain back some terrain in their exchanges with PR departments.</p></div>
</div>
<div class="attribution">
&#8212; Nicolas Kayser-Bril (Independent Data Journalist)
</div></div>
<div class="paragraph"><p><strong>As a new "flavor" of information that reaches a new group of consumers</strong></p></div>
<div class="quoteblock">
<div class="content">
<div class="paragraph"><p>In the same way that consumers of the arts might prefer painting to poetry, fashion to film, consumers of information find it easier, or more fun, to consumer information of one type vs. another. A long essay on the Arab Spring reaches a different audience than a 90-second news segment on the same topic. The addition of meaningful and substantial data as a "format" for information that could also be delivered in other ways reaches a new segment of information consumers.</p></div>
</div>
<div class="attribution">
&#8212; Tom Fries (Bertelsmann Foundation)
</div></div>
<div class="paragraph"><p><strong>As a source of new perspectives</strong></p></div>
<div class="quoteblock">
<div class="content">
<div class="paragraph"><p>Good data journalism allows information consumers to see relationships, proportions and other qualities in ways that other formats do not, and different portrayals of information tell different stories, even with the same data.</p></div>
</div>
<div class="attribution">
&#8212; Tom Fries (Bertelsmann Foundation)
</div></div>
<div class="paragraph"><p><strong>As a teaching tool for journalists</strong></p></div>
<div class="quoteblock">
<div class="content">
<div class="paragraph"><p>In the same way that learning a new language allows a person to begin to make a different set of concepts concrete, learning the skills and tools associated with data journalism improves the thinking of journalists who take the time to do it. The process of adding data journalism to the "toolbox" of the average journalist also forces collaboration with categories of professionals with which journalists are usually not accustomed to working such as developers or designers.</p></div>
</div>
<div class="attribution">
&#8212; Tom Fries (Bertelsmann Foundation)
</div></div>
<div class="paragraph"><p><strong>As a forensic tool for readers</strong></p></div>
<div class="quoteblock">
<div class="content">
<div class="paragraph"><p>Journalists, and news outlets generally, betray their perspective in the data that they choose to present, and how they choose to present it. A discerning reader can see the output as a "tell" of a journalist&#8217;s or organization&#8217;s political or intellectual disposition.</p></div>
</div>
<div class="attribution">
&#8212; Tom Fries (Bertelsmann Foundation)
</div></div>
<div class="paragraph"><p><strong>Redressing the balance of power</strong></p></div>
<div class="quoteblock">
<div class="content">
<div class="paragraph"><p>"The task of journalism is to hold power to account. The corporate sector and governments have Big Data, and they know more about you than you know about them. Data journalism can help redress the balance of power."</p></div>
</div>
<div class="attribution">
&#8212; Andrew Mackenzie (Independent Researcher)
</div></div>
<div class="paragraph"><p><strong>As response to the digitization of sources</strong></p></div>
<div class="quoteblock">
<div class="content">
<div class="paragraph"><p>In a time where sources go digital, journalists can and have to be closer to those sources. The Internet opened up possibilities beyond our current understanding. Data journalism is just the beginning of evolving our past practices to adapt to the online.</p></div>
<div class="paragraph"><p>Data journalism serves two important purposes for news organisations: finding unique stories (not from news wires) and execute your watchdog function. Especially in times of financial peril, these are important goals for newspapers to achieve.</p></div>
<div class="paragraph"><p>From the standpoint of a regional newspaper, data journalism is crucial. We have the saying &#8216;a loose tile in front of your door is considered more important than a riot in a far-away country&#8217;. It&#8217;s hits you in the face and impacts your life more directly. At the same time, digitization is everywhere. Because local newspapers have this direct impact in their neighbourhood and sources become digitalized, a journalist must know how to find, analyze and visualize a story from data.</p></div>
</div>
<div class="attribution">
&#8212; Jerry Vermanen (De Stentor)
</div></div>
</div>
<div class="sect2">
<h3 id="_our_favourite_examples">Our favourite examples</h3>
</div>
<div class="sect2">
<h3 id="_data_journalism_in_perspective_by_liliana_bounegru_european_journalism_centre">Data journalism in perspective by Liliana Bounegru (European Journalism Centre)</h3>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_2_in_the_newsroom">2. In the newsroom</h2>
<div class="sectionbody">
<div class="paragraph"><p>A survey on training needs for data journalism circulated by the European Journalism Centre between April and August 2011 showed that 39% of the 234 respondents envisioned their organisation to start engaging in data journalism through engaging a combination of external experts and existing staff. 36% envisioned training existing staff.</p></div>
<div class="paragraph"><p>In the following chapter journalists involved in data journalism projects explain models for doing data journalism in their newsroom.</p></div>
<div class="paragraph"><p>In-house expertise</p></div>
<div class="sect2">
<h3 id="_how_the_news_apps_team_at_chicago_tribune_works_by_brian_boyer_chicago_tribune">How the news apps team at Chicago Tribune works by Brian Boyer (Chicago Tribune)</h3>
<div class="paragraph"><p>The news applications team at the Chicago Tribune is a band of happy hackers embedded in the newsroom. We work closely with editors and reporters to help 1) research and report stories, 2) illustrate stories online and 3) build evergreen web resources for the fine people of Chicagoland.</p></div>
<div class="paragraph"><p>It&#8217;s important that we sit in the newsroom. We usually find work via face-to-face conversations with reporters. They know that we&#8217;re happy to help write a screen scraper for a crummy government website, tear up a stack of PDFs, or otherwise turn non-data into something you can analyze. It&#8217;s sort of our team&#8217;s loss leader&#8201;&#8212;&#8201;this way we find out about potential data projects at their outset.</p></div>
<div class="paragraph"><p>Unlike many teams in this field, our team was founded by technologists for whom journalism was a career change. Some of us acquired a masters degree in journalism after several years coding for business purposes, and others were borrowed from the open government community.</p></div>
<div class="paragraph"><p>We work in an agile fashion. To make sure we&#8217;re always in sync, every morning begins with a 5-minute stand up meeting. We frequently program in pairs&#8201;&#8212;&#8201;two developers at one keyboard are often more productive than two developers at two keyboards. Most projects don&#8217;t take more than a week to produce, but on longer projects we work in week-long iterations, and show our work to stakeholders&#8201;&#8212;&#8201;reporters and editors usually&#8201;&#8212;&#8201;every week. "Fail fast" is the mantra. If you&#8217;re doing it wrong, you need to know as soon as possible, especially when you&#8217;re coding on a deadline!</p></div>
<div class="paragraph"><p>There&#8217;s a tremendous upside to hacking iteratively, on a deadline: We&#8217;re always updating our toolkit. Every week we crank out an app or two, then, unlike normal software shops, we can put it to the back of our mind and move on to the next project. It&#8217;s a joy we share with the reporters, every week we learn something new.</p></div>
<div class="paragraph"><p># Projects realized with external experts</p></div>
<div class="paragraph"><p>Some datasets or methods require special skills i.e. programming skills to manage the data and visualize it. Don&#8217;t stop your passion for your idea because of missing these skills in your newsroom. There are a lot of highly motivated developers and designers out there to come into your project.</p></div>
</div>
<div class="sect2">
<h3 id="_the_abc_s_data_journalism_play_by_wendy_carlisle">The ABC’s data journalism play by Wendy Carlisle</h3>
<div class="paragraph"><p>Now in its 70th year the Australian Broadcasting Corporation is Australia’s national public broadcaster. Annual funding is around AUS$1bn which delivers seven radio networks, 60 local radio stations, three digital television services, a new international television service and an online platform to deliver this ever expanding offering of digital and user generated content. At last count there were in excess of 4,500 full time equivalent staff and nearly 70% of them make content.</p></div>
<div class="paragraph"><p>We are a national broadcaster fiercely proud of our independence – because although funded by government – we are separated at arm’s length through law. Our traditions are independent public service journalism. The ABC is regarded the most trusted news organisation in the country.</p></div>
<div class="paragraph"><p>These are exciting times  and under a managing director – the former newspaper executive Mark Scott – content makers at the ABC have been encouraged to  as the   corporate mantra puts it – be “agile”.</p></div>
<div class="paragraph"><p>Of course that’s easier said than done.</p></div>
<div class="paragraph"><p>But one initiative in recent times designed to encourage this has been  a competitive staff pitch for money to develop multi-platform projects.</p></div>
<div class="paragraph"><p>This is how the ABC’s first ever data journalism project was conceived.</p></div>
<div class="paragraph"><p>Sometime early in 2010 I wandered into the pitch session to face with three senior “ideas” people with my proposal.</p></div>
<div class="paragraph"><p>I’d been chewing it over for some time. Greedily lapping up the data journalism that the now legendary Guardian data journalism blog was offering, and that was just for starters.</p></div>
<div class="paragraph"><p>It was my argument that no doubt within 5 years the ABC would have its own data journalism unit. It was inevitable, I opined. But the question was how are we going to get there, and whose going to start.</p></div>
<div class="paragraph"><p>For those readers unfamiliar with the ABC  think of a vast bureaucracy built up over 70 years. Its primary offering was always radio and television. With the advent of online in the last decade this content offering unfurled into text, stills and a degree of interactivity previously unimagined. The web space was forcing the ABC to rethink how it cut the cake (money) and rethink what kind of cake it was baking (content).</p></div>
<div class="paragraph"><p>It is of course a work in progress.</p></div>
<div class="paragraph"><p>But something else was happening with data journalism. Government 2.0 (which as we discovered is largely observed in the breach in Australia) was starting to offer new ways of telling stories that were hitherto buried in the zero’s and dots.</p></div>
<div class="paragraph"><p>All this I said to the folk during my pitch. I also said we needed to identify new skills sets, train journalists in new tools. We needed a project to hit play.</p></div>
<div class="paragraph"><p>And they gave me the money. </p></div>
<div class="paragraph"><p>On the 24th of November 2011 the ABC’s multiplatform project and ABC News Online went live with “Coal Seam Gas by the Numbers”</p></div>
<div class="paragraph"><p><a href="http://www.abc.net.au/news/specials/coal-seam-gas-by-the-numbers/promise/">http://www.abc.net.au/news/specials/coal-seam-gas-by-the-numbers/promise/</a></p></div>
<div class="paragraph"><p>It was five pages of interactive maps, data visualisations and text.</p></div>
<div class="paragraph"><p>It wasn’t exclusively data journalism – but a hybrid of journalisms that was born of the mix of people on the team and the story, which to put in context is raging as one of the hottest issues in Australia.</p></div>
<div class="paragraph"><p>The jewel was an interactive map showing coal seam gas wells and leases in Australia. Users could search by location and switch between modes to show leases or wells. By zooming in users could see who the explorer was, the status of the well and its drill date.  Another map showed the location of coal Seam gas activity compared to the location of groundwater systems in Australia.</p></div>
<div class="paragraph"><p>We had data visualisations which specifically addressed this issue of waste salt and water production that would be produced depending on the scenario that emerged.</p></div>
<div class="paragraph"><p>Another section of the project investigated the release of chemicals into a local river system</p></div>
<div class="paragraph"><p><strong>Our team</strong></p></div>
<div class="ulist"><ul>
<li>
<p>
A web developer and designer.
</p>
</li>
<li>
<p>
A lead journalist
</p>
</li>
<li>
<p>
A part time researcher with expertise in data extraction, excel spread sheets and data cleaning
</p>
</li>
<li>
<p>
A part time junior journalist.
</p>
</li>
<li>
<p>
A consultant executive producer
</p>
</li>
<li>
<p>
A academic consultant with expertise in data mining, graphic visualisation and advanced research skills.
</p>
</li>
<li>
<p>
The services of a project manager and the administrative assistance of the ABC’s multi-platform unit.
</p>
</li>
<li>
<p>
Importantly we also had a reference group of journalists and others whom we consulted on a needs basis.
</p>
</li>
</ul></div>
<div class="paragraph"><p><strong>Where did we get the data from?</strong></p></div>
<div class="paragraph"><p>The data for the interactive maps were scrapped from shapefiles downloaded from government websites.</p></div>
<div class="paragraph"><p>Other data on salt and water were taken from a variety of reports.</p></div>
<div class="paragraph"><p>The data on chemical releases was taken from Environmental permits issued by the government.</p></div>
<div class="paragraph"><p><strong>“The Learnings”</strong></p></div>
<div class="paragraph"><p>Coal seam gas by the numbers was an ambitious in content and scale.  Uppermost in my mind was what did we learn and how might we do it differently next time?</p></div>
<div class="paragraph"><p>The data journalism project brought a lot of people into the room who do not normally meet at the ABC. In lay terms – the hacks and the hackers. Many of us did not speak the same language or even appreciate what the other does.  Data journalism is disruptive!</p></div>
<div class="paragraph"><p>The practical things:</p></div>
<div class="ulist"><ul>
<li>
<p>
Co-location of the team is vital. Our developer and designer were off-site and came in for meetings. This is definitely not optimal! Place in the same room as the journalists.
</p>
</li>
<li>
<p>
Our consultant EP was also on another level of the building. We needed to be much closer, just for the drop-by factor
</p>
</li>
<li>
<p>
Choose a story that is solely data driven.
</p>
</li>
</ul></div>
<div class="paragraph"><p><strong>The big picture…some ideas.</strong></p></div>
<div class="ulist"><ul>
<li>
<p>
Big media organisations need to engage in capacity building to meet the challenges of data journalism. My hunch is there are a lot of geeks and hackers hiding in media technical departments desperate to get out. So –we need “hack and hacker meets” workshops where the secret geeks, younger journalists, web developers and designers come out to play with more experienced journalists for skill sharing and mentoring. Task: download this data set and go for it!
</p>
</li>
<li>
<p>
Ipso facto Data journalism is interdisciplinary.  DJ teams are made of people who would not in the past have worked together. The digital space has blurred the boundaries/
</p>
</li>
<li>
<p>
We live in a fractured, distrustful body politic.  The digital space makes everyone a content maker…we can all be journalists now, right?  Wrong. Journalists need to reassert themselves as ethical, trustworthy, honest story tellers.  Data journalism offers the opportunity to turn  the signal to noise ratio into something we can all understand. Journalists are going to need to be literate in data journalism tools to
</p>
</li>
<li>
<p>
Data journalism is still all about story telling.
</p>
</li>
<li>
<p>
Increasingly the journalists of tomorrow will be   data journalists.
</p>
</li>
<li>
<p>
Australia is behind Europe and the United States in data journalism. Why? That’s another discussion….
</p>
</li>
</ul></div>
<div class="paragraph"><p>Wendy Carlisle has been an ABC journalist for 20 years and is primarily an investigative reporter working with radio’s investigative program “Background Briefing” and the Four Corners program on ABC TV.  She was the lead journalist on “Coal Seam gas: by the Numbers”
Background Briefing <a href="http://www.abc.net.au/radionational/programs/backgroundbriefing/">http://www.abc.net.au/radionational/programs/backgroundbriefing/</a>
Four Corners <a href="http://www.abc.net.au/4corners/">http://www.abc.net.au/4corners/</a></p></div>
</div>
<div class="sect2">
<h3 id="_data_journalism_at_the_zeit_online_by_sascha_venohr_zeit_online">Data journalism at the Zeit Online by Sascha Venohr (Zeit Online)</h3>
<div class="paragraph"><p>The PISA based Wealth Comparison project is an interactive visualisation that enables comparison of standards of living in different countries. The interactive uses data from the OECD&#8217;s comprehensive world education ranking report, PISA 2009, published in December 2010. The report is based on a questionnaire which asks fifteen-year-old pupils about their living situation at home.</p></div>
<div class="paragraph"><p>The idea was to analyse and visualise this data to provide a unique way of comparing standards of living in different countries.</p></div>
<div class="paragraph"><p>First of all our in-house editorial team decided which facts seemed to be useful to make living standards comparable and should be visualised, including:</p></div>
<div class="paragraph"><p>Wealth (number of owned TVs, cars and available bathrooms at home)
Family-situation (are there grandparents living with the family together, percentage share of families with only one child, unemployment of parents and mother&#8217;s job status)
Access to knowledge sources (internet at home, frequency of using e-mail and quantity of owned books)
Additionally the OECD set three regional wealth-items in the questionnaire. This preselection of items shows on very special way the development-level of each country.</p></div>
<div class="paragraph"><p>With the help of the internal design team these facts were translated into self-explanatory icons. A front end design was built to make comparison between the different countries like in a card-game possible.</p></div>
<div class="paragraph"><p>Next we contacted people from the German Open Data Network to find developers who could help with the project. This community of highly motivated people suggested Gregor Aisch, a very talented information designer, to code the application that would make our dreams come true (Flash-free#, which is important to us!). Based on the Raphaël-Javascript Library he created a very high quality and interactive visualisation with a beautiful bubble-style.</p></div>
<div class="paragraph"><p>The result of our hand in hand work was a very successful interactive which got a lot of traffic and because of the easy possibility to combine two countries by URL. We can re-use the tool in our daily editorial work (i.e. on the coverage of the living situation in Indonesia we can have a quick view on the situation in that country compared to Germany. The know-how transferred to our in house-team was a great starting-capital for future projects.</p></div>
<div class="paragraph"><p>For links and more information related to this case study. Please see the appendix.</p></div>
</div>
<div class="sect2">
<h3 id="_how_to_hire_a_hacker_by_lucy_chambers_open_knowledge_foundation">How to hire a hacker by Lucy Chambers (Open Knowledge Foundation)</h3>
<div class="paragraph"><p>One of the things that I am regularly asked by journalists is <em>how do I get a coder to help me with my project?</em>. Don&#8217;t be deceived into thinking this is a one-way process; civic-minded hackers and data-wranglers are often just as keen to get in touch with journalists. Reporters are power-users of the tools they build, often thinking outside the box to use them in contexts they hadn&#8217;t considered before so feedback is invaluable, they also help to build context and buzz around projects and give them social relevance. It&#8217;s obviously a symbiotic relationship. Whether you are looking to hire, or possibilities to collaborate with a shoestring budget, there will be someone out there to help.</p></div>
<div class="paragraph"><p>"You may find that your organisation already has people with all the skills you need, but they are not necessarily already in your newsroom. Wander around, visit the technology and IT departments and you are likely to strike gold. It is also important to appreciate coder culture, come across someone who has a computer that looks like this&#8230;</p></div>
<div class="paragraph"><p>todo: insert image of laptop</p></div>
<div class="olist lowerroman"><ol class="lowerroman">
<li>
<p>
you are probably onto a winner." (Aron Pilhofer)
</p>
</li>
</ol></div>
<div class="paragraph"><p>Hiring and Engaging</p></div>
<div class="ulist"><ul>
<li>
<p>
Post on coders jobs boards e.g. the Python Job Board
</p>
</li>
<li>
<p>
Disseminate ads via mailing lists. NICAR-L and the Data-Driven-Journalism List are two notably popular ones.
</p>
</li>
<li>
<p>
Organisations such as Scraperwiki have a great address book of trusted and willing coders who can be hired for individual scraping tasks.
</p>
</li>
</ul></div>
<div class="paragraph"><p>Making contacts &amp; possibilities for collaboration</p></div>
<div class="ulist"><ul>
<li>
<p>
Look out for groups such as Hacks/Hackers which bring techies together with the media community - now springing up all over the world
</p>
</li>
<li>
<p>
Local Interest Communities: A quick Google e.g. Javascript + (Location) will usually bring you some interesting results. Sites such as Meetup.com - are also great places to look.
</p>
</li>
<li>
<p>
Hackathons &amp; Competitions. Whether or not there is prize money available: app and visualisation competitions and development days are often fruitful ground for collaboration and making connections.
</p>
</li>
<li>
<p>
Ask a nerd! Nerds hang around with more nerds, as seen in Sascha&#8217;s example, communities such as the open data community are often good places to start.
</p>
</li>
</ul></div>
</div>
<div class="sect2">
<h3 id="_regiohack_by_jerry_vermanen_de_stentor">Regiohack by Jerry Vermanen (De Stentor)</h3>
<div class="paragraph"><p>Introduction</p></div>
<div class="paragraph"><p>March 2010, an digital culture organisation in Utrecht called SETUP held an event titled Hacking Journalism. In a room filled with programmers, nerds and journalists, the need for collaboration between these people was evident.</p></div>
<div class="paragraph"><p><em>We organize hackathons to make cool applications, but we can&#8217;t recognise interesting stories in data. What we build has no social relevance</em>, said the programmers. <em>We recognize the importance of data journalism, but we miss the technical skills to really dig into this field</em>, according to the journalists.</p></div>
<div class="paragraph"><p>Working for a regional newspaper, there was no money or incentive to hire a programmer for the newsroom. Besides that, data journalism was still an unproven discipline at that time for Dutch newspapers.</p></div>
<div class="paragraph"><p>The hackathon model seemed perfect. An unconstrained environment for collaboration, especially when combined with pizza and energy drink. RegioHack was a hackathon organised by my employer, the regional newspaper de Stentor, our sister publication TC Tubantia and Saxion Hogescholen Enschede, who provided the location for the event.</p></div>
<div class="paragraph"><p>Main goals</p></div>
<div class="paragraph"><p>The setup was as following: everyone could enlist for a 30-hour hackathon. We provided the food, drinks, workshops, etc. We aimed for 30 participants, which we divided into 6 groups. These groups were assigned different subjects: crime, health, transport, safety, ageing and power. The three main goals for this event:</p></div>
<div class="paragraph"><p>Journalistic innovation through data journalism stories</p></div>
<div class="paragraph"><p>For us, data journalism is something new and unknown. The only way we can prove its use, is through well crafted stories. We planned to produce at least three data stories.</p></div>
<div class="paragraph"><p>Sharing knowledge and connecting individuals across disciplines</p></div>
<div class="paragraph"><p>We, the journalists, don&#8217;t how data journalism is done and we don&#8217;t pretend to know. By putting journalists, students and programmers in one room for 30 hours, we want them to share knowledge and insights.</p></div>
<div class="paragraph"><p>Organise a data journalism event</p></div>
<div class="paragraph"><p>Newspapers don&#8217;t organize a lot of social events, let alone hackathons. We want to experience how such an event can yield results. In fact, the setting of RegioHack was tense: 30 hours with people that speak in jargon, bashing your head against basic questions, working out of your comfort zone. By making it a social event - remember the pizza and energy drink? - we wanted to unnerve the situation and create an environment in which journalists and programmers learn through collaboration.</p></div>
<div class="paragraph"><p>Results</p></div>
<div class="paragraph"><p>Before the event, TC Tubantia had an interview with the widow of a policeman who had written a book on her husband&#8217;s working years. She also had a document with all registered murders in the eastern part of the Netherlands, maintained by her husband since 1945. Normally, we would publish this document on our website. This time, we made a dashboard in Tableau software. We also blogged about how this came together on our RegioHack site.</p></div>
<div class="paragraph"><p>During the hackathon, one project group came up with the subject of development of schools and the ageing of our region. By making a visualisation of future projections, we understood which cities would get in trouble after a few years of decline in enrollments. With this insight, we made an article on how this would affect schools in our region.</p></div>
<div class="paragraph"><p>We also started a very ambitious project, called De Tweehonderd van Twente (in English, The Two Hundred of Twente) to determine who had the most power in our region and build a database of the most influential people. Through a Google-ish calculation - who has the most ties with powerful organisations - a list of influential people will be composed. This could lead to a series of articles, but it&#8217;s also a powerful tool for journalists. Who has connections with who? You can ask questions to this database and use it in our daily routine. Also, this database has cultural value. Artists already asked if they could use this database when finished to make interactive art installations.</p></div>
<div class="paragraph"><p>After RegioHack, we noticed that journalists considered data journalism as a viable addition to traditional journalism. My colleagues continued to use and build on the techniques learned on the day to create more ambitious and technical projects such as a database of the administrative costs of housing. With this data, I made an interactive map in Fusion Tables. We asked our readers to play around with the data and crowdsourced results (here, for example). After a lot of questions on how we made a map in Fusion Tables, I also recorded a video tutorial.</p></div>
<div class="paragraph"><p>What did we learn?</p></div>
<div class="paragraph"><p>We learned a lot, but we also came along a lot of obstacles. We recognized these four:
Where to begin: question or data?</p></div>
<div class="paragraph"><p>Almost all projects stalled when searching for information. What datasets are interesting? Where do we find those? And does this answer our research question? When when we found an interesting dataset, it mostly didn&#8217;t answer the question asked.</p></div>
<div class="paragraph"><p>Tip: To avoid this problem, organise <em>data-parties</em> in advance of a hackday. Whether online or in person, these give people an opportunity to identify (and acquire if scraping is needed) useful datasets, familiarise themselves with them and clean them up and remix them. This saves precious time on the day.</p></div>
<div class="paragraph"><p>Lucy Chambers</p></div>
<div class="paragraph"><p># Programming knowledge requested #</p></div>
<div class="paragraph"><p>Technical knowledge was not distributed evenly among the participants. A small group of programmers carried the weight of doing the scraping and visualizing. We expected this in advance, so we assigned a central desk with technical skills (Tableau, custom visualisations, Excel, etc.). In addition to what knowledge every project group had, this central desk could help the groups out with problems.</p></div>
<div class="paragraph"><p>Not a lot of combining data</p></div>
<div class="paragraph"><p>Participants didn&#8217;t combine many datasets. Instead, they took one dataset, tried to visualize it and draw conclusions based on the results. This isn&#8217;t wrong, but data journalism gets more interesting in the interconnections of datasets.</p></div>
<div class="paragraph"><p>What&#8217;s the routine?</p></div>
<div class="paragraph"><p>What above all comes down to, is that there&#8217;s no routine. The participants have some skills under their belt, but don&#8217;t know how and when to use them.</p></div>
<div class="paragraph"><p>One journalist compared it with baking a cake. <em>We have all the ingredients: flour, eggs, milk, etcetera. Now we throw it all in a bag, shake it and hope a cake comes out of it.</em> Indeed, we have all the ingredients, but don&#8217;t know what the recipe is.</p></div>
<div class="paragraph"><p>Tip: Think in advance of the type of people who will take part in your hackday and make sure they will all have something to do. It is uncomfortable for all involved if some parties feel they don&#8217;t have a role and are just watching other people work. Non-techies can be making calls and doing background research, data-wrangling, getting releases ready or just making sure the energy drinks keep flowing! Make sure you keep talking to people in your team, and ask them whether there is anything you could do to help out&#8230;</p></div>
<div class="paragraph"><p>Lucy Chambers</p></div>
<div class="paragraph"><p>What now?</p></div>
<div class="paragraph"><p>Our first experiences with data journalism could help other journalists or programmers aspiring the same field of work and we are working to produce a report.</p></div>
<div class="paragraph"><p>Also, we are considering how to continue RegioHack in a hackathon form. It&#8217;s social, educational and productive and a great introduction to data journalism.</p></div>
<div class="paragraph"><p>But for data journalism to work, we have to integrate it in the newsroom. Journalists have to think in data, in addition to quotes, press releases, council meetings and what more. By doing RegioHack, we proved to our audience that data journalism isn&#8217;t just hype. We can write better informed and more distinctive articles, while presenting our readers different articles in print and online.</p></div>
<div class="paragraph"><p>(Late addition)</p></div>
</div>
<div class="sect2">
<h3 id="_the_mapa76_hackathon_by_mariano_blejman_hacks_hackers">The Mapa76 hackathon by Mariano Blejman (Hacks/Hackers)</h3>
<div class="paragraph"><p>We opened the chapter of Hacks/Hackers Buenos Aires in April 2011, and we made two meetups to publicize the idea of working associatively between journalists and software developers. To these meetups attended between 120 and 150 people each time. The third meeting was a 30-hour hackathon, where eight people worked in html5 interactive visualization and popcorn in a digital journalism conference in the city of Rosario, 300 kilometers from Buenos Aires. During these meetings, an idea showed up: apply scrapping techniques to interpret large volumes of data and then display them. Then came a project that we called Mapa76.info, which aims to go from the automatic extraction of data to display lifre stories in timeline display and maps. A difficult task.</p></div>
<div class="paragraph"><p>Why Mapa76?
On March 24, 1976 there was a coup in Argentina, which lasted until 1983. In that period there were an estimated 30,000 disappeared people, thousands of deaths, and 500 children born in captivity appropriated for the military dictatorship. Over 30 years later, in Argentina the number of persons convicted of crimes against humanity committed during the dictatorship, amounts to 262 people (September 2011). Currently there are 14 ongoing trials and 7 with definite date for its inception. As is known publicly that there are 802 peoples processed in various court cases that are open. Of these, 412 have one or more causes elevated to trial, while another 102 have the upliftment order to trial by the prosecution.</p></div>
<div class="paragraph"><p>Such prosecutions generate large volumes of information that make hard the work of researchers, journalists, human rights organizations, judges, prosecutors, etc. - Is complex when collecting and analyzing data that can help to clarify facts. Much more information is produced in a dispersed manner, which researchers can process. Basically the problem is that investigators doesn&#8217;t take advantage of IT tools applied to solving such problems. In most cases, both from the journalistic perspective and from the judiciary, the survey data was done manually on the basis of records already generated (some digital, some not). These practices represent the universe of analysis, to overlook certain facts and ultimately significantly inhibit exploration of the facts, and the construction of conjectures and / or global findings by researchers newspaper. Mapa76 should be considered as a tool of investigative journalism, historical and judicial open access through the Web. But also include private layers.</p></div>
<div class="paragraph"><p>Previous work
To reach the hackathon, we worked before in conducting a platform for programmers and journalists could work the day of the meeting. Martin Sarsale developed the basics algorithms to extract information from simple text documents. Some libraries were also used from DocumentCloud.org project, but not many. We came with a basic platform, where the user could work with a document on the site, this document was automatically analyzed and mined there names, dates and places automatically. Once extracted this information, "operator" take the case to tell his life story dates manually linking with "actions" (birth, place of arrest, the alleged place of disappearance, etc.)</p></div>
<div class="paragraph"><p>The goal is to provide a platform for automatic extraction of data on the judgments of the military dictatorship in Argentina. The aim is to achieve automatic (or semiautomatic) display of life histories of the period 1976-1983 based on written evidence, arguments and judgments about dictatorship age. This platform aims to provide tools to analyze a document database (a set of documents) and help identify finding relationships difficult in manual way. The extracted data (names, places and dates) are collected, stored and can be refined by the researcher, in addition to viewing on a geolocated, timelines and maps of relationships between people. We found existing projects related to the investigation and we had several meetings with investigators, some of them post-hackathon.</p></div>
<div class="ulist"><ul>
<li>
<p>
Federal Network Places of Memory
This systematization meets the highest amount of information about State terrorism nationally and can be easily accessed through a search engine navigable online, allowing multiple combinations of data and have photographs and mapping applications to performfixed and interactive maps based on Geographical Information Systems (GIS). The project also contemplates the use of the tool, constant updating and cooperative construction of information among all users of the system, after collation and validation by the National Archives of Memory. The aim is to establish itself as a substantial contribution to the investigations of crimes against humanity, especially those related to judicial proceedings.
</p>
</li>
<li>
<p>
Provincial Archives of Memory in Cordoba
It is primarily a task for <strong>file</strong>, digitize and preserve documents. They currently have over 100,000 pages scanned from testimony given in the different cases of crimes against humanity that took place at the Juzgado Federal No. 2, police memos, rulings, judgments and so on. Systematized in collections using library software Greenstone Digital Library (<a href="http://www.greenstone.org/">http://www.greenstone.org/</a>).
</p>
</li>
<li>
<p>
National Prosecuting Authority - Pablo Parenti
Coordination and Monitoring Unit of the Causes of Human Rights of the Attorney General&#8217;s Office, Research Tools (Excalibur)
</p>
</li>
<li>
<p>
Asociaci√≥n Nunca M√°s + Gabriel Acquistapace (Drupal)
</p>
</li>
<li>
<p>
No time contacted the Forensic Anthropology Team and Memoria Abierta, or the work of In√©s Caridi, Faculty of Sciences of the UBA, who made a mathematical model for Forensic Anthropology. Each of these projects have different types of quality, quantity and availability of public or private documents, but more or less similar purposes.
</p>
</li>
</ul></div>
<div class="paragraph"><p>What could serve mapa76?
Both journalists and investigators, prosecutors, witnesses, can use it to establish relationships between disappeared, places, dates and conditions of detention.
It will allow to follow the story of a person&#8217;s life and how was his course during his captivity and subsequent disappearance or release. In the case of not having information about a person, journalist, lawyer or prosecutor, can access the platform Mapa76 and "comb" a vast amount of documents for new information. This could allow identification of new witnesses, inquiring about test criteria, reaffirming that there was a systematic plan for the appropriation of children by armed forces, etc.</p></div>
<div class="paragraph"><p>How does Mapa76 work?
Mapa76 is now a developing prototype.
It has three basic modules:
(A) the automatic extraction of data-names, dates and places;
(B) consultation and treatment data in context, and
&#169; the display of the selected data on maps and timelines.
Using defined search patterns (which may be: personal names, organizations, places and dates), this software1- programmed in Ruby recognizes the words in the document that match the preset and stored in a database.</p></div>
<div class="paragraph"><p>This base can be refined by the researcher and each finding can be viewed in context. Then in a second step, the data extracted / purified / selected can be viewed on a map or geo referenced in a timeline, and they in turn provide contextual information regarding the consultation.</p></div>
<div class="paragraph"><p>To develop this platform we will work with layers in order to filter specific information regarding allegations, convictions, political-military group, a clandestine detention center, names, etc.</p></div>
<div class="paragraph"><p>One way to understand the behavior patterns of Mapa76.info would be: the user uploads a document. This is analyzed by the software of automatic data extraction. Then the user chooses a person and paragraphs in which choose the name mentioned, the relevant dates and places related to the time.</p></div>
<div class="paragraph"><p>Call for hackathon
We make a public announcement through the page Hacks/Hackers Buenos Aires <a href="http://meetupba.hackshackers.com">http://meetupba.hackshackers.com</a> At that time we had about 200 enrolled, there are currently about 540. We also called Human Rights associations and made a formal request for the hackathon in Tecnopolis, a vast exhibition organized by the national government dedicated to science in Argentina, which occurred place within the former Batallon 601, the base of military operations in the dictatorship government. The meeting was attended by about forty people including journalists, militant organizations, developers and designers. Other participants of hackathon were Junar makers of a platform for "streaming" data to organize the information automatically and export it to other websites. We received sponsorship from Mozilla Argentina and Tecn√≥polis.</p></div>
<div class="paragraph"><p>During the hackathon
We searched for isolated tasks, so that participants could move independently</p></div>
<div class="paragraph"><p>Programmers, Designers: Make an interface that combines timeline + map /
Designers
Incorporate the platform components: filter by organizations, agencies, for testimonials, etc..</p></div>
<div class="paragraph"><p>Developers:
Try other ways to extract data
‚óè names, addresses, dates
‚óè Organizations (currently done with regexes + a Bayesian filter for names).
‚óè misspelled names, etc.
‚óè alias
‚óè disambiguations
‚óè different ways of referring to the same person
‚óè "Jorge Julio Lopez"
‚óè "Julio Lopez"
‚óè "Lopez"
‚óè Work on the API data mapa76 exposing</p></div>
<div class="paragraph"><p>Journalists:
Find use cases:
‚óè Who was who?
‚óè Follow the story of a person. What happened?
‚óè Compare two life stories
‚óè Compare the story depending on version
‚óè Combing documents to try to tell a story based on documents
‚óè Incorporate other sources such as newspapers, databases, etc..</p></div>
<div class="paragraph"><p>Identify key words:
‚óè Kidnapping
‚óè Transfer
‚óè Survivor
‚óè Captivity
‚óè Identity Theft</p></div>
<div class="paragraph"><p>Sort documents and relationships between documents:
‚óè Testimony
‚óè Expertise
‚óè Case
‚óè Judgment
‚óè Fundamentals</p></div>
<div class="paragraph"><p>Future
Improve the charging interface
‚óè Smarter, faster
‚óè Make a prototype query interface
‚óè timelines (visualizations)
- Per person
- For detention center
- Per organization
‚óè Consultation
- Who was with who, where, etc..
‚óè Layers own info / private
Newly transcribed testimony</p></div>
<div class="paragraph"><p>Later hackathon problems</p></div>
<div class="paragraph"><p>Probably the main problem we had after the hackathon was that short-term objectives set were very demanding, very ambitious expectations, and the spread of ideas and the model of volunteer work hard to manage. All people involved and participating in the project are qualified with day jobs of high complexity, and could not move forward with the necessary regularity and systemization. In part, because the more involved people also participated in other activities planned by Hacks/Hackers Buenos Aires in 2011, the year we made nine events: four hackatones, four meetups, a web seminar and a lecture on a Free Software conference.</p></div>
<div class="paragraph"><p>What are we doing?</p></div>
<div class="paragraph"><p>‚Ä¢ Generate technical documentation of the prototype.
‚Ä¢ Define the functional scope of version 1.0, modular development plan, quantify the cost and size the equipment.
‚Ä¢ Develop a Project Plan for version 1.0 and do a benchmarking for development funds.
‚Ä¢ To survey the existence of functional modules to integrate into the platform Mapa76. For example, visualization of relationships between entities (people) to managing versions of a work product of the consultation on the system, etc.
‚Ä¢ Improve data loading interface to make it more efficient and faster in the searches.
‚Ä¢ Polish the query interface to improve the timelines per person per centerdetention.
‚Ä¢ Set up a database query to establish relationships between people. By example, who was with who, where the disappearance occurred, how was your hostage situation, when it happened, and so on.
‚Ä¢ Generate data layers of information for public and private consultation, to allow
store them "stories" at the user level, etc.</p></div>
<div class="paragraph"><p>Current Status</p></div>
<div class="paragraph"><p>At present, there is a group of four people directly involved in the project trying to get to prototype mapa76.info: Martin Sarsale (developer), Mark Vanetta (programmer), Andrew Snitcovsky (graphic designer), Mariana Berruezo (product concept) and Mariano Blejman (journalist). And may be 15 "satelite" collaborators.</p></div>
<div class="ulist"><ul>
<li>
<p>
Mapa76.info is under development through <a href="mailto:mapa76-dev@googlegroups.com">mapa76-dev@googlegroups.com</a> (Ruby, JQuery, MySQL) For developers who want to join the group, contact "Martin Sarsale" &lt;<a href="mailto:martin.sarsale@gmail.com">martin.sarsale@gmail.com</a>&gt;
</p>
</li>
<li>
<p>
Code <a href="https://github.com/mapa76/">https://github.com/mapa76/</a>
</p>
</li>
<li>
<p>
Project manager <a href="https://www.pivotaltracker.com/projects/344053">https://www.pivotaltracker.com/projects/344053</a>
</p>
</li>
<li>
<p>
In early March 2012 we will have a hackathon for visualizations timelines.
</p>
</li>
</ul></div>
<div class="paragraph"><p>Who we are
Mapa76.info is an initiative of the Buenos Aires chapter of Hacks / Hackers, an area of
meeting comprised of journalists, software programmers and designers who come together to help build the future of media. The organizers of Hacks / Hackers Buenos Aires is made up Mariano Blejman (P√°gina/12), Martin Sarsale (Sumavisos), Guillermo Movia (Mozilla Argentina), Cesar Miquel (EasyTech), Mariana Berruezo, Sorin Sergio and Ezequiel Clerici.</p></div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_3_case_studies">3. Case studies</h2>
<div class="sectionbody">
<div class="paragraph"><p>So you have the data&#8230; what next? Where do you start and where could you end?</p></div>
<div class="paragraph"><p>In this section, top data-journalists take us through useful tips and tricks for how they have produced their data-powered stories before diving deeper into what made them determined to work on a particular project, why they chose the methods they did, the barriers they encountered, lessons learned and the eventual impact the stories made&#8230;.</p></div>
<div class="paragraph"><p>More case studies</p></div>
<div class="paragraph"><p>MPs expenses scandal in the UK is an example of how a simple Freedom of Information (FOI) request can trigger a large scale investigation. An FOI request uncovered some interesting stories about how elected officials spend taxpayers' money.
Subsidies for the Bus Transportation System: Angelica Peralta Ramos , LA NACION (Argentina)</p></div>
<div class="paragraph"><p>case study 1</p></div>
<div class="paragraph"><p>You&#8217;ve poured hours into tidying up and working with your data and got your story. What do you do with the data now? In this chapter, we take a look at cases where journalists have decided to open up their datasets and publish them along with their stories: What role did the datasets play in the original story? What was the motivation for opening them up? What did others choose to do with the data?</p></div>
<div class="sect2">
<h3 id="_data_stories_by_martin_rosenbaum_bbc">Data Stories by Martin Rosenbaum (BBC)</h3>
<div class="paragraph"><p>Data journalism can sometimes give the impression that it is mainly about presentation of data ‚Äì such as visualisations which quickly and powerfully convey an understanding of an aspect of the figures, or interactive searchable databases which allow individuals to look up say their own local street or hospital. All this can be very valuable, but like other forms of journalism, data journalism should also be about stories. So what are the kinds of stories you can find in data? Based on my experience at the BBC, I have drawn up a list or <em>typology</em> of different kinds of data stories.</p></div>
<div class="paragraph"><p>I think it helps to bear this list below in mind, not only when you are analysing data, but also at the stage before that, when you are collecting it (whether looking for publicly available datasets or compiling freedom of information requests).</p></div>
<div class="paragraph"><p><strong>1. Measurement</strong></p></div>
<div class="paragraph"><p>The simplest story ‚Äì counting or totalling something.</p></div>
<div class="paragraph"><p><em>Local councils across the country spent a total of ¬£x billion on paper clips last year</em>.</p></div>
<div class="ulist"><ul>
<li>
<p>
but it&#8217;s often difficult to know if that&#8217;s a lot or a little. For that, you need context ‚Äì which can be
provided by:
</p>
</li>
</ul></div>
<div class="paragraph"><p><strong>2. Proportion</strong></p></div>
<div class="paragraph"><p><em>Last year local councils spent two-thirds of their stationery budget on paper clips</em></p></div>
<div class="ulist"><ul>
<li>
<p>
or
</p>
</li>
</ul></div>
<div class="paragraph"><p><strong>3. Internal comparison</strong></p></div>
<div class="paragraph"><p><em>Local councils spend more on paper clips than on providing meals-on-wheels for the elderly</em></p></div>
<div class="ulist"><ul>
<li>
<p>
or
</p>
</li>
</ul></div>
<div class="paragraph"><p><strong>4. External comparison</strong></p></div>
<div class="paragraph"><p><em>Council spending on paper clips last year was twice the nation&#8217;s overseas aid budget</em></p></div>
<div class="ulist"><ul>
<li>
<p>
or there are other ways of exploring the data in a contextual or comparative way:
</p>
</li>
</ul></div>
<div class="paragraph"><p><strong>5. Change over time</strong></p></div>
<div class="paragraph"><p><em>Council spending on paper clips has trebled in the past four years</em></p></div>
<div class="ulist"><ul>
<li>
<p>
or
</p>
</li>
</ul></div>
<div class="paragraph"><p><strong>6. <em>League tables</em></strong></p></div>
<div class="paragraph"><p>(these are often geographical or by institution, and you must make sure the basis for comparison is fair, eg taking into account the size of the local population)</p></div>
<div class="paragraph"><p><em>Borsetshire Council spends more on paper clips for each member of staff than any other local authority,at a rate four times the national average</em></p></div>
<div class="ulist"><ul>
<li>
<p>
or you can divide the data subjects into groups:
</p>
</li>
</ul></div>
<div class="paragraph"><p><strong>7. Analysis by categories</strong></p></div>
<div class="paragraph"><p><em>Councils run by the Purple Party spend 50% more on paper clips than those controlled by the Yellow Party</em></p></div>
<div class="ulist"><ul>
<li>
<p>
or you can relate factors numerically
</p>
</li>
</ul></div>
<div class="paragraph"><p><strong>8. Association</strong></p></div>
<div class="paragraph"><p><em>Councils run by politicians who have received donations from stationery companies spend more on paper clips, with spending increasing on average by ¬£100 for each pound donated</em></p></div>
<div class="paragraph"><p>(but, of course, always remember that correlation and causation are not the same thing).</p></div>
<div class="paragraph"><p>So if you&#8217;re investigating paper clip spending, are you also getting the following figures:
- total spending to provide context?
- geographical/historical/other breakdowns to provide comparative data?
- the additional data you need to ensure comparisons are fair, such as population size?
- other data which might provide interesting analysis to compare or relate the spending to?</p></div>
</div>
<div class="sect2">
<h3 id="_hospital_billing_by_steve_doig_walter_cronkite_school_of_journalism_and_mass_communication">Hospital billing by Steve Doig (Walter Cronkite School of Journalism and Mass Communication)</h3>
<div class="paragraph"><p>Investigative reporters at CaliforniaWatch had gotten tips from sources that a large chain of hospitals in California might be systematically gaming the federal Medicare program that pays for the costs of medical treatments of Americans aged 65 or older. The particular scam that was alleged is called "upcoding", which means reporting patients having more complicated conditions ‚Äì worth higher reimbursement ‚Äì than actually existed. But a key source was a union that was fighting with the hospital chain&#8217;s management, and the CaliforniaWatch team knew that independent verification was necessary for the story to have credibility.</p></div>
<div class="paragraph"><p>Luckily, California&#8217;s department of health has public records that give very detailed information about each case treated in all the state&#8217;s hospitals. The 128 variables include up to 25 diagnosis codes from the "International Statistical Classification of Diseases and Related Health Problems" manual (commonly known as ICD-9) published by the World Health Organization. While patients aren&#8217;t identified by name in the data, other variables tell the age of the patient, how the costs are paid and which hospital treated him or her. The reporters realized that with these records, they could see if the hospitals owned by the chain were reporting certain unusual conditions at significantly higher rates than were being seen at other hospitals.</p></div>
<div class="paragraph"><p>The data sets were large ‚Äì nearly 4 million records per year, and the reporters wanted to study six years worth of records in order to see how patterns changed over time. They ordered the data from the state agency; it arrived on CD-ROMs that were easily copied into a desktop computer. The reporter doing the actual data analysis used a system called SAS to work with the data. SAS is very powerful (allowing analysis of many millions of records) and is used by many government agencies, including the California health department, but it is expensive - the same kind of analysis could have been done using any of a variety of other database tools, such as Microsoft Access or the open-source MySQL#.</p></div>
<div class="paragraph"><p>With the data in hand and the programs written to study it, finding suspicious patterns was relatively simple. For example, one allegation was that the chain was reporting various degrees of malnutrition at much higher rates than were seen at other hospitals. Using SAS, the data analyst extracted frequency tables that showed the numbers of malnutrition cases being reported each year by each of California&#8217;s more than 300 acute care hospitals. The raw frequency tables then were imported into Microsoft Excel for closer inspection of the patterns for each hospital; Excel&#8217;s ability to sort, filter and calculate rates from the raw numbers made seeing the patterns easy.</p></div>
<div class="paragraph"><p>Particularly striking were reports of a condition called Kwashiorkor, a protein deficiency syndrome that almost exclusively is seen in starving infants in famine-afflicted developing countries. Yet the chain was reporting its hospitals were diagnosing Kwashiorkor among elderly Californians at rates as much as 70 times higher than the state average of all hospitals. Read the story.</p></div>
<div class="paragraph"><p>For other stories, the analysis used similar techniques to examine the reported rates of conditions like septicemia, encephalopathy, malignant hypertension and autonomic nerve disorder. Read the story. And another analysis looked at allegations that the chain was admitting from its emergency rooms into hospital care unusually high percentages of Medicare patients, whose source of payment for hospital care is more certain than is the case for many other emergency room patients. Read the story.</p></div>
<div class="paragraph"><p>To summarize, stories like these become possible when you use data to produce evidence to test independently allegations being made by sources who may have their own agendas. These stories also are a good example of the necessity for strong public records laws; the reason the government requires hospitals to report this data is so that these kinds of analyses can be done, whether by government, academics, investigators or even citizen journalists. The subject of these stories is important because it examines whether millions of dollars of public money is being spent properly.</p></div>
</div>
<div class="sect2">
<h3 id="_care_home_crisis_by_cynthia_o_8217_murchu_financial_times">Care home crisis by Cynthia O&#8217;Murchu (Financial Times)</h3>
<div class="paragraph"><p>A Financial Times investigation into the quality of care provided in care homes began with a simple question: is care provided by private providers better than that offered by the non-profit sector or the government?</p></div>
<div class="paragraph"><p>The analysis was timely, because the financial problems of Southern Cross, then the country&#8217;s largest care home operator, were coming to a head. The government had for decades promoted a privatization drive in the care sector and continued to tout the private sector for its astute business practices.</p></div>
<div class="paragraph"><p>Our inquiry began with analyzing data we obtained from the UK regulator in charge of inspecting care homes. The information was public, but it required a lot of persistence to get the data in a form that was usable.</p></div>
<div class="paragraph"><p>The data included ratings (now defunct) on individual homes' performance and a breakdown of whether they were private, government-owned or non-profit. The Care Quality Commission (CQC), up to June 2010, rated care homes on quality ranging from a verdict of "zero stars - poor" to "three stars ‚Äì excellent".</p></div>
<div class="paragraph"><p>The first step required extensive data cleaning, as the data provided by the Care Quality Commission for example contained categorizations that were not uniform. This was primarily done using Excel. We also determined ‚Äì through desk and phone research ‚Äì whether particular homes were owned by private-equity groups. Before the financial crisis, the care home sector was a magnet for private equity and property investors, but several ‚Äì such as Southern Cross ‚Äì had begun to face serious financial difficulties. We wanted to establish what effect, if any, private equity ownership had on quality of care.</p></div>
<div class="paragraph"><p>A relatively straight forward set of Excel calculations enabled us to establish that the non-profit and government-run homes on average performed significantly better than the private sector. Some private equity-owned care home groups performed well over average, and others well below average.</p></div>
<div class="paragraph"><p>Paired with on-the-ground reporting, case studies of neglect, an in-depth look at the failures in regulatory policies as well as other data on levels of pay, turnover rates etc, our analysis was able to paint a picture of the true state of elderly care.</p></div>
<div class="paragraph"><p>Some tips:</p></div>
<div class="ulist"><ul>
<li>
<p>
Make sure you keep notes on how you manipulate the original data.
</p>
</li>
<li>
<p>
Keep a copy of the original data and never change the original.
</p>
</li>
<li>
<p>
Check and double check the data. Do the analysis several times (if need be from scratch).
</p>
</li>
<li>
<p>
If you mention particular companies or individuals, give them a right to reply.
</p>
</li>
</ul></div>
</div>
<div class="sect2">
<h3 id="_a_9_month_investigation_into_european_structural_funds_by_cynthia_o_8217_murchu_financial_times">A 9 month investigation into European Structural Funds by Cynthia O&#8217;Murchu (Financial Times)</h3>
<div class="paragraph"><p>In 2010, the Financial Times and the Bureau of Investigative Journalism (BIJ) joined forces to investigate European Structural Funds. The intention was to review who the beneficiaries of European Structural Funds are and check whether the money was put to good use. At ‚Ç¨347bn over seven years Structural Funds is the second largest subsidy programme in the EU. The programme has existed for decades, but apart from broad, generalised overviews, there was little transparency about who the beneficiaries are. As part of a rule change in the current funding round, authorities are obliged to make public a list of beneficiaries, including project description and amount of EU and national funding received.</p></div>
<div class="paragraph"><p>The project was done with up to 12 journalists and one coder and took nine months. Data gathering alone took many months. The project resulted in five days of coverage in the Financial Times and the BIJ, a BBC radio documentary, and several TV documentaries.</p></div>
<div class="paragraph"><p>Before you tackle a project of this level of effort, you have to be certain that the findings are original, and that you will end up with good stories nobody else has.</p></div>
<div class="paragraph"><p>The process was broken up into a number of distinct steps:</p></div>
<div class="paragraph"><p><strong>1. Identify who keeps the data and how it is kept</strong></p></div>
<div class="paragraph"><p>The EU Commission&#8217;s Directorate General for the Regions have a portal to the websites of regional authorities that publish the data. We believed that the EU commission would have an overarching database of project data that we could either access directly, or which we could obtain through a Freedom of Information request. No such database exists to the level of detail we required. We quickly realised that many of the links the Commission provided were faulty and that most of the authorities published the data in PDF format, rather than analysis-friendly formats such as CSV or XML.</p></div>
<div class="paragraph"><p>A team of up to 12 people worked on identifying the latest data and collating the links into one large spreadsheet we used for collaboration. Since the data fields were not uniform (for example headers were in different languages, some data sets used different currencies, some included breakdowns of EU and National Funding) we needed to be as precise as possible in translating and describing the data fields available in each data set.</p></div>
<div class="paragraph"><p><strong>2. Download and prepare the data</strong></p></div>
<div class="paragraph"><p>The next step consisted of downloading all the spreadsheets, PDFs and, in some cases ,web scraping the original data.</p></div>
<div class="paragraph"><p>Each data set had to then be standardised. Our biggest task was extracting data out of PDFs, some hundreds of pages long. Much of this was done using UnPDF and ABBYY FineReader, which allow data to be extracted to formats such as CSV or Excel.</p></div>
<div class="paragraph"><p>It also involved checking and double checking that the PDF extraction tools had captured the data correctly. This was done using filtering, sorting and summing up totals (to ensure it corresponded with what was printed on the PDFs).</p></div>
<div class="paragraph"><p><strong>3. Create a database</strong></p></div>
<div class="paragraph"><p>The team&#8217;s coder set up a SQL database. Each of the files prepared were then used as a building block for the overall SQL database. A once a day process would upload all the individual data files into one large SQL database, which could be queried on the fly through its front end by using queries.</p></div>
<div class="paragraph"><p><strong>4. Double-checking and analysis</strong></p></div>
<div class="paragraph"><p>The team analysed the data in two main ways:</p></div>
<div class="olist loweralpha"><ol class="loweralpha">
<li>
<p>
via the database front end. This entailed typing particular keywords of interest (e.g. "tobacco", "hotel", "company A" in to the search engine. With help of the google translate function, which was plugged into the search functionality of our database, those keywords would be translated into 21 languages and would return appropriate results. These could be downloaded and reporters could do further research on the individual projects of interest.
</p>
</li>
<li>
<p>
macro-analysis using the whole database. Occasionally, we would download a full data set, which could then be analysed for example using keywords, or aggregating data by country, region, type of expenditure, number of projects by beneficiary etc.
</p>
</li>
</ol></div>
<div class="paragraph"><p>Our story lines were informed by both these analyses, but also through on the ground and desk research.</p></div>
<div class="paragraph"><p>Double-checking the intergrity of the data (by aggregating and checking against what authorities said had been allocated) took a substantial amount of time. One of the main problems was that authorities would for the most part only divulge the amount of "EU and national funding". Under EU rules, each programme is allowed to fund a certain percentage of the total cost using EU funding. The level of EU funding is determined, at programme level, by the so-called co-financing rate. Each programme (e.g. regional competitiveness) is made up of numerous projects. At the project levels, technically one project could receive 100 per cent EU funding, and another none at all, as long as grouped together, the amount of EU funding at the programme level is not more than the approved co-financing rate.</p></div>
<div class="paragraph"><p>This meant that we needed to check each EU amount of funding we cited in our stories with the beneficiary company in question.</p></div>
<div class="paragraph"><p>Read the story:</p></div>
<div class="paragraph"><p>Financial times coverage
The Bureau of Investigative Journalism Coverage</p></div>
</div>
<div class="sect2">
<h3 id="_the_tell_all_telephone_by_sascha_venohr_zeit_online">The tell-all telephone by Sascha Venohr (Zeit Online)</h3>
<div class="paragraph"><p>Most people&#8217;s understanding of what can actually be done with the data provided by our mobile phones is theoretical; there were few real-world examples. That is why Malte Spitz from the German Green party decided to publish his own data. To access the information, he had to file a suit against telecommunications giant Deutsche Telekom. The data is the basis for ZEIT ONLINE&#8217;s accompanying interactive map, were contained in a massive Excel document. Each of the 35,831 rows of the spreadsheet represent an instance when Spitz&#8217;s mobile phone transferred information over a half-year period.</p></div>
<div class="paragraph"><p>Seen individually, the pieces of data are mostly harmless. But taken together, they provide what investigators call a profile ‚Äì a clear picture of a person&#8217;s habits and preferences, and indeed, of his or her life. This profile reveals when Spitz walked down the street, when he took a train, when he was in an plane. It shows that he mainly works in Berlin an which cities he visited. It shows when he was awake and when he slept.</p></div>
<div class="paragraph"><p>To illustrate just how much detail from someone&#8217;s life can be mined from this stored data, ZEIT ONLINE has "augmented" Spitz&#8217;s information with records that anyone can access: the politician&#8217;s tweets and blog entries were added to the information on his movements. It is the kind of process that any good investigator would likely use to profile a person under observation. ZEIT ONLINE decided to keep one part of Spitz&#8217;s data record private, namely, whom he called and who called him. That kind of information would not only infringe on the privacy of many other people in his life, it would also, even if the numbers were encrypted, reveal much too much about Spitz (but government agents in the real world would have access to this information).</p></div>
<div class="paragraph"><p>We were very happy to work with Lorenz Matzat and Michael Kreil from Open Data City to find a solution how to understand and extract the geolocation from the dataset. Every connection of Spitz' mobile phone has to be triangulated to the positions of the antenna pole. Every pole has three antennas, each covering 120¬∞. The two programmers found out, that the saved position indicated the direction from the pole Spitz' mobile phone was connecting.</p></div>
<div class="paragraph"><p>Matching this with the positions of the poles-map of the state-controlled agency gave us the possibility to get his position for each of the 260,640 minutes during the 181 days and put it via API on a Google Map. Together with the in-house graphics and design team we created a great interface to navigate: By pushing the play button, you will set off on a trip through Malte Spitz&#8217;s life.</p></div>
<div class="paragraph"><p>After a very successful launch of the Project in Germany, we recognized a great traffic from outside Germany and decided to translate the app in an English version. After earning the German Grimme Online Award, the project was honoured with an ONA award in September 2011, the first time for a German news website.</p></div>
<div class="paragraph"><p>See the data
Read the story</p></div>
</div>
<div class="sect2">
<h3 id="_citizen_data_reporters_by_amanda_rossi_friends_of_janu_ria">Citizen data reporters by Amanda Rossi (Friends of Janu√°ria)</h3>
<div class="paragraph"><p>Coordinator of the Friends of Janu√°ria citizen media project, Brasil</p></div>
<div class="paragraph"><p>Large newsrooms are not the only ones that can work on data-powered stories. Even those that may be considered as citizen reporters have the opportunity to use data in their stories. The same skills that are useful for data journalists can also help ordinary citizens access data about their local reality, and turn them into stories.</p></div>
<div class="paragraph"><p>This was the primary assumption that led to the citizen media outreach project Friends of Janu√°ria, which began in September of 2011. The project received a grant from Rising Voices, the outreach arm of Global Voices Online, and received additional support from the organization Article 19. Between September and October 2011, a group of young residents of a small town in Brazil were trained in basic journalism techniques and budget monitoring. They also learned how to make Freedom of Information requests and access publicly available information from official databases on the Internet.</p></div>
<div class="paragraph"><p>The pilot project took place in Janu√°ria, a small town of approximately 65,000 residents located in north of the state of Minas Gerais, which is one of the poorest regions of Brazil. The town is also renowned for the failure of its local politicians. In just six years (2004-2010), Janu√°ria had seven different mayors. Almost all of them were removed from office due to wrongdoing in their public administrations, including charges of corruption.</p></div>
<div class="paragraph"><p>Small towns like Janu√°ria often fail to attract attention from the Brazilian media, which tends to focus on larger cities and state capitals. However, there is an opportunity for residents of small towns to become a potential ally in the monitoring of the public administration because they know the daily challenges facing their local communities better than anyone. With the Internet as another important ally, residents can now better access information such as budget information and other data about their towns.</p></div>
<div class="paragraph"><p>After taking part in twelve workshops, some of the new citizen reporters from Janu√°ria began to demonstrate how this concept of accessing publicly available data in small towns can be put into practice. For example, Soraia Amorim, a 22 year-old citizen journalist, wrote a story about the number of doctors that are on the city payroll according to Federal Government data. However, she found that the official number did not match with the town&#8217;s reality. To write this story, Soraia had access to health data about her town, which is available online at the website of the Sistema √önico de Sa√∫de (SUS - Unique Health System), a federal program that provides free medical assistance to the Brazilian population. According to SUS data, Janu√°ria should have 71 doctors in various health specialities. See the table below:</p></div>
<div class="paragraph"><p>The number of doctors indicated by SUS data did not match what she knew about the local reality. Taking into account that the town&#8217;s residents were always complaining about the lack of doctors and that even some patients had to travel to neighboring towns to see a doctor. Later, Soraia interviewed a woman that had recently been in a motorcycle accident and could not find medical assistance at Janu√°ria&#8217;s hospital because no doctor was available. She also talked to the town&#8217;s Health Secretary, who admitted that there were less doctors in town than the number published by SUS.</p></div>
<div class="paragraph"><p>These initial findings raise many questions about reasons for this difference between the official information published online and the town&#8217;s reality. One of them is that the federal data may be wrong, which would mean that there is an important lack of health information in Brazil. Another possibility may be that the town is incorrectly reporting the information to SUS. Both of these possibilities should lead to a deeper investigation to find the definitive answer. However, Soraia&#8217;s story is an important part of this chain because it highlights an inconsistency and may also encourage others to look more closely about this issue. "Access to data is a tool to provide information so that citizens can do something and fight for their rights," says Soraia.</p></div>
<div class="paragraph"><p>This was an important first step, and only after a few short trainings, Soraia was better equipped to search for data, make a Freedom of Information request, and write a story. "I used to live in the countryside, and finished high school with a lot of difficulty. When people asked me what I wanted to do with my life, I always told them that I wanted to be a journalist. But I imagined that it was almost impossible due to the world I lived in." After taking part in the Friends of Janu√°ria training. Soraia believes that access to information and data is an important tool to change the reality of her town. "I feel capable of changing my town, my country, the world. And I will pursue the access to information in my daily life," adds Soraia. See below for an excerpt from her story:</p></div>
<div class="paragraph"><p>"According to the Health Handbook of SUS, Janu√°ria has approximately one doctor per one thousand residents (71 doctors). (&#8230;) Despite this number, many people are not assisted. That is what happened with Geiciane (who had a motorcycle accident and could not be assisted at the town&#8217;s hospital). (&#8230;) The town&#8217;s Health Secretary, Andr√© Rocha, does not confirm the amount of doctors mentioned by SUS".</p></div>
<div class="paragraph"><p>Another citizen journalist from the project is Alysson Monti√©riton, 20 years-old, who also used data for his final article. It was during the project&#8217;s first class, when the citizen reporters walked around the city to look for subjects that could become stories, that Alysson decided to write about a broken traffic light located in a very important intersection, which had remained broken since the beginning of the year. After learning how to look for data on the Internet, Alysson searched for the number of vehicles that exists in town and the amount of taxes paid by those who own vehicles. See below an extract of his report:</p></div>
<div class="paragraph"><p>"The situation in Janu√°ria gets worse because of the high number of vehicles in town. According to IBGE (the most important statistics research institute in Brazil), Janu√°ria had 13,771 vehicles (among which 7,979 were motorcycles) in 2010. (&#8230;) The town&#8217;s residents believe that the delay in fixing the traffic light is not a result of lack of resources. According to the Treasury Secretary of Minas Gerais state, the town received 470 thousand reais in vehicle taxes in 2010".</p></div>
<div class="paragraph"><p>By having access to data, Alysson was able to show that Janu√°ria has many vehicles (almost one for every five residents) and that a broken traffic light could put a lot of people in danger. Besides, he was able to tell his audience the amount of funds received by the town from taxes paid by vehicle owners and, based on that, to question whether this money would not be enough to repair the traffic light to provide safe conditions to drivers and pedestrians. See the graph below:</p></div>
<div class="paragraph"><p>"At the IBGE site , I clicked on "Cities", looked for Janu√°ria, and saw all the information that is available about the town. Then, I selected the data on the number of vehicles that exists in Janu√°ria. Besides, I looked for the amount of money that Janu√°ria receives from IPVA (vehicle tax). The town receives a good amount of funds and should provide a better maintenance for equipment, like traffic lights and so many others public equipment that citizens demand on a daily basis", explains Alysson.</p></div>
<div class="paragraph"><p>Although these two reports, written by Soraia and Alysson, are very simple, they show that data can be used by citizen reporters. You don&#8217;t need to be in a large newsroom, with the need for specialists, to use data in your reports. After twelve workshops, Soraia and Alysson, with no journalism background, were able to work on data powered stories and write interesting pieces about their local reality. In addition, their articles show that data itself can be useful even on a small scale. In other words, that there is also valuable information in small datasets and tables - not only in huge spreadsheets.</p></div>
<div class="paragraph"><p>"I was able to learn how to look for data about my town on the Internet. I invite my readers to look for data about your own town on the Internet. I searched about my town and wrote a piece about it", says Alysson. If you like his example, accept his challenging invitation.</p></div>
</div>
<div class="sect2">
<h3 id="_which_car_model_mot_failure_rates_by_martin_rosenbaum_bbc">Which car model? MOT failure rates by Martin Rosenbaum (BBC)</h3>
<div class="paragraph"><p>In January 2010 the BBC obtained data about the MOT pass and fail rates for different makes and models of cars. This is the test which assesses whether a car is safe and roadworthy ‚Äì any car over three years old has to have an MOT test annually.</p></div>
<div class="paragraph"><p>We obtained the data under freedom of information following an 18-month battle with VOSA, the Department for Transport agency which oversees the MOT system. VOSA turned down our FOI request for these figures on the grounds that it would breach commercial confidentiality. It argued that it could be <em>commercially damaging</em> to vehicle manufacturers with high failure rates. However, we then appealed to the Information Commissioner, who ruled that disclosure of the information would be in the public interest. VOSA then released the data, 18 months after we asked for it.</p></div>
<div class="paragraph"><p>We analysed the figures, focusing on the most popular models and comparing cars of the same age. This showed wide discrepancies. For example, among three year old cars, 28% of Renault M√©ganes failed their MOT in contrast to only 11% of Toyota Corollas. The figures were reported on television, radio and online.</p></div>
<div class="paragraph"><p>The data was given to us in the form of a 1,200 page PDF document, which we then had to convert into a spreadsheet to do the analysis. As well as reporting our conclusions, we published this Excel spreadsheet (with over 14,000 lines of data) on the BBC News website along with our story. This gave everyone else access to the data in a usable form.</p></div>
<div class="paragraph"><p>The result was that others then used this data for their own analyses which we did not have time to do in the rush to get the story out quickly or in some cases would have stretched our technical capabilities at the time. This included examining the failure rates for cars of other ages, comparing the records of manufacturers rather than individual models, and creating searchable databases for looking up the results of individuals models. We added links to these sites to our online news story, so our readers could get the benefit of this work.</p></div>
<div class="paragraph"><p>This illustrated some advantages of releasing the raw data to accompany a data-driven story. There may be exceptions (for example, if you are planning to use the data for other follow-up stories later and want to keep it to yourself meanwhile), but on the whole publishing the data has several important benefits.</p></div>
<div class="olist arabic"><ol class="arabic">
<li>
<p>
Your job is to find things out and tell people about them. If you&#8217;ve gone to the trouble of obtaining all the data, it&#8217;s part of your job to pass it on.
</p>
</li>
<li>
<p>
Other people may spot points of significant interest which you&#8217;ve missed, or simply details that matter to them even if they weren&#8217;t important enough to feature in your story.
</p>
</li>
<li>
<p>
Others can build on your work with further, more detailed analysis of the data, or different techniques for presenting or visualising the figures, using their own ideas or technical skills which may probe the data productively in alternative ways.
</p>
</li>
<li>
<p>
It&#8217;s part of incorporating accountability and transparency into the journalistic process. Others can understand your methods and check your working if they want to.
</p>
</li>
</ol></div>
</div>
<div class="sect2">
<h3 id="_finnish_parliamentary_elections_and_campaign_funding_by_esa_m_kinen_helsingin_sanomat">Finnish Parliamentary elections and campaign funding by Esa M√§kinen (Helsingin Sanomat)</h3>
<div class="paragraph"><p>In the spring of 2012, there are many ongoing trials about the campaign election funding of Finnish general elections of 2007.</p></div>
<div class="paragraph"><p>After the elections in 2007, the press found out that the laws on publicising campaign funding had no effect on politicians. Basically, campaign funding has been used to buy favours from politicians and they had not declared their funding as mandated by Finnish law.</p></div>
<div class="paragraph"><p>After these incidents, the laws became stricter. After the general election in March 2011, Helsingin Sanomat decided that we should dig very carefully all the available data on campaign funding. The new law stipulates, that election funding must be declared and only donations below 1500 euros may be anonymous.</p></div>
<div class="paragraph"><p>First step was to get the data and find interested developers.</p></div>
<div class="paragraph"><p>Helsingin Sanomat has organized HS Open hackathons since march 2011. We invite Finnish coders, journalists and graphic designers to the basement of Sanoma building. Participants are divided into groups of three, and they are encouraged to develop applications and visualizations. We have had about 60 participants in each of our three events so far.</p></div>
<div class="paragraph"><p>We decided that campaign funding data should be the focus of HS Open #2, May 2011.</p></div>
<div class="paragraph"><p>The National Audit Office of Finland is the authority that keeps records of campaign funding. That was the easy part. Chief information Officer, Jaakko Hamunen, built a website that provides real time access to their campaign funding database. The Audit Office made this in just two months after our request.</p></div>
<div class="paragraph"><p>Vaalirahoitus.fi site will provide the press and public information on campaign funding on every elections from now on.</p></div>
<div class="paragraph"><p>Second step was to get ideas: what should we do with the data?</p></div>
<div class="paragraph"><p>The participants of HS Open 2 came up with twenty different prototypes about what to do with the data. You can find all the prototypes here (text in Finnish).</p></div>
<div class="paragraph"><p>Researcher of bioinformatics, Janne Peltola, noted that campaign funding data looked like gene data they research in terms of containing many interdependencies. Moreover, they have an open source tool to map out these interdependencies. It&#8217;s called Cytoscape. So we ran the data through Cytoscape, and had a very interesting prototype.</p></div>
<div class="paragraph"><p>Third step was to implement the idea on paper and on our site, HS.fi.</p></div>
<div class="paragraph"><p>The law on campaign funding states, that elected members of parliament must declare their funding two months after the elections. In practise, this meant that we got the real data in mid-June. In HS Open, we had data only from the MPs who had filed their report before the deadline.</p></div>
<div class="paragraph"><p>There was also a problem with the data format. National Audit Office provided the data as two CSV files. First contained the total budget of campaigns, the other listed all the donors. We had to combine these two; we made a file that contained three columns: donor, receiver and amount. If the politicians had used their own money, in our data format looked like Politician A donated x euros to Politician A. Counter-intuitive, but it worked for Cytoscape.</p></div>
<div class="paragraph"><p>When the data was cleaned and reformatted, we just ran it through Cytoscape. Then our graphics department made a page of it.</p></div>
<div class="paragraph"><p>Second part was to make a visualization for our web page. You can find the result here.</p></div>
<div class="paragraph"><p>The idea of this was not network analysis. We wanted to show people in easy format how much campaign funding there is and who gives it. The first view shows the distribution of funding between MPs. When you click on one MP, you get the breakdown on his or her funding. You can also vote on whether this particular donor is good or not. The visualization was made by Juha Rouvinen and Jukka Kokko, from ad agency Satumaa.</p></div>
<div class="paragraph"><p>The web version of campaign funding visualization uses the same data as the network analysis.</p></div>
<div class="paragraph"><p>Fourth step was to publish the data.</p></div>
<div class="paragraph"><p>Of course, the National Audit Office already publishes the data, so there was no need to republish. But, as we had a better data structure, we decided to publish it. We give out our data with Creative Commons licence. And there have been several visualizations made by independent developers that have used our data subsequently. We have also published some of these.</p></div>
<div class="paragraph"><p>The tools we used for the project were: Excel and Google Refine for data cleaning and analysis. Cytoscape for network analysis. Illustrator and Flash for the visualizations. The Flash should have been HTML5, but we ran out of time.</p></div>
<div class="paragraph"><p>What did we learn? Perhaps the most important thing was, that data structures can be very hard. If the original data is not in suitable format, recalculating and converting it will take a lot of time.</p></div>
</div>
<div class="sect2">
<h3 id="_illinois_school_report_cards_by_brian_boyer_chicago_tribune">Illinois school report cards by Brian Boyer (Chicago Tribune)</h3>
<div class="paragraph"><p><a href="http://schools.chicagotribune.com/">http://schools.chicagotribune.com/</a>
2011 Illinois school report cards</p></div>
<div class="paragraph"><p>Each year, the Illinois State Board of Education releases school "report cards"&#8201;&#8212;&#8201;data on the demographics and performance of all the public schools Illinois. It&#8217;s a massive dataset, this year&#8217;s drop was ~9,500 <strong>columns</strong> wide.</p></div>
<div class="paragraph"><p>The problem with that much data is choosing what to present. (As with any software project, the hard part is not <strong>building</strong> the software, but building the <strong>right</strong> software.)</p></div>
<div class="paragraph"><p>We worked with the reporters and editor from the education team to choose the interesting data. (There&#8217;s a lot of data out there that seems interesting but which a reporter will tell you is actually flawed or misleading.)</p></div>
<div class="paragraph"><p>We also surveyed and interviewed folks with school-age kids in our newsroom. We did this because of an empathy gap&#8201;&#8212;&#8201;nobody on the news apps team has school-age kids. Along the way, we learned much about our users and much about the usability (or lack thereof!) of the previous version of our schools site.</p></div>
<div class="paragraph"><p>We aimed to design for a couple specific users and use cases: 1) parents with a child in school who want to know how their school measures up, and 2) parents who&#8217;re trying to sort out where to live, since school quality often has a major impact on that decision.</p></div>
<div class="paragraph"><p>The first time around, the schools site was about a six week, two developer project. Our 2011 update was a four-week, two developer project. (There were actually three people actively working on the recent project, but none were full-time, so it adds up to about two.)</p></div>
<div class="paragraph"><p>A key piece of this project was information design. Although we present far less data than is available, it&#8217;s still a <strong>lot</strong> of data, and making it digestible was a challenge. Luckily, we got to borrow someone from our graphics desk&#8201;&#8212;&#8201;a designer who specializes in presenting complicated information. He taught us much about chart design and, in general, guided us to a presentation that is readable, but does not underestimate the reader&#8217;s ability or desire to understand the numbers.</p></div>
<div class="paragraph"><p>The site was built in Python and Django. The data is housed in MongoDB&#8201;&#8212;&#8201;the schools data is heterogeneous and hierarchical, making it a poor fit for a relational database. (Otherwise we probably would have used PostgreSQL.)</p></div>
<div class="paragraph"><p>We experimented for the first time with Twitter&#8217;s Bootstrap user interface framework on this project, and were happy with the results. The charts are drawn with Flot.</p></div>
<div class="paragraph"><p>The app is also home to the many stories about school performance that we&#8217;ve written. It acts as sort of a portal in that way&#8201;&#8212;&#8201;when there&#8217;s a new school performance story, we put it at the top of the app, alongside lists of schools relevant to the story. (And when a new story hits, readers of chicagotribune.com are directed to the app, not the story.)</p></div>
<div class="paragraph"><p>Early reports are that readers love the schools app. The feedback we&#8217;ve received has been largely positive (or at least constructive!), and page views are through the roof. As a bonus, this data will remain interesting for a full year, so although we expect the hits to tail off as the schools stories fade from the homepage, our past experience is that readers have sought out this application year-round.</p></div>
<div class="paragraph"><p>A few key ideas we took away from this project are:</p></div>
<div class="olist arabic"><ol class="arabic">
<li>
<p>
The graphics desk is your friend. They&#8217;re good at making complex information digestible.
</p>
</li>
<li>
<p>
Ask the newsroom for help. This is the second project for which we&#8217;ve conducted a newsroom-wide survey and interviews, and it&#8217;s a great way to get the opinion of thoughtful people who, like our audience, are diverse in background and generally uncomfortable with computers. :)
</p>
</li>
<li>
<p>
Show your work! Much of our feedback has been requests for the data that the application. We&#8217;ve made a lot of the data publicly available via an API, and we will shortly release the stuff that we didn&#8217;t think to include initially.
</p>
</li>
</ol></div>
</div>
<div class="sect2">
<h3 id="_electoral_hack_in_real_time_by_hacks_hackers_buenos_aires">Electoral Hack in real-time by Hacks/Hackers Buenos Aires</h3>
<div class="paragraph"><p>With input from Andy Tow, Sergio Sorin, Mariana Berruezo, Martin Sarsale and Mariano Blejman</p></div>
<div class="paragraph"><p>Electoral Hack <a href="http://elecciones.hhba.info">http://elecciones.hhba.info</a> is a Web environment of political analysis that illustrates real-time data from the provisional ballot results of the elections of October 23, 2011, in Argentina and provides several levels of analysis. The system also features a selection of information from previous elections and socio-demographic statistics from across the country. Electoral Hack was an initiative of Hacks/Hackers Buenos Aires with the political analyst Andy Tow, and was the result of joint volunteer work by members of the local chapter of Hacks/Hackers (journalists, programmers, designers, analysts, political scientists, etc.). Thematic maps of the primary elections of 2011and the general elections of 2007, as well as sociodemographic variables, are also available. Information was updated in real time with information from the provisional ballot count of the national elections of 2011 in Argentina and gave summaries of the presidential and vice-presidential elections as well as the renewal of Congress.</p></div>
<div class="paragraph"><p>What data did we use?
All information posted came from official sources. The National Electoral Bureau provided access to data of the provisional count by Indra; the Department of the Interior provided information about elected posts and candidates from different political parties; the university project yoquierosaber.org provided biographical information and the policy platforms of each presidential ticket; while socio-demographic information came from the 2001 National Census of Population and Housing (INDEC), except for the total population which corresponded to the 2010 Census (INDEC), and from the Ministry of Health of the Nation.</p></div>
<div class="paragraph"><p>How was it developed?
The application was generated during the 2011 Election Hackathon by Hacks/Hackers Buenos Aires the day before the election on October 23, 2011.  The hackathon was a day of work and programming with collaborations from 30 volunteer specialists. Electoral Hack was developed as an open platform for anyone interested in introducing improvements over time.  As technological tools we utilized developments that integrated Google Fusion Tables, maps, and vector graphics libraries, etc.</p></div>
<div class="paragraph"><p>We worked on the construction of polygons for displaying geographic mapping and electoral demographics. Combining polygons in GIS software and geometries from public tables in Google Fusion Tables we generated tables with keys corresponding to the electoral database of the Ministry of Interior—Indra and sociodemographic data from INDEC, creating from this data visualizations in Google Maps.</p></div>
<div class="paragraph"><p>Using the GoogleMaps API we put several thematic maps online representing the spatial distribution of voting with different tones of color, where the gradation of intensity expressed different intervals of the percentage of votes for the various presidential tickets in different administrative departments and polling stations, with particular emphasis on major urban centers: the City of Buenos Aires, the 24 districts of Greater Buenos Aires, the City of Cordoba, and Rosario.</p></div>
<div class="paragraph"><p>We used the same technique to generate thematic maps of previous elections, namely the presidential primaries of 2011 and the election of 2007, as well as of the distribution of sociodemographic data, such as for poverty, child mortality, and living conditions, allowing for analysis and comparison. We also put a display online showing the spatial distribution of the differences in percentage of votes obtained by each ticket in the general election of October compared to the August primary election.</p></div>
<div class="paragraph"><p>Later, using partial data from the provisional ballot counts, we created an animated map depicting the anatomy of the count, in which the progress of the vote count can be seen in accelerated time from the closing of the local polls until the following morning.</p></div>
<div class="paragraph"><p>Pros:</p></div>
<div class="ulist"><ul>
<li>
<p>
We set out to find data and articulate it and we were able to do that.  At hand we had the Unicef’s database of child sociodemographics, <a href="http://infoargentina.unicef.org.ar/">http://infoargentina.unicef.org.ar/</a>, as well as the database of candidates created by the group yoquierosaber.org of Torcuato Di Tella University.  During the hackathon we gathered a large volume of additional data that we did not end up including.
</p>
</li>
<li>
<p>
It was clear that the journalistic and programming work was enriched by scholarship. Without the contribution of Andy Tow and Hilario Moreno Campos, the project would have been impossible to pull off.
</p>
</li>
</ul></div>
<div class="paragraph"><p>Cons:</p></div>
<div class="ulist"><ul>
<li>
<p>
We ran into several problems: that the sociodemographic data we could use (indices of basic needs unmet, population, housing, etc.) were not up-to-date (most were from the 2001 census), and that their systematization was not publicly available or did not go to the local level (local average GDP, main economic activity, education level, number of schools, doctors per capita, and a long list of possibilities that would been great to have).
</p>
</li>
<li>
<p>
On the other hand, the system was intended as a tool, one that others could use to recombine and display whatever data they wanted, so that the press could insert into their sites the information that interested them.  But it remained for future development.
</p>
</li>
<li>
<p>
As participation was voluntary and collaborative, and as we had such a short time to plan, there was the practical impossibility of finishing what he had imagined as the final development. But we showed we were on the right track and we were able to develop a massive project.
</p>
</li>
<li>
<p>
For the same reason, all the collaborative work of 30 people ended up condensed into a single programmer when the data offered by the government began to appear, and we ran into some problems regarding the importation of date in real time that were solved within hours.
</p>
</li>
</ul></div>
<div class="paragraph"><p>Implications
Electoral Hack’s platform had a big impact at the media, with television coverage, radio, graphics and online sites; its maps were used by several media platforms during the elections and in subsequent days. As the days went by, the maps and visualizations were updated, increasing traffic even more. On Election Day, the site created that very day received about 20 thousand unique visitors and its maps were reproduced by the newspaper Página/12 on its cover page for two consecutive days as well as in articles in La Nacion; some of its maps appeared in the print edition of the newspaper Clarín. It was the first time that an interactive display of real-time maps had been used in the history of Argentine journalism. In the central maps one could clearly see the overwhelming victory of Cristina Fernandez de Kirchner by 54 percent of the vote, broken up by color saturation. It also served to help understand very specific individual cases where local candidates swept the provincial votes.</p></div>
<div class="paragraph"><p>What is Hacks/Hackers Buenos Aires?
In April 2011, a local chapter of Hacks / Hackers opened in Buenos Aires, a meeting space comprising journalists, software developers, and IT professionals coming together to help build the future of media. The organizers of Hacks / Hackers Buenos Aires are Mariano Blejman (Página/12), Martin Sarsale (Sumavisos), Guillermo Movia (Mozilla Argentina), Cesar Miquel (EasyTech), Mariana Berruezo, Sorin Sergio Clerici, and Ezekiel. In 2011 there were four meetups, four hackathons, a conference, and a webinar. We currently have a base of 540 people, including journalists, programmers, designers, and entrepreneurs.</p></div>
<div class="paragraph"><p>For this project, the Buenos Aires Chapter of Hacks/Hackers joined with the blogger and political analyst Andy Tow Hilario and political scientist Moreno-Field – of the PR consulting firm Dicen – to develop  with journalists, social scientists, programmers and graphic designers an innovative information system. Those who worked on the  idea of Sergio Sorin’s included: Andy Tow - Andres Snitcofsky - Catalina Marino - Cesar Miquel - Damian Janowski – Ezequiel Clerici - Federico Ricciardi - Fernanda Roux - Florence Coelho - Guido Labonia - Guillermo Movia - Hilario Moreno Campos - Juan Pablo Renzi - Kevin Hanna - Lucas Aznar - Luis E. Guardiola - Mariana Berruezo - Mariano Mancuso - Martin Sarsale – Momi Peralta Ramos - Nicholas Cisco - Oscar Guindzberg - Sebastian Alvarez - Sebastian Melendez - Soledad Gherardi - Tania Wassaf - Valeria Bula and Mariano Blejman.</p></div>
<div class="paragraph"><p>Twitter @ HacksHackersBA
Mail <a href="mailto:ba@hackshackers.com">ba@hackshackers.com</a>
Web <a href="http://meetupba.hackshackers.com">http://meetupba.hackshackers.com</a>
Blog <a href="http://www.hackshackers.com">http://www.hackshackers.com</a></p></div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_4_finding_data">4. Finding data</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_where_to_look">Where to look?</h3>
<div class="paragraph"><p>‚óè   Overview: An overview of open data sources, what they contain, how to find them, how to search them, examples of open data being used by journalists
‚óã   Is data copyrightable? "In the United States, data will be protected by copyright only if they express creativity. Some databases will satisfy this condition, such as a database containing poetry or a wiki containing prose. Many databases, however, contain factual information that may have taken a great deal of effort to gather, such as the results of a series of complicated and creative experiments." <a href="http://sciencecommons.org/old/databases/">http://sciencecommons.org/old/databases/</a>
‚óã         What is Open Data?
                     ‚ñ†<a href="http://en.wikipedia.org/wiki/Open_data">http://en.wikipedia.org/wiki/Open_data</a>, <a href="http://opendefinition.org/">http://opendefinition.org/</a>
                     ‚ñ†Data published using Creative Commons tools and licenses: <a href="http://wiki.creativecommons.org/Data">http://wiki.creativecommons.org/Data</a>
                     ‚ñ†Other open licenses for data: <a href="http://opendefinition.org/licenses/">http://opendefinition.org/licenses/</a>
                     ‚ñ†National and local governments have begun publishing data on portals like <a href="http://www.data.gov/">http://www.data.gov/</a> and <a href="http://data.cityofchicago.org/">http://data.cityofchicago.org/</a>
                     ‚ñ†Commercial re-use clauses
‚óè   Authors: Jonathan Gray (Open Knowledge Foundation), Brian Boyer (Chicago Tribune), Jane Park (Creative Commons), John Keefe (WNYC), Chrys Wu (Hacks/Hackers),
‚óè   Editor: Lucy Chambers, Friedrich Lindenberg
‚óè         Length: 1-3 pages (with links and examples)</p></div>
<div class="paragraph"><p>Introduction</p></div>
<div class="paragraph"><p>by Lucy Chambers
In this chapter, we aim to show you how to get your hands on the best data available to further your investigation. Sometimes you don&#8217;t have to look far, the data may already be out there: it is a matter of knowing where to look. Other times, you may need to find the right people to ask. In particular, this chapter deals with:
‚óè         Tips and tricks for where to look for data: before launching into FOI requests or embarking down a technical route such as scraping.
‚óè         Search tips: How to make Google give you want you want, in the format you want it.
‚óè   Communities for data: useful forums and mailing lists to ask experts directly
‚óè         Data houses: data catalogues, government portals, accessing research data and social data sites
Finally, we&#8217;ll have a quick look at the legal implications of re-using data from web databases ‚Äì while you may decide to break copyright to get your story out, it is still useful to know the boundaries in current law.</p></div>
<div class="paragraph"><p>A Journalist&#8217;s Guide to Finding Data&#8201;&#8212;&#8201;Open or Otherwise!</p></div>
<div class="paragraph"><p>I need to find data for my story. Where should I look?</p></div>
<div class="paragraph"><p><strong>1. Google for 5 minutes.</strong></p></div>
<div class="paragraph"><p>Often enough, databases on the web are included in Google ‚Äì whether the publisher intended or not. When searching for data, make sure that you include both search terms relating to the content of the data you&#8217;re trying to find as well as some information on the format or source that you would expect it to be in.
‚óè   A handy feature of google is that it allows you to limit the file types of the returned results. For example, you can look only for spreadsheets (filetype:XLS filetype:CSV), geodata (filetype:shp), or database extracts (filetype:MDB, filetype:SQL, filetype:DB). If you&#8217;re into that sort of thing, you can even look for PDFs (filetype:pdf).
‚óè   Another option is searching by a part of a URL: (inurl:downloads filetype:xls) will try to find all Excel files that have "downloads" in their web address (if you find a single download, its often worth just checking what other results exist for the same folder on the web server). You can also limit your search to only those results on a single domain name: (site:agency.gov).
‚óè   Another popular trick is not to search for content directly, but for places where bulk data may be available. For example, (site:agency.gov Directory Listing) may give you some listings generated by the web server with easy access to raw files, while (site:agency.gov Database Download) will look for intentionally created listings.</p></div>
<div class="paragraph"><p><strong>2. Ask an Expert.</strong></p></div>
<div class="paragraph"><p>Professors, government offices and industry folks often know where to look. Call 'em up! Show up at the office. Ask nicely. "I&#8217;m doing a story on X. Where would I find this?" "Do you know who has this?"</p></div>
<div class="paragraph"><p><strong>3. Ask a mailing list.</strong></p></div>
<div class="paragraph"><p>Mailing lists combine the wisdom of a whole community on a particular topic. For data journalists, NICAR-L is an excellent starting point. This is a listserv of data-journalism geeks, working on all kinds of projects. Chances are that someone has done a story like yours, and would have an idea of where to start ‚Äî if not a link to the data itself.</p></div>
<div class="paragraph"><p>Other Mailing lists you could try:
‚óè   Project Wombat - "Project Wombat is a discussion list for difficult reference questions". Ask them anything. They&#8217;ll likely be able to find you an answer with references
‚óè   Open Knowledge Foundation Lists: <a href="http://wiki.okfn.org/Mailing_Lists">http://wiki.okfn.org/Mailing_Lists</a>. In particular the Data Driven Journalism List, which acts as a hub for many journalistic enquiries.
‚óè   theInfo is an older set of mailing lists that has a lot of knowledgeable experts subscribed.</p></div>
<div class="paragraph"><p>Topic Specific Lists:
‚óè   Transparency: Sunlight Labs Mailing List - The Sunlight Foundation focuses their activities into promoting and facilitating government transparency.
‚óè   Mapping: North American Cartographic Information Society
‚óè   Mapping: Society of Cartographers CARTO-SoC
‚óè   Mapping: MAPS-L: <a href="mailto:LISTSERV@LISTSERV.UGA.EDU">LISTSERV@LISTSERV.UGA.EDU</a> - MAPS-L is an international discussion forum for anyone dealing with cartographic information, cartographers, remote sensors, geographers, and cartomaniacs of all types.</p></div>
<div class="paragraph"><p><strong>4. Use Forums.</strong></p></div>
<div class="paragraph"><p>Search for existing answers or ask a question at Get The Data or on Quora. GetTheData is Q&amp;A site where you can ask your data related questions, including where to find data relating to a particular issue, how to query or retrieve a particular data source, what tools to use to explore a data set in a visual way, how to cleanse data or get it into a format you can work with.</p></div>
<div class="paragraph"><p><strong>5. Join Hacks/Hackers</strong></p></div>
<div class="paragraph"><p>A rapidly expanding international grassroots journalism organization with dozens of chapters and thousands of members across four continents The mission is to create a network of journalists ("hacks") and technologists ("hackers") who rethink the future of news and information. With such a broad network - you stand a strong chance of someone knowing where to look for the thing you seek.</p></div>
<div class="paragraph"><p><strong>6. Learn a bit about government IT.</strong></p></div>
<div class="paragraph"><p>Understanding the technical and administrative context in which governments maintain their information is often helpful when trying to access data. Whether it&#8217;s CORDIS, COINS or THOMAS ‚Äì big-acronym databases often become most useful once you understand a bit about their intended purpose.</p></div>
<div class="paragraph"><p>Find government org charts and look for departments/units with a cross-cutting function (e.g. reporting, IT services), then explore their web sites. A lot of data is kept in multiple departments and while for one, a particular database may be their crown jewels, another may give it to you freely.</p></div>
<div class="paragraph"><p>Look out for dynamic infographics on government sites. These are often powered by structured data sources/APIs that can be used independently (e.g. flight tracking applets, weather forecast java apps).</p></div>
<div class="paragraph"><p><strong>7. Google it again using phrases and improbable sets of words you&#8217;ve spotted since last time.</strong></p></div>
<div class="paragraph"><p><strong>8. Write a FOI Request!</strong></p></div>
<div class="paragraph"><p>If you believe that a government body has the data you need, a freedom of information request may be your best tool. See chapter 4.1.2 Asking for data, for more information.</p></div>
<div class="paragraph"><p>2.1.2 Data catalogues and social data</p></div>
<div class="paragraph"><p>by Friedrich Lindenberg</p></div>
<div class="paragraph"><p>Over the last couple of years, a number of data portals have sprung up all over the web, that aim to help you get access to all kinds of information:</p></div>
<div class="paragraph"><p>‚óè         Government Data Portals: the government&#8217;s willingness to release a given dataset will vary from country to country. A growing number of states are providing data portals (inspired by data.gov and data.gov.uk) to promite the civic and commercial re-use of government information. An up-to-date, global index of such sites can be found at datacatalogs.org. Another handy site is the Guardian World Government Data, a meta search engine that includes many international government data catalogues.
‚óè         The DataHub ‚Äì a community-driven resource that makes it easy to find, share and reuse open content and data, especially in ways that are machine automatable.
‚óè   ScraperWiki an online tool to make the process of extracting "useful bits of data easier so they can be reused in other apps, or rummaged through by journalists and researchers." Most of the scrapers and their databases are public and can be re-used.
‚óè   World Bank and United Nations data portals provide high-level indicators for all countries, often for many years in the past.
‚óè         A number of startups are emerging, that aim to build communities around data sharing and re-sale. This includes Buzzdata - a place to share and collaborate on private and public datasets ‚Äì and data shops such as Infochimps and DataMarket.
‚óè   DataCouch - A place to upload, refine, share &amp; visualize your data.
‚óè   An interesting Google subsidiary, Freebase, provides "an entity graph of people, places and things, built by a community that loves open data."</p></div>
<div class="paragraph"><p># Research data #</p></div>
<div class="paragraph"><p>by Jonathan Gray
‚óè   Google Scholar
‚óè   Mendeley
‚óè   IBM Research - Technical paper search
‚óè   JSTOR - "With more than a thousand academic journals and over 1 million images, letters, and other primary sources, JSTOR is one of the world&#8217;s most trusted sources for academic content."
‚óè   UK Data Archive - "The UK&#8217;s largest collection of digital research data in the social sciences and humanities"</p></div>
<div class="paragraph"><p># Reference data #</p></div>
<div class="paragraph"><p>by Friedrich Lindenberg</p></div>
<div class="paragraph"><p>It&#8217;s often helpful to pull in data from code sheets, such as lists of countries, states, organizations etc. This kind of data - which is used to augment another collection - is called reference data. Other examples of reference data include code sheets to decipher classifications and taxonomies, such as the EU&#8217;s procurement vocabulary: on most portals, only short identifiers are provided, while the actual term descriptions are quite helpful.</p></div>
<div class="paragraph"><p>Some sources for reference data include:
‚óè   GeoNames and Nominatim for geographic names and coordinates.
‚óè   National statistics agencies
‚óè   Web sites of standards bodies, such as the ISO (e.g. country codes) or the IATI Standard site.
‚óè   International organisations such as the United Nations (e.g. classification of functions of government).</p></div>
</div>
<div class="sect2">
<h3 id="_doing_the_legal_stuff">Doing the legal stuff</h3>
<div class="paragraph"><p>(Source: The Open Data Manual, with input from Helen Darbishire, Access Info)</p></div>
<div class="paragraph"><p>Most of the new Open Data portals being set up by governments now make clear that the data that is released can be used free of charge, same goes for information obtained through Freedom of Information Requests. If you have scraped the data from a database such as a public body&#8217;s website, then you may be particularly liable for limitations on reuse. Sometimes there will be intellectual property limitations, but normally the only requirement is that you cite the source of the information.</p></div>
<div class="paragraph"><p>When talking about databases and intellectual property we need to distinguish between the structure and the content of a database (when we use the term <em>data</em> we shall mean the content of the database itself). Structural elements include things like the field names and a model for the data&#8201;&#8212;&#8201;the organization of these fields and their inter-relation.</p></div>
<div class="paragraph"><p>In many jurisdictions it is likely that the structural elements of a database will be covered by copyright (it depends somewhat on the level of <em>creativity</em> involved in creating this structure).</p></div>
<div class="paragraph"><p>The distinction between the "contents" of a database and the collection is especially crucial for factual databases since no jurisdiction grants a monopoly right in the individual facts (the "contents") even though it may grant right(s) in them as a collection.</p></div>
<div class="paragraph"><p>To illustrate, consider the simple example of a database which lists the melting point of various substances. While the database as a whole might be protected by law so that one is not allow to access, reuse or redistribute it without permission this would never prevent you from stating the fact that substance Y melts at temperature Z.</p></div>
<div class="paragraph"><p>You can read more about the situation your jurisdiction in the Guide to Open Data Licensing.</p></div>
<div class="paragraph"><p>If you find you are having problems with your right to reuse information, then please let the campaign group Access Info Europe know (<a href="mailto:helpdesk@access-info.org">helpdesk@access-info.org</a>). Access Info will help you with legal advice and will try to find lawyers in your country should that be necessary.
Success? What now&#8230;? Share the data&#8230;</p></div>
<div class="paragraph"><p>When you publish your project, do the rest of us a favor and include the data you&#8217;ve collected! It would be a damned shame if all that beautiful data you dug up (cleaned up, formatted and augmented) went stale on your hard drive. Let others source-check your story and perhaps even find different stories in the data.</p></div>
<div class="paragraph"><p>When you do publish data, include an explicit IP statement, in particular an open license like the Creative Commons Zero or Attribution terms or the Open Database License (ODbL). If the data is government information not covered by copyright, publish it using Creative Commons' PD Mark dedication to make it clear that the data will be available in the public domain forever and for others to reuse!</p></div>
<div class="paragraph"><p>NOTE FOR JWYG: Much of the information in this chapter formerly related to submitting an FOI request, differences in legislation in different countries&#8230; It&#8217;s interesting but not specific to data journalism. The overflow material is in this doc: FOI tipsheet (also linked from overflow materials) Thought perhaps we could turn the other sections into an FOI tipsheet and publish it on DDJnet and link to it</p></div>
</div>
<div class="sect2">
<h3 id="_asking_for_data">Asking for data</h3>
<div class="paragraph"><p>Authors: Helen Darbishire (Access Info), Fabrizio Scrollini (London School of Economics), Martin Rosenbaum (BBC)
Peer-reviewers: Martin Rosenbaum, BBC
Editor: Sam Leon (Open Knowledge Foundation), Lucy Chambers (Open Knowledge Foundation), Liliana Bounegru (European Journalism Centre)</p></div>
<div class="paragraph"><p># Your Right to Data #</p></div>
<div class="paragraph"><p>Information held by public bodies is one of the principle sources of data for the work of any journalist. The right of access to information (aka "freedom of information") is the right of anyone to access information held by public authorities. Many countries now have freedom of information laws to make this information available to the public. This chapter deals with:
FOIA for data - what to ask for, what format to request it in and how to ask for it
Case studies about stories based on successful data FOI requests.</p></div>
<div class="paragraph"><p>For more detailed information on the process of submitting an FOI request, the difference in FOI legislation in different countries and appeal procedures see the FOI tipsheet.
Before FOI - Where to look</p></div>
<div class="paragraph"><p>Before you delve into making a FOI request, see if the data is already available - or has already been requested by others. See 2.1 <em>Where Does Data Live</em> for more information on where you can look online for the data.
Privacy Concerns and FOI for databases</p></div>
<div class="paragraph"><p>Privacy concerns: For data journalists trying to get access to a full data set, issue might come up because some of the information in the data set is confidential and the public authority uses this as a reason for not providing the entire database. This is not acceptable under the right of access to information: public bodies have to provide at least partial access.</p></div>
<div class="paragraph"><p>For example, by removing personal data fields from the database so that the data journalist can see all the statistical and factual data the public authority would not be violating confidentiality. All exceptions must be justified by the public body and you have a right to appeal.
TIP: "Look at the <em>disclosure logs</em> that some public authorities have on their websites, which list some or all of their responses to previous FOI requests. They are a good source of ideas about what kind of information you are actually likely to obtain." (Martin Rosenbaum - BBC)</p></div>
<div class="paragraph"><p>FOI requests - useful tips</p></div>
<div class="paragraph"><p>from the Legal Leaks Toolkit (Access Info Europe and n-ost)
Tips by Martin Rosenbaum, BBC
Fact boxes - Helen &amp; Fabrizio
Before you submit your request</p></div>
<div class="paragraph"><p><strong>1. Plan ahead to save time.</strong></p></div>
<div class="paragraph"><p>Think about submitting a formal access request whenever you set out to look for information. It&#8217;s better not to wait until you have exhausted all other possibilities. You will save time by submitting a request at the beginning of your research and carrying out other investigations in parallel.</p></div>
<div class="paragraph"><p>Tip: Be prepared for delay. FOI can be most useful where the information would still be of use to you weeks or even months later.</p></div>
<div class="paragraph"><p><strong>2. Check the rules about fees</strong></p></div>
<div class="paragraph"><p>Before you start submitting a request, check the rules about fees for either submitting requests or receiving information. That way, if a public official suddenly asks you for money, you will know what your rights are.</p></div>
<div class="paragraph"><p>FACT BOX: You can ask for electronic documents to avoid copying and posting costs, mention in your request that you would prefer the information in electronic format. That way you will avoid paying a fee, unless of course the information is not available electronically, although these days it&#8217;s usually possible to scan documents which are not already digitalised and then to send them as an attachment by e-mail.</p></div>
<div class="paragraph"><p>Writing your FOI request:
Start out simple: In all countries, it is better to start with a simple request for information and then to add more questions once you get the initial information. That way you don&#8217;t run the risk of the public institution applying an extension because it is a "complex request".</p></div>
<div class="paragraph"><p>Tip: A request for information only held by one part of a public authority will probably be answered more quickly than one which requires a search across the entire authority. A request which involves the authority in consulting third parties (eg, a private company which supplied the information, another government which is affected by it) can take particularly long. Be persistent.</p></div>
<div class="paragraph"><p>Tip: Think <em>inside the filing cabinet</em>. Try to find out what data is collated. For example, if you get a blank copy of the form the police fill out after traffic accidents, you can then see what information they do or do not record about car crashes.
FACT BOX: Time Limits in freedom of information laws</p></div>
<div class="paragraph"><p>Most freedom of information laws provide a time limit for authorities to reply to you. Globally, the range in most laws is from a few days to one month. Read more: (6)
Submit multiple requests: If you are unsure where to submit your request, there is nothing to stop you submitting the request with two, three or more bodies at the same time. In some cases, the various bodies will give you different answers, but this can actually be helpful in giving you a fuller picture of the information available on the subject you are investigating.</p></div>
<div class="paragraph"><p>Tip: Especially important if you are planning to compare data from different public authorities. Is your request in any way ambiguous?</p></div>
<div class="paragraph"><p>For example, if you ask for figures for <em>the past three years</em>, some authorities will send you information for the past three calendar years and others for the past three financial years, which you won&#8217;t be able to directly compare.
Submit international requests: Increasingly requests can be submitted electronically, so it doesn&#8217;t matter where you live. Alternatively, if you do not live in the country where you want to submit the request, you can sometimes send the request to the embassy and they should transfer it to the competent public body. You will need to check with the relevant embassy first if they are ready to do this ‚Äì sometimes the embassy staff will not have been trained in the right to information and if this seems to be the case, it&#8217;s safer to submit the request directly to the relevant public body.
Use appropriate language: use language and etiquette appropriate to any other professional communication in your country.</p></div>
<div class="paragraph"><p>Tip: If you are planning to send the same request to many public authorities start by sending an initial draft of the request to a few authorities as a pilot exercise. This will show you whether you are using the right terminology to obtain the material you want and whether answering your questions is feasible, so that you can then revise the request if necessary before sending it to everyone.</p></div>
<div class="paragraph"><p>Mention your right to information: Usually the law does not require that you mention the access to information law or freedom of information act, but this is recommended because it shows you know your legal rights and is likely to encourage correct processing of the requests according to the law. We note that for requests to the EU it&#8217;s important to mention that it&#8217;s an access to documents request and it&#8217;s best to make a specific mention of Regulation 1049/2001.
Hide your request in a more general one: If you decide to hide your real request in a more general one, then you should make your request broad enough so that it captures the information you want but not so broad as to be unclear or discourage a response. Specific and clear requests tend to get faster and better answers.
Anticipate the exceptions: If you think that exceptions might be applied to your request, then, when preparing your questions, separate the question about the potentially sensitive information from the other information that common sense would say should not fall under an exception. Then split your question in two and submit the two requests separately.
Ask for access to the files#: If you live near where the information is held (e.g. in the capital where the documents are kept), you can also ask to inspect original documents. This can be helpful when researching information that might be held in a large number of documents that you&#8217;d like to have a look through. Such inspection should be free of charge and should be arranged at a time that is reasonable and convenient for you.</p></div>
<div class="paragraph"><p>FACT BOX: Governments are not obliged to process data for you, but should give you all the data they have, and if it is data that they should have according to perform their legal competencies, they should certainly produce it for you.
Keep a record! Make your request in writing and save a copy or a record of it so that in the future you are able to demonstrate that your request was sent, in case you need to make an appeal against failure to answer. This also gives you evidence of submitting the request if you are planning to do a story on it.
Speed up answers by making it public that you submitted a request: If you write or broadcast a story that the request has been submitted, it can put pressure on the public institution to process and respond to the request. You can update the information as and when you get a response to the request ‚Äì or if the deadline passes and there is no response you can make this into a news story as well. Doing this has the additional benefit of educating members of the public about the right of access to information and how it works in practice.
Involve your colleagues in using access to information: If your colleagues are sceptical about the value of access to information requests, one of the best ways to convince them is to write a story based on information you obtained using an access to information law. Mentioning in the final article or broadcast piece that you used the law is also recommended as a way of enforcing its value and raising public awareness of the right.
Data Specific FOI requests</p></div>
<div class="paragraph"><p>What to ask for: <em>Raw Data Now</em></p></div>
<div class="paragraph"><p>The focus of many access to information advocates has turned towards persuading governments to make more information available in formats you can use and reuse.</p></div>
<div class="paragraph"><p>This reframing of the right to data seeks to make clear that:
Detailed information should be available in a "disaggregated" or "granular" format ‚Äì this is sometimes also referred to as "raw data" before it has been "cooked" by public officials and statisticians.
Entire data sets or databases should be made available, free of charge and in an open source and machine readable format in order to permit re-use of the information.</p></div>
<div class="paragraph"><p>In the following sections you will find a brief definition of what access to information means globally and nationally.</p></div>
<div class="paragraph"><p>Read More: A recent report by the Open Knowledge Foundation and Access Info Europe provides a thorough description of this evolving field.
Case Studies</p></div>
</div>
<div class="sect2">
<h3 id="_wobbing_works_use_it_by_brigitte_alfter_freelance_journalist">Wobbing works. Use it! by Brigitte Alfter (Freelance Journalist)</h3>
<div class="paragraph"><p>www.alfter.dk, journalist / co-founder of the www.Wobbing.eu network, co-founder of the www.Farmsubsidy.org network</p></div>
<div class="paragraph"><p>Using freedom of information legislation ‚Äì or wobbing, as the slang goes ‚Äì is an excellent tool. But it requires method and often persistence. Three examples from Brigitte Alfter about the strength and challenges of Wobbing:</p></div>
<div class="paragraph"><p>Case Study 1: The Farmsubsidy project</p></div>
<div class="paragraph"><p>These years the EU pays almost ‚Ç¨ 60 billion to farmers and the farming industry. Per year. This has been going on since late 1950ies and the political narrative was that the subsidies help our poorest farmers. However a first FOI breakthrough in Denmark in 2004 indicated: The story was just a narrative. The small farmers were struggling as they so often complained about in private and in public, and in reality most of the money went to a few large land owners and to the agro industry. So obviously I wanted to know: Is there a pattern throughout the EU?</p></div>
<div class="paragraph"><p>In the summer of 2004 I asked the European Commission for the data. Every year in February the Commission receives data from the member states. Data show who applied for EU funding, how much beneficiaries got, and whether they got it for farming their land, developing their region or for exporting milkpowder. At the time, the Commission received the figures on CD-roms with csv files. A lot of data, but in principle easy to work with. If you got them out, that is.</p></div>
<div class="paragraph"><p>In 2004 the Commission refused ‚Äì the key argument being, that the data were uploaded into a database and couldn&#8217;t be retrieved without a lot of work. An argument, that the European Ombudsmand called "maladministration". You can find all documents in this case on www.wobbing.eu here. Back in 2004 we did not have the time to be legal foodies. We wanted the data.</p></div>
<div class="paragraph"><p>So we teamed up throughout Europe and went country by country to get the data. English, Swedish and Dutch colleagues got the data in 2005. Finland, Poland, Portugal, regions of Spain, Slovenia and other countries opened up in the too. Even in wob-difficult Germany I got a breakthrough and received some data in the province of North Rhine-Westfalia in 2007. I had to go to court to get the data ‚Äì but it then resulted in some nice articles in news magazine Stern and Stern online.</p></div>
<div class="paragraph"><p>Was it random, that Denmark and the UK were the first to open? Not necessarily. In a larger political picture the farm subsidies at the time had to be seen in the context of the WTO negotiations, where subsidies were under pressure. Denmark and the UK are among the more liberal countries in the EU, so there may well have been political winds blowing into the direction of transparency in those countries.</p></div>
<div class="paragraph"><p>The story did not stop there, for more episodes and for the data go to www.farmsubsidy.org.</p></div>
<div class="paragraph"><p>Lessons learnt: Go wob-shopping. We have a fabulous diversity of freedom of information laws in Europe, and different countries have different political interests at given points of time. This is an advantage to use.</p></div>
<div class="paragraph"><p>Case Study 2: Side effects</p></div>
<div class="paragraph"><p>We are all guinea pigs when it comes to taking medicine. Drugs can have side-effects. We know, we consider it compared to the effect we wish for ‚Äì and we make a decision. Unfortunately often this decision is not an informed decision.</p></div>
<div class="paragraph"><p>When teenagers take a pill against pimples, they hope for a smooth skin and not for a bad mood. Yet exactly that happened with one drug, where the youngsters turned depressive and even suicidal. The danger of this particular side effect was not easily available - an obvious story for journalists.</p></div>
<div class="paragraph"><p>There are data about side-effects. The producers regularly have to deliver information to the health authorities about observed side-effects. They are held by national or EU-authorities once a drug is allowed on the market.</p></div>
<div class="paragraph"><p>The initial breakthrough again came on national level in Denmark. During a cross-border research by a Danish-Dutch-Belgian team, also the Netherlands opened. Another example of wob-shopping: This time it helped to point out to the Dutch authorities, that the data were accessible in Denmark.</p></div>
<div class="paragraph"><p>But the story was true, in Europe there were suicidal young people and sadly also suicides in several countries. Journalists, university researchers, the family of a young victim were all pushing hard to get access to this information. The EU-ombudsman helped push the transparency at the European Medicines Agency - and it looks, as if he succeeded. So now the task is upon journalists to get out data and analyse the material thoroughly. Are we ‚Äì as one researcher put it ‚Äì all guinea pigs, or are the control mechanisms sound?</p></div>
<div class="paragraph"><p>Lessons learnt: Don&#8217;t take no for an answer when it&#8217;s about transparency, be persistent and follow a story over time. Things may well change and allow better reporting based upon better access at a later point.</p></div>
<div class="paragraph"><p>Case Study 3: Smuggling death</p></div>
<div class="paragraph"><p>Recent history can be utterly painful for entire populations, particularly after wars and in times of transition. So how can journalists obtain hard data to investigate, whether ‚Äì for example - last decades war profiteers are in power now? This was the task two Slovenian, a Croatian and a Bosnian journalist set out, the Slovene team being the driving force.</p></div>
<div class="paragraph"><p>In their home country Slovenia parliamentary commissions had set out to look into the question of profiting from the Balkan wars, but never reached a conclusion. Yet there was a highly valuable trail of declassified documents and data. 6000 pages which the Slovene team obtained through a freedom of information request.</p></div>
<div class="paragraph"><p>In this case the data had to be extracted from the documents and sorted in databases ‚Äì for example transports had to be traced by vessel number in ports and license plates of trucks. Thanks to the data and in combination with further data, analysis and research, they were able to map numerous of the routes of the illegal weapon trade.</p></div>
<div class="paragraph"><p>But the team succeeded and the results are unique and have already brought the first award to the team. But more importantly the story matters for the entire region and may well be picked up by journalists in other countries along the routes of the deadly material.</p></div>
<div class="paragraph"><p>Lessons learnt: Get out good raw material even if you find it in unexpected places and combine with already accessible public data.</p></div>
<div class="paragraph"><p>Useful resources
For further case studies and tools to facilitate making FOI requests, please see Further Resources.</p></div>
</div>
<div class="sect2">
<h3 id="_getting_data_from_the_web_by_friedrich_lindenberg_open_knowledge_foundation">Getting data from the web by Friedrich Lindenberg (Open Knowledge Foundation)</h3>
<div class="paragraph"><p>Note for JWYG: see also original scraping chapter and import useful bits: <a href="https://docs.google.com/document/d/1idnhY0XSFeLSRlRfMU2nm_dnjN-8iKLWP7W_8-WqVi4/edit">https://docs.google.com/document/d/1idnhY0XSFeLSRlRfMU2nm_dnjN-8iKLWP7W_8-WqVi4/edit</a> - old scraping version</p></div>
<div class="paragraph"><p>You&#8217;ve tried everything else, and you haven&#8217;t managed to get your hands on the data you want. You&#8217;ve found the data on the web, but, alas - no download options are available and copy-paste has failed you. Fear not, there may still be a way to get the data out. This chapter is about the technical solutions to acquiring machine-readable information. Some examples include:
Getting data from web-based APIs, such as interfaces provided by online databases and many modern web applications (including Twitter, Facebook and many others#). This is a fantastic way to access government or commercial data, as well as data from social media sites.
Extracting data from PDFs. This is very difficult, as PDF is a language for printers and does not retain much information on the structure of the data that is displayed within a document. Extracting information from PDFs is beyond the scope of this book, but there are some tools# and tutorials that may help you do it.
Screen scraping web sites. During screen scraping, you&#8217;re extracting structured content from a normal web page with the help of a scraping utility or by writing a small piece of code. While this method is very powerful and can be used in many places, it requires a bit of understanding about how the web works.</p></div>
<div class="paragraph"><p>With all those great technical options, don&#8217;t forget the simple options: often it is worth to spend some time searching for a file with machine-readable data or to call the institution which is holding the data you want.
What is machine-readable data?</p></div>
<div class="paragraph"><p>The goal for most of these methods is to get access to machine-readable data. Machine readable data is created for automatic processing by a computer, instead of the presentation to a human user. The structure of such data relates to contained information, and not the way it is displayed eventually. Examples of easily machine-readable formats include CSV, XML, JSON and Excel files, while formats like Word documents, HTML pages and PDF files are more concerned with the visual layout of the information. PDF for example is a language which talks directly to your printer, it&#8217;s concerned with position of lines and dots on a page, rather than distinguishable characters.
Scraping web sites: what for?</p></div>
<div class="paragraph"><p>Everyone has done this: you go to a web site, see an interesting table and try to copy it over to Excel so you can add some numbers up or store it for later. Yet this often does not really work, or the information you want is spread across a large number of web sites. Copying by hand can quickly become very tedious, so it makes sense to use a bit of code to do it.</p></div>
<div class="paragraph"><p>The advantage of scraping is that you can do it with virtually any web site - from weather forecasts to government spending, even if that site does not have an API for raw data access.</p></div>
<div class="paragraph"><p>What you can and cannot scrape</p></div>
<div class="paragraph"><p>There are, of course, limits to what can be scraped. Some factors that make it harder to scrape a site include:</p></div>
<div class="paragraph"><p>Badly formatted HTML code with little or no structural information e.g. older government websites.
Authentication systems that are supposed to prevent automatic access e.g. CAPTCHA codes and paywalls.
Session-based systems that use browser cookies to keep track of what the user has been doing.
A lack of complete item listings and possibilities for wildcard search.
Blocking of bulk access by the server administrators.</p></div>
<div class="paragraph"><p>Another set of limitations are legal barriers: some countries recognize database rights, which may limit your right to re-use information that has been published online. Sometimes, you can choose to ignore the license and do it anyway - depending on your jurisdiction, you may have special rights as a journalist. Politically, for Government data you&#8217;ll be fine too. Commercial organizations ‚Äì and certain NGOs ‚Äì react with less tolerance and may try to claim that you&#8217;re "sabotaging" their systems. Other information may infringe the privacy of individuals and thereby violate data privacy laws or professional ethics.</p></div>
<div class="paragraph"><p>Tools that help you scrape</p></div>
<div class="paragraph"><p>There are many programs that can be used to extract bulk information from a web site, including browser extensions and some web services. Depending on your browser, tools like Readability (which helps extract text from a page) or DownThemAll (which allows you to download many files at once) will help you automate some tedious tasks, while Chrome&#8217;s Scraper extension is explicitly built to extract tables from a web site. Developer extensions like FireBug (for Firefox, the same thing is already included in Chrome, Safari and IE) let you track exactly how a web site is structured and what communications happen between your browser and the server.</p></div>
<div class="paragraph"><p>ScraperWiki is a web site that allows you to code scrapers in a number of different programming languages, including Python, Ruby and PHP. If you want to get started with scraping without the hassle of setting up a programming environment on your computer, this is the way to go. Other web services, such as Google Spreadsheets and Yahoo! Pipes also allow you to perform some extraction from other web sites.</p></div>
<div class="paragraph"><p>How does a web scraper work?</p></div>
<div class="paragraph"><p>Web scrapers are usually small pieces of code written in a programming language such as Python, Ruby or PHP. Choosing the right language is largely a question of which community you have access to: if there is someone in your newsroom or city already working with one of these languages, then it makes sense to adopt the same language.</p></div>
<div class="paragraph"><p>While some of the click-and-point scraping tools mentioned before may be helpful to get started, the real complexity involved in scraping a web site is in addressing the right pages and the right elements within these pages to extract the desired information. These tasks aren&#8217;t about programming, but understanding the structure of the web site and database.</p></div>
<div class="paragraph"><p>When displaying a web site, your browser will almost always make use of two technologies: HTTP is a way for it to communicate with the server and to request specific resource, such as documents, images or videos. HTML is the language in which web sites are composed.</p></div>
<div class="paragraph"><p>The anatomy of a web page</p></div>
<div class="paragraph"><p>Any HTML page is structured as a hierarchy of boxes: a large box (or "tag") will contain many smaller ones - for example a table that has many smaller divisions: rows and cells. There are many types of tags that perform different functions - some produce boxes, others tables, images or links. Tags can also additional properties such as unique identifiers and belong to groups called <em>classes</em>, which makes it possible to target and capture individual elements within a document. Selecting the appropriate elements this way and extracting their content is the key to writing a scraper.
Viewing the elements in a web page: everything can be broken up into boxes within boxes.
To scrape web pages, you&#8217;ll need to learn a bit about the different types of elements that can be in an HTML document - for example, the &lt;table&gt; element wraps a whole table, which has &lt;tr&gt; elements for its rows, which in turn contain &lt;td&gt; for each cell. The most common element type you will encounter is &lt;div&gt;, which can basically mean anything#. The easiest way to get a feel for these elements is by using the developer toolbar in your browser: they will allow you to hover over any part of a web page and see what the underlying code is.</p></div>
<div class="paragraph"><p>Tags work like book ends, marking the start and the end of a unit. For example &lt;strong&gt; signifies the start of a bold piece of text &lt;/strong&gt; signifies the end of that section. Easy.</p></div>
<div class="paragraph"><p>An Example: Nuclear incidents</p></div>
<div class="paragraph"><p>NEWS is the International Atomic Energy Agency&#8217;s (IAEA) portal on world-wide radiation incidents and a strong contender for membership in the Weird Title Forum. The web page lists incidents in a simple, blog-like site that can be easily scraped. To start, create a new Python scraper on ScraperWiki and you will be presented with a text area that is mostly empty, except for some scaffolding code. In another browser window, open the IAEA site and open the developer toolbar in your browser. In the "Elements" view, try to find the HTML element for one of the news item titles.
Your browsers developer toolbar helps you connect elements on the web page with the underlying HTML code.</p></div>
<div class="paragraph"><p>Investigating this page will reveal that the titles are &lt;h4&gt; elements within a &lt;table&gt;. Each event is a &lt;tr&gt; row, which also contains a description and a date. If we want to extract the titles of all events, we should find a way to select each row in the table sequentially, while fetching all the text within the title elements.</p></div>
<div class="paragraph"><p>In order to turn this process into code, we need to make ourselves aware of all the steps involved. To get a feeling for the kind of steps required, let&#8217;s play a simple game: In your ScraperWiki window, try to write up individual instructions for yourself, for each thing you are going to do while writing this scraper like in a recipe (prefix each line with a hash sign to tell Python that this not real computer code). E.g.:</p></div>
<div class="paragraph"><p># Look for all rows in the table
# Unicorn must not overflow on left side.</p></div>
<div class="paragraph"><p>Try to be as precise as you can and don&#8217;t assume that the program knows anything about the page you&#8217;re attempting to scrape.</p></div>
<div class="paragraph"><p>Once you&#8217;ve written down some pseudo-code, let&#8217;s compare this to the essential code for our first scraper:</p></div>
<div class="listingblock">
<div class="content">
<pre><tt>1. import scraperwiki
2. from lxml import html</tt></pre>
</div></div>
<div class="paragraph"><p>In this first section, we&#8217;re importing existing functionality from libraries - snippets of pre-written code. "scraperwiki" will give us the ability to download web sites, while "lxml" is a tool for the structured analysis of HTML documents. Good news: if you are writing a Python scraper with ScraperWiki, these two lines will always be the same.</p></div>
<div class="listingblock">
<div class="content">
<pre><tt>3. url = "http://www-news.iaea.org/EventList.aspx"
4. doc_text = scraperwiki.scrape(url)
5. doc = html.fromstring(doc_text)</tt></pre>
</div></div>
<div class="paragraph"><p>Next, (Line 3) we&#8217;re making a name (variable): "url" - and assign the URL of the IAEA page as its value. This tells the scraper that this thing exists and we want to pay attention to it. Note that the URL itself is in quotes as it is not part of the program code but a string - a sequence of characters.</p></div>
<div class="paragraph"><p>Line 4. We then use the "url" variable as input to a function, scraperwiki.scrape. A function will provide some defined job - in this case it&#8217;ll download a web page. When it&#8217;s finished, it&#8217;ll assign its output to another variable, "doc_text". "doc_text" will now hold the actual text of the website - not the visual form you see in your browser, but the source code, including all the tags. Since this form is not very easy to parse, we&#8217;ll use another function, html.fromstring, to generate a special representation where we can easily address elements - the so-called document object model (DOM).</p></div>
<div class="listingblock">
<div class="content">
<pre><tt>6. for row in doc.cssselect("#tblEvents tr"):
7.      link_in_header = row.cssselect("h4 a").pop()
8.      event_title = link_in_header.text
9.      print event_title</tt></pre>
</div></div>
<div class="paragraph"><p>Lines 6-9. In this final step, we use the DOM to find each row in our table and extract the event&#8217;s title from its header. Two new concepts are used: the for loop and element selection (.cssselect). The for loop essentially does what its name implies - it will traverse a list of items, assigning each a temporary alias ("row" in this case) and then run any indented instructions for each item.</p></div>
<div class="paragraph"><p>The other new concept, element selection, is making use of a special language to find elements in the document. CSS selectors are normally used to add layout information to HTML elements and can be used to precisely pick an element out of a page. In this case (Line. 6) we&#8217;re selecting "#tblEvents tr" which will match each &lt;tr&gt; within the table element with the ID "tblEvents" (the hash simply signifies ID). Note that this will return a list of &lt;tr&gt; elements.</p></div>
<div class="paragraph"><p>As can be seen on the next line (Line. 7), where we&#8217;re applying another selector to find any &lt;a&gt; (which is a hyperlink) within a &lt;h4&gt; (a title). Here we only want to look at a single element (there&#8217;s just one title per row), so we have to "pop" it off the top of the list returned by our selector with the .pop() function.</p></div>
<div class="paragraph"><p>Note that some elements in the DOM contain actual text, i.e. text that is not part of any markup language, which we can access using the "[element].text" syntax seen on line 8. Finally, in line 9, we&#8217;re printing that text to the ScraperWiki console. If you hit run in your scraper, the smaller window should now start listing the event&#8217;s names from the IAEA web site.</p></div>
</div>
<div class="sect2">
<h3 id="_crowdsourcing_data_by_marianne_bouchart_data_journalism_blog">Crowdsourcing data by Marianne Bouchart (Data Journalism Blog)</h3>
<div class="paragraph"><p># Crowdsourcing data #</p></div>
<div class="paragraph"><p>2.3.2  Crowdsourcing data</p></div>
<div class="paragraph"><p><strong>1. definition of crowdsourcing data for journalism</strong></p></div>
<div class="ulist"><ul>
<li>
<p>
Crowdsourcing is the act of sourcing tasks traditionally performed by specific individuals to a group of people or community (crowd) through an open call.
</p>
</li>
</ul></div>
<div class="paragraph"><p>Jeff Howe from Wired Magazine established that the concept of crowdsourcing depends essentially on the fact that because it is an open call to a group of people, it gathers those who are most fit to perform tasks, solve complex problems and contribute with the most relevant and fresh ideas.</p></div>
<div class="paragraph"><p>It has been used by journalists to turn readers into contributors for a few years now, some more successfully than others.</p></div>
<div class="paragraph"><p>“When you have tones of files, statistics, reports to go through, like with the MPs expenses or the wikileaks cables, crowdsourcing is a really good way to get things done,” argues Simon Rogers, editor of the Guardian’s Data Blog in an interview conducted for this handbook.</p></div>
<div class="paragraph"><p>Data crowdsourcing can be helpful both for collecting data and sorting out big datasets you already have but that are too big for you to sort out alone. It’s also a very good way to get different perspectives on a subject you work on as the audience will often point out details that you didn’t notice on your own.
Although this trend to ask the audience to become contributors in the analysis of big data has been a real success in projects such as the MPs expenses in the UK or the Wikileaks cables worldwide, the actual “collection” of data via crowdsourcing stays a problem as journalists and editors still don’t know how to make sure the data people contribute is reliable.
This sub-chapter will show you how data crowdsourcing can be used both as a collection tool and a helping hand to sort out existing datasets. We will be examining the example of  one media organisation, the Guardian, and how it implemented data crowdsourcing in their everyday work.
To crowdsource data, journalists can use surveys, polls, or comment fields. They can also ask their audience to contribute to a Google Doc or a spreadsheet.</p></div>
<div class="paragraph"><p>Online resources like the Open Data Cook Book gives you help on how to collect data using Google forms.</p></div>
<div class="paragraph"><p>“Google forms (part of the Google Docs online office suite) allow you to collect and share data, the wesbite says.</p></div>
<div class="paragraph"><p>“This recipe isn&#8217;t about using an existing source of data, but instead allows you to create your own data. You might want to run a survey, or log events that occur. With Google forms you can easily share the data you collect as a spreadsheet.”</p></div>
<div class="paragraph"><p>Go to the Open Data cook book website to follow their tutorial.</p></div>
<div class="paragraph"><p>Other platforms such as Help Me Investigate, Crowdsourcing.com or GetTheData.org offer good guidance on how to get the crowd involved in your data gathering.</p></div>
<div class="paragraph"><p>(intro to buzz data)</p></div>
<div class="paragraph"><p>Video interview with Nick Edouard from Buzz Data: Data crowdsourcing meets social media <a href="http://vimeo.com/36859634">http://vimeo.com/36859634</a></p></div>
<div class="paragraph"><p><strong>2. How to collect data from content already published in social media</strong></p></div>
<div class="paragraph"><p>using Twitter hashtags (examples: emergency hashtags used in the event of an earthquake or huricane in the US)
using Tools such as:
People Browser
Social mention (media monitoring tools)
Disqus</p></div>
<div class="paragraph"><p>Keyword search, what is discussed about specific topics on the web?
Look for the right keywords. Certain topics have different words in different countries (football, soccer, etc.).
Make your search as specified or as wide as possible, as adapted as possible to your search.
consider language, country, target groups
gather information from different platforms to get a common opinion
cross check to make sure the information is reliable and consistent</p></div>
<div class="paragraph"><p><strong>3. Data Crowdsourcing at the Guardian - Getting the readers involved</strong></p></div>
<div class="paragraph"><p>While there is already a huge amount of data available online, some media organisations are going one step further by getting their readers involved in the process of collecting and analysing data.</p></div>
<div class="paragraph"><p>“Crowdsourcing” has become a common practice in the world of data journalism and the Guardian is now considered as a reference in the field.</p></div>
<div class="paragraph"><p>The Guardian’s Data Blog won the 2011 Newspaper Awards prize for Best use of New Media and the Guardian’s DataStore was honoured at the Knight Batten awards for innovation in journalism in 2011.</p></div>
<div class="paragraph"><p>We went to the Guardian’s offices in London to talk to their data journalism team about data crowdsourcing and the different projects they have recently set up.</p></div>
<div class="paragraph"><p>Simon Rogers is the editor of the Data Blog. He told us how crowdsourcing can help when other ways of collecting and analysing data are not enough&#8230;</p></div>
<div class="paragraph"><p>SIMON ROGERS, Editor of the Data Blog:</p></div>
<div class="paragraph"><p>“ I suppose the thing about crowdsourcing is that when you have tones of stuff to go through, like with the MPs expenses where there were 40,000 pages to go through, that’s impossible for one person to do. Also, when you have got all this stuff that is inaccessible or in a bad format, that’s really where crowdsourcing can help.</p></div>
<div class="paragraph"><p>One thing the Guardian has got is lots of readers, lots of pairs of eyes, and if there is something interesting to do then it can really work. That’s what we did with the MPs expenses. We had 450,000 documents and very little time to do anything. So what better way than open up to readership?</p></div>
<div class="paragraph"><p>But what we found with the MPs expenses is that what we generated was tip-offs, stories more than data. So although it was remarquably successful in terms of traffic, people really liked it, it wasn’t in terms of actual raw data. It was hard to know who we could trust.</p></div>
<div class="paragraph"><p>We are doing something at the moment with MixMag on drug use and that has been phenomenal as well, it looks like it is going to be bigger than the British crime survey in terms of how many people come back to it. It’s brilliant.</p></div>
<div class="paragraph"><p>I think what those things have in common is that they are things people care about so they are willing to spend the time. A lot of the crowdsource we have done mostly relies on the obsessives. So with the MPs expenses we had a massive amount of traffic at the beginning and it really died down. But what we still got are people that are obsessively going through every page looking for that story. One person has done 30,000 pages. They know a lot of stuff.</p></div>
<div class="paragraph"><p>So in terms of generating stories it worked really well and people really liked it, it made the Guardian ‘look good’, but I think in terms of generating raw data, not so much.</p></div>
<div class="paragraph"><p>Then we did it with the Sarah Palin papers, that was again a good way of looking for stories, scouring the raw information for stories.</p></div>
<div class="paragraph"><p>But it also seems to me that some of the (crowdsourcing projects) that we’ve done that worked really well have been more like ‘all fashioned’ surveys. When you are asking people about their experience, about their lives, about what they’ve done, they work very well because people aren’t as likely to make that up. They will say what they feel. When we asked people to kind of do our job for us, you have to find a framework for people to produce the data in a way you can trust them.”</p></div>
<div class="paragraph"><p>Q: You were talking about reliability, how do you deal with that on a daily basis? How do you trust people to contribute?</p></div>
<div class="literalblock">
<div class="content">
<pre><tt>“I think the approach that Old Weather have got, where they get ten people to do each one, is a really good test on that. It is an old fashioned computer science technique to get a warm up as a train starter. I think that’s a very good way to do it.</tt></pre>
</div></div>
<div class="paragraph"><p>With the MPs expenses, we tried to minimise the risk of MPs going online and editing their own records to make themselves look better. But you can’t permanently guard that site, you can only look out for certain urls or if it’s coming from the SW1 area of London. So that’s a bit trickier. That’s where I felt out of it because I felt like the data we were getting out was not reliable. Even though stories were great, it wasn’t producing raw numbers that we could confidently use.”</p></div>
<div class="paragraph"><p>Q: Could you give a piece of advice to journalists who want to use crowdsourcing to collect data online?</p></div>
<div class="paragraph"><p>“I would say:
making it something personal to people really helps,
otherwise make it about what the people care about, or are gonna care about after, even when the news dies down, where they will still want to get involved.
also, what is really worth it is when you make it more like a game. Second time around when we did the expenses story, it was much more like a game, with individual tasks for people to do. It really help to give people specific tasks and things to do. That made a big difference because I think if you just present people with the mountain of information to go through, it’s a hard work, that’s what we get paid for. So I think making it fun is really important.”</p></div>
<div class="paragraph"><p>In October 2011 The Data Blog set up a crowdsourced map of the Occupy movement.</p></div>
<div class="paragraph"><p>Asking people to map where protests were taking place around the world, the Guardian data team managed to get hundreds of contributors and over 6000 Facebook shares by simply using a Google form.</p></div>
<div class="paragraph"><p>The result is a very straight forward map of the Occupy movement worldwide which is still being updated today.</p></div>
<div class="paragraph"><p>The Guardian’s data journalism team has done many of these projects in the past few years, each of them proved to be very successful. So we asked them for tips on how to use crowdsourcing in data journalism and we asked Data journalist James Ball to tell us about the Guardian’s most successful crowdsourcing projects…</p></div>
<div class="paragraph"><p>JAMES BALL, data journalist at the Guardian</p></div>
<div class="paragraph"><p>“I think the one that got the biggest response was something that we did on olympic ticketing. Thousands of people in the UK tried to get tickets for this year’s Olympics and there was a lot of fury that people hadn’t got them. People had ordered hundreds of pounds worth and been told they’ll get nothing. But no one really knew if it was just some people complaining quite loudly while actually most people were happy.</p></div>
<div class="paragraph"><p>So we tried to work out a way to find out. We decided the best thing we could really do, in the lack of any good data, was to ask people. And we thought we’d have to treat it as a light thing because it wasn’t a balanced sample.</p></div>
<div class="paragraph"><p>We created a Google form and asked very specific questions. It was actually a long form, it asked how much in value people had ordered their tickets, how much their card had been debited for, which events they went for, this kind of thing.</p></div>
<div class="paragraph"><p>We put it up as a small picture on the front of the site but it was shared around really rapidly. I think this is one of the key things, you can’t just think ‘what do I want to know’ for my story’, you have to think ‘what are people wanting to tell me right now’. And it’s only when you tap into what are people wanting to talk about that crowdsourcing is going to be successful.</p></div>
<div class="paragraph"><p>But the volume of responses on this, which is one of the earliest times we have ever tried to do this approach, was huge. We had a thousand responses in less than an hour and seven thousands by the end of that day.</p></div>
<div class="paragraph"><p>So obviously we took presenting the results a bit more seriously at this point, we had no idea how well it would do. So we put on some cavy hats you know, Guardian readers may be more wealthy than other people, also people who got less than they expected might be more willing to talk to us.</p></div>
<div class="paragraph"><p>So we didn’t know how much value the results would have. So we did that and cut it down and we had a good seven thousand records to base it on and we found about half the people who’d asked for tickets had got nothing, how many people had ordered and got. We ran all of this stuff, and obviously because so many people had taken part the day before, there was a lot of interest in the results.</p></div>
<div class="paragraph"><p>A few weeks later, the official summary report came out, and our numbers were shockingly close, they were almost exactly spot on. I think partly through luck but also because we got just so many people.</p></div>
<div class="paragraph"><p>Generally though, I tend to find that crowdsourcing is more useful for the content around the data story or for getting people to follow one up. So if we publish a story based on data that we do internally and I do quite a lot of work on lobying, I publish the full register or the whole thing that I based it on because sometimes you will get someone who doesn’t like a particular MP or Lord who will say ‘Oh, that research doesn’t declare anything but I know that they work for here’. Now you love to get that as an email rather than on your open tools that you usually use but that’s fine. You know, it’s all about learning how to follow up. So while it is less frequent that it helps you compile data, it often lets you do more with the data than you could yourself. You can only know so many people.</p></div>
<div class="paragraph"><p>Q: Can you talk me through the process of going through all the data you collected, you had about 7000 responses, how did you deal with this huge amount of data?</p></div>
<div class="paragraph"><p>“We’d preempted that we wanted to do analysis on it, which helped. If you start something in a comment thread, it is often too late to turn it into a usable way. So you have to set out at the start and think ‘what’s the best tool for what I want to know?’ Is it a comment thread, is it building an app, and if it is building an app, you have to think ‘Is this worth the wait? And is it worth the resource to do it?’</p></div>
<div class="paragraph"><p>In this case we thought of Google forms and if someone fills that out it turns into a row on a spreadsheet. So it meant that straight away, even if it was still updating, I could open the spreadsheet and see all of the results.</p></div>
<div class="paragraph"><p>I could have tried to do the work in Google but I downloaded it into Microsoft Excel and then did things like sort it from low to high and found the people who decided to write in instead of putting digits on how much they spent and fixed all of those. I decided not to exclude as little as I could. So rather than taking only valid responses, I tried to fix other ones. People had used foreign currencies so I converted them to sterling, all of which was a bit painstaking.</p></div>
<div class="paragraph"><p>But the whole analysis was done in a few hours, then I knocked out the obviously silly entries. A lot of people decided to fill it out pointing out they spent nothing on tickets, that’s a bit facetious but fine, I mean, that was less than a hundred out of seven thousands several hundreds.</p></div>
<div class="paragraph"><p>And then a few dozens who put obviously fake high amounts to try and destort it. I am talking things like ten million pounds in there, I wasn’t jumping to assumptions. So that left me with a set that I could use with the normal data principles we use every day. I did what’s called a ‘pivot table’, I did some averaging, that kind of thing.</p></div>
<div class="paragraph"><p>So because we planned it ahead, it meant it was quite simple, if we had stumble into it just through a comment thread, it would have been harder. It shows the value of thinking about what you might do with the results of crowdsourcing in advance.”</p></div>
<div class="paragraph"><p>Q: There was a lot of manual work to sort out the data, how many people did you have on this?</p></div>
<div class="paragraph"><p>“Just me. We hadn’t really any idea it would get the momentum it did so I worked with the Sports blog editor, we put our heads together and thought this might be a fun project. We did it, start to finish, in 24 hours. We had the idea, we put something up at lunch time, we put it on the front of the site, we saw it was proving quite popular, we kept it on the front of the site for the rest of the day and we presented the results online the next morning.”</p></div>
<div class="paragraph"><p>Q: What made you choose Google Doc for this, compared to other poll or survey tools that are available online?</p></div>
<div class="paragraph"><p>“It’s because it gives complete control over the results. I don’t have to use anyone else’s analytic tools. I can put it easily into a database software or into spreadsheets which was enough for that job. When you start using specialist polling softwares, you are often going through their tools and their restrictions. I think if the information we’d been asking for was particularly sensitive, we might have hesitated before using Google and thought about doing something ‘in house’. But generally, the ease of dropping a Google Form into a Guardian page and it’s virtually visible to the user that we are using one, is so convenient.</p></div>
<div class="paragraph"><p>Q: What advice would you give to other journalists who want to give a go to data crowdsourcing, in terms of what questions to ask the audience or how to tackle the subject?</p></div>
<div class="paragraph"><p>“- You have to have very specific things you want to know, and as much as possible, ask things that get multiple choice responses.
Try to get some basic demographics of who you are talking to so you can see if your sample might be biased.
If you are asking for amounts and things like this, try in the guidance to specify that it’s in digits, that they have to use a specific currency and things like that. A lot won’t, but the more you hold their hand through, the better.
and always, always put on a comment box because a lot of people will fill out the other things but what they really want is to give you their opinion on the story, specially on a consumer story or an outrage. You might actually be interested in only a few of the controlled fields but as long as they have got a chance to comment and as long as you use and reflects on some of the interesting ones, you get a much greater degree of response.”</p></div>
<div class="ulist"><ul>
<li>
<p>
trust/ethical issues around data crowdsourcing/ limitation
</p>
</li>
</ul></div>
<div class="paragraph"><p>“There’s some kind of flaw in the theory that crowdsourcing is a realistic way of converting data into information and stories, because it doesn’t seem to be happening.” Nick Davies: Data, crowdsourcing and the ‘immeasurable confusion’ around Julian Assange</p></div>
<div class="ulist"><ul>
<li>
<p>
Some media create a ranking, reward system to establish trust within the community that contributes to their data (The Guardian, Citizen side, etc.)
</p>
</li>
<li>
<p>
Most importantly, webmasters are needed to verify and clean the data along the way
</p>
</li>
<li>
<p>
In the data crowdsourcing process, give precise guidelines to your crowd, ask people to give a source link to verify the information they contribute
</p>
</li>
</ul></div>
<div class="paragraph"><p>There is still a lot to be done to make crowdsourcing data more reliable.</p></div>
<div class="paragraph"><p>END</p></div>
<div class="paragraph"><p>VERY ROUGH DRAFT!
Marianne Bouchart will be writing up this section in the next few days with more information, a case study and an interview. Other members of the team were keen to learn about the subject but didn&#8217;t have more to contribute on the spot.</p></div>
<div class="paragraph"><p>How to collaborate (or crowdsource) by combining Delicious and Google Docs</p></div>
<div class="paragraph"><p>INTRO</p></div>
<div class="paragraph"><p>How do you collect the data ? Introduction on how to get data using crowdsourcing</p></div>
<div class="paragraph"><p>Benefits of crowdsourcing data: crowdsource is a great way to to collect information on a specific topic, or even finding a story idea</p></div>
<div class="paragraph"><p>Difference between data you can use and data you can only read, how useful are they?</p></div>
<div class="paragraph"><p>Difference between active and passive contribution</p></div>
<div class="paragraph"><p><strong>1. Passive data crowdsourcing</strong></p></div>
<div class="paragraph"><p>Tools:
People Browser
Social mention (media monitoring tools)
Disqus</p></div>
<div class="paragraph"><p>Keyword search, what is discussed about specific topics on the web? Look for the right keywords. Certain topics have different words in different countries (football, soccer, etc.). Make your search as specified or as wide as possible, as adapted as possible to your search. - consider language, land, target groups - gather information from different platforms to get a common opinion</p></div>
<div class="paragraph"><p>Good to cross check to make sure the information is reliable.</p></div>
<div class="paragraph"><p><strong>2. Active data crowdsourcing</strong></p></div>
<div class="paragraph"><p>Tools:
Google Forms: via Tim Davies <a href="http://www.opendatacookbook.net/wiki/recipe/grow_your_own_with_google_forms">http://www.opendatacookbook.net/wiki/recipe/grow_your_own_with_google_forms</a>
Buzzdata
Help me investigate
My Society
Twitter # examples: emergency hashtags like in an earthquake
Get the data.org</p></div>
<div class="paragraph"><p>The Guardian&#8217;s data crowdsourcing projects -&#8594; case study by Marianne Bouchart</p></div>
<div class="paragraph"><p>Googles Fusion Tables, wikis give people the link to it
Polls (online or not) but ask the right questions (unbiased)</p></div>
<div class="paragraph"><p><strong>3. Legal and ethical issues in data crowdsourcing</strong></p></div>
<div class="paragraph"><p>how to verify the information people give? Can you trust the crowd?</p></div>
<div class="paragraph"><p>Some media create a ranking, reward system to establish trust within the community that contributes to their data</p></div>
<div class="paragraph"><p>To verify whether a source is reliable, you could also check klout/kred ranking of sources?</p></div>
<div class="paragraph"><p>Most importantly, webmasters are needed to verify and clean the data along the way</p></div>
<div class="paragraph"><p>In the data crowdsouricng process, give precise guidelines to your crowd, ask people to give a source link to verify the information they contribute</p></div>
<div class="paragraph"><p>interview with Pete Forde from Buzz Data on legal and ethical issues around data crowdsourcing (already done, needs editing)</p></div>
<div class="paragraph"><p>CCL
the future of crowdsourcing data for journalism
whosedata.net project
There is still a lot to be done to make crowdsourcing data more reliable. etc.</p></div>
<div class="paragraph"><p>You can now see a basic scraper operating: it downloads the web page, transforms it into the DOM form and then allows you to pick and extract certain content. Given this skeleton, you can try and solve some of the remaining problems using the ScraperWiki and Python documentation:
Can you find the address for the link in each event&#8217;s title?
Can you select the small box that contains the date and place by using its CSS class name and extract the element&#8217;s text?
ScraperWiki offers a small database to each scraper so you can store the results - copy the relevant example from their docs and adapt it so it will save the event titles, links and dates.
The event list has many pages - can you scrape multiple pages to get historic events as well?</p></div>
<div class="paragraph"><p>As you&#8217;re trying to solve these challenges, have a look around ScraperWiki: there are many useful examples in the existing scrapers - and quite often, the data is pretty exciting, too. This way, you don&#8217;t need to start off your scraper from scratch: just choose one that is similar, fork it and adapt to your problem.
Further Reading</p></div>
<div class="paragraph"><p>Other Tutorials:
Scraperwiki screencast for non-programmers: <a href="http://blog.scraperwiki.com/2011/08/15/scraperwiki-tutorial-screencast-for-non-programmers/">http://blog.scraperwiki.com/2011/08/15/scraperwiki-tutorial-screencast-for-non-programmers/</a>
ProPublica tutorial: <a href="http://www.propublica.org/nerds/item/scraping-websites">http://www.propublica.org/nerds/item/scraping-websites</a></p></div>
<div class="paragraph"><p>Handling PDF:
PDFs generated from Text files (Bad): <a href="http://www.propublica.org/nerds/item/turning-pdfs-to-text-doc-dollars-guide">http://www.propublica.org/nerds/item/turning-pdfs-to-text-doc-dollars-guide</a>
PDFs generated from Images (Worse): <a href="http://www.propublica.org/nerds/item/image-to-text-ocr-and-imagemagick">http://www.propublica.org/nerds/item/image-to-text-ocr-and-imagemagick</a></p></div>
<div class="paragraph"><p>Working with APIs:
Beginner&#8217;s guide for journalists who want to understand API documentation
Working with APIs: <a href="http://chronicle.com/blogs/profhacker/working-with-apis-part-1/22674">http://chronicle.com/blogs/profhacker/working-with-apis-part-1/22674</a>
Getting Started With Google Charts and Interactive Charts. There are also samples in the Google Visualization API Gallery.
How to grab useful political data with the They Work For You API</p></div>
<div class="paragraph"><p><strong>3.4 Crowdsourcing Expert Opinions: Annotating and analysing datasets</strong></p></div>
<div class="paragraph"><p>Overview: How to enable people to annotate and comment on datasets (particularly, harnessing expert opinions)
Thinking about your audience - how do you engage the right type of people?
Do&#8217;s and don&#8217;ts - lessons learnt
Tools &amp; Technical considerations
Write-access - can people damage your data?
File-sharing
Getting the data out
Authors: Ted Han?
Length: 1 page</p></div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_5_understanding_data">5. Understanding data</h2>
<div class="sectionbody">
<div class="paragraph"><p>Other ideas for this section:</p></div>
<div class="ulist"><ul>
<li>
<p>
Preparing Data
</p>
</li>
<li>
<p>
Reconciliation
</p>
</li>
<li>
<p>
Combining Datasets (cf: <a href="http://www.opendatacookbook.net/wiki/recipe/a_dataset_mixture_with_yahoo_pipes">http://www.opendatacookbook.net/wiki/recipe/a_dataset_mixture_with_yahoo_pipes</a>)
</p>
</li>
<li>
<p>
Text Mining
</p>
</li>
</ul></div>
<div class="sect2">
<h3 id="_become_data_literate_in_3_simple_steps_by_nicolas_kayser_bril_j">Become data literate in 3 simple steps by Nicolas Kayser-Bril (J++)</h3>
<div class="paragraph"><p>Just as literacy refers to "the ability to read for knowledge, write coherently and think critically about printed material" data-literacy is the ability to consume for knowledge, produce coherently and think critically about data. Data literacy includes statistical literacy but also understanding how to work with large data sets, how they were produced, how to connect various data sets and how to interpret them. This chapter will focus on the former. The latter is covered in chapter 3.2 Working with data.</p></div>
<div class="paragraph"><p>Innumeracy is a big thing</p></div>
<div class="paragraph"><p>Poynter&#8217;s News University offers classes of Math for journalists, in which reporters get help with concepts such as percentage changes and averages. Interestingly enough, these concepts are being taught simultaneously near Poynter&#8217;s offices, in Floridian schools, to fifth grade pupils (age 10-11), as the curriculum attests.</p></div>
<div class="paragraph"><p>That journalists need help in math topics normally covered before high school shows how far newsrooms are from being data literate. This does not go without problems. How can a data-journalist make use of a bunch of numbers on climate change if she doesn&#8217;t know what a confidence interval means? How can a data-reporter write a story on income distribution if he cannot tell the mean from the median?</p></div>
<div class="paragraph"><p>A reporter certainly does not need a degree in statistics to become more efficient when dealing with data. When faced with numbers, a few simple tricks can help her get a much better story. As Max Planck Institute professor Gerd Gigerenzer says, better tools will not lead to better journalism if they are not used with insight.</p></div>
<div class="paragraph"><p>Get data literate in 3 simple steps!</p></div>
<div class="paragraph"><p>Even if you lack any knowledge of math or stats, you can easily become a seasoned data-journalist by asking 3 very simple questions.</p></div>
<div class="paragraph"><p><strong>1. How was the data collected?</strong></p></div>
<div class="paragraph"><p>Amazing GDP growth</p></div>
<div class="paragraph"><p>The easiest way to show off with spectacular data is to fabricate it. It sounds obvious, but data as commonly commented upon as GDP figures can very well be phony. Former British ambassador Craig Murray reports in his book, Murder in Samarkand, that growth rates in Uzbekistan are subject to intense negotiations between the local government and international bodies. In other words, it has nothing to do with the local economy.</p></div>
<div class="paragraph"><p>GDP is used as the number one indicator because governments need it to watch over their main source of income ‚Äì VAT. When a government is not funded by VAT, or when it does not make its budget public, it has no reason to collect GDP data and will be better-off fabricating them.</p></div>
<div class="paragraph"><p>Crime is always on the rise</p></div>
<div class="paragraph"><p>"Crime in Spain grew by 3%", writes El Pais. Brussels is prey to increased crime from illegal aliens and drug addicts, says RTL. This type of reporting based on police-collected statistics is common, but it doesn&#8217;t tell us much about violence.</p></div>
<div class="paragraph"><p>We can trust that within the European Union, the data isn&#8217;t tampered with. But police personnel respond to incentives. When performance is linked to elucidation rate, for instance, policemen have an incentive to reports as much as possible on incidents that don&#8217;t require an investigation. One such crime is smoking pot. This explains why drug-related crimes in France increased fourfold in the last 15 years while consumption remained constant.</p></div>
<div class="paragraph"><p>What you can do</p></div>
<div class="paragraph"><p>When in doubt about a number&#8217;s credibility, always double check, just as you&#8217;d have if it had been a quote from a politician. In the Uzbek case, a phone call to someone who&#8217;s lived there for a while suffices (<em>Does it feel like the country is 3 times as rich as it was in 1995, as official figures show?</em>).</p></div>
<div class="paragraph"><p>For police data, sociologists often carry out <em>victimization</em> studies, in which they ask people if they are subject to crime. These studies are much less volatile than police data. Maybe that&#8217;s the reason why they don&#8217;t make headlines.</p></div>
<div class="paragraph"><p>Other tests let you assess precisely the credibility of the data, such as Benford&#8217;s law, but none will replace your own critical thinking.</p></div>
<div class="paragraph"><p><strong>2. What&#8217;s in there to learn?</strong></p></div>
<div class="paragraph"><p>Risk of Multiple Sclerosis doubles when working at night</p></div>
<div class="paragraph"><p>Surely any German in her right mind would stop working night shifts after reading this headline. But the article doesn&#8217;t tell us what the risk really is in the end.</p></div>
<div class="paragraph"><p>Take 1,000 Germans. A single one will develop MS over his lifetime. Now, if every one of these 1,000 Germans worked night shifts, the number of MS sufferers would jump to 2. The additional risk of developing MS when working in shifts is 1 in 1,000, not 100%. Surely this information is more useful when pondering whether to take the job.</p></div>
<div class="paragraph"><p>On average, 1 in every 15 Europeans totally illiterate</p></div>
<div class="paragraph"><p>The above headline looks frightening. It is also absolutely true. Among the 500 million Europeans, 36 million probably don&#8217;t know how to read. As an aside, 36 million are also under 7 (data from Eurostat).</p></div>
<div class="paragraph"><p>When writing about an average, always think "an average of what?" Is the reference population homogeneous? Uneven distribution patterns explain why most people drive better than average, for instance. Many people have zero or just one accident over their lifetime. A few reckless drivers have a great many, pushing the average number of accidents way higher than what most people experience. The same is true of the income distribution: most people earn less than average.</p></div>
<div class="paragraph"><p>What you can do</p></div>
<div class="paragraph"><p>Always take the distribution and base rate into account. Checking for the mean and median, as well as mode (the most frequent value in the distribution) helps you gain insights in the data. Knowing the order of magnitude makes contextualization easier, as in the MS example. Finally, reporting in natural frequencies (1 in 100) is way easier for readers to understand that using percentage (1%).</p></div>
<div class="paragraph"><p><strong>3. How reliable is the information?</strong></p></div>
<div class="paragraph"><p>The sample size problem</p></div>
<div class="paragraph"><p>"80% dissatisfied with the judicial system", says a survey reported in Zaragoza-based Diaro de Navarra. How can one extrapolate from 800 respondents to 46 million Spaniards? Surely this is full of hot air.</p></div>
<div class="paragraph"><p>When researching a large population (over a few thousands), you rarely need more than a thousand respondents to achieve a margin of error under 3%. It means that if you were to retake the survey with a totally different sample, 9 times out of 10, the answers you&#8217;ll get will be within a 3% interval of the results you had the first time around. Statistics are a powerful thing, and sample sizes are rarely to blame in dodgy surveys.</p></div>
<div class="paragraph"><p>Drinking tea lowers the risk of stroke</p></div>
<div class="paragraph"><p>Articles about the benefits of tea-drinking are commonplace. This short item in Die Welt saying that tea lowers the risk of myocardial infarction is no exception. Although the effects of tea are seriously studied by some, many pieces of research fail to take into account lifestyle factors, such as diet, occupation or sports.</p></div>
<div class="paragraph"><p>In most countries, tea is a beverage for the health-conscious upper classes. If researchers don&#8217;t control for lifestyle factors in tea studies, they tell us nothing more than <em>rich people are healthier - and they probably drink tea</em>.</p></div>
<div class="paragraph"><p>What you can do</p></div>
<div class="paragraph"><p>The math behind correlations and error margins in the tea studies are certainly correct, at least most of the time. But if researchers don&#8217;t look for co-correlations (e.g. drinking tea correlates with doing sports), their results are of little value.</p></div>
<div class="paragraph"><p>As a journalist, it makes little sense to challenge the numerical results of a study, such as the sample size, unless there are serious doubts about it. However, it is easy to see if researchers failed to take into account relevant pieces of information.</p></div>
<div class="paragraph"><p>Read on</p></div>
<div class="paragraph"><p>Just as you wouldn&#8217;t trust a person you&#8217;ve just met on the street (or in government), don&#8217;t trust data you&#8217;ve just been introduced to. The resources below will give you more tools to assess the credibility of your data:</p></div>
<div class="paragraph"><p>Tools</p></div>
<div class="paragraph"><p>Section 3.3 of this book
Excel for data journalists: How do simple operations (averages, proportions) in Excel.</p></div>
<div class="paragraph"><p>Blogs</p></div>
<div class="paragraph"><p>XKCD: Nerdy strips that often make use of statistical concepts.
Understanding uncertainty: Analysis of risk-related current events.</p></div>
<div class="paragraph"><p>Books</p></div>
<div class="paragraph"><p>Calculated Risks, by Gerd Gigerenzer. The most comprehensive piece on how to communicate risks effectively and how to understand what a percentage really means. If you want to be able to debunk PR mystifications, that&#8217;s the place to start.
The Tiger that isn&#8217;t, by Andrew Dilnot &amp; Michael Blastland. Lots of examples (from the UK) examining journalistic failures at fending off data-driven PR.
Numbers Rule Your World, by Kaiser Fung. The easier-to-read version of Gigerenzer&#8217;s book. Through 6 in-depth examples, it introduces you to similar concepts but remains shallow.
Proofiness, by Charles Seife. Explains in an enjoyable way how numbers can be fabricated ‚Äì and how you can deconstruct them.</p></div>
</div>
<div class="sect2">
<h3 id="_tips_for_working_with_numbers_in_the_news_from_michael_blastland_bbc">Tips for working with numbers in the news from Michael Blastland (BBC)</h3>
<div class="olist arabic"><ol class="arabic">
<li>
<p>
The best tip for handling data is to enjoy yourself. Data can appear forbidding. But allow it to intimidate you and you&#8217;ll get nowhere. Treat it as something to play with and explore and it will often yield secrets and stories with surprising ease. So handle it simply as you&#8217;d handle other evidence, without fear or favour. In particular, think of this as an exercise in imagination. Be creative by thinking of the alternative stories that might be consistent with the data and explain it better, then test them against more evidence. <em>What other story could explain this?</em> is a handy prompt to think about how this number, this obviously big or bad number, this clear proof of this or that, might be nothing of the sort.
</p>
</li>
<li>
<p>
Don&#8217;t confuse scepticism about data with cynicism. Scepticism is good; cynicism has simply thrown up its hands and quit. If you believe in data journalism, and you probably do or you wouldn&#8217;t be reading this book, then you must believe that data has something far better to offer than the lies and damned lies of caricature or the killer facts of swivel-eyed headlines. Data often give us profound knowledge, if used carefully. We need to be neither cynical nor na√Øve, but alert.
</p>
</li>
<li>
<p>
If I tell you that drinking has gone up during the recession, you might tell me it&#8217;s because everyone is depressed. If I tell you that drinking is down, you might tell me it&#8217;s because everyone is broke. In other words, what the data says makes no difference to the interpretation that you are determined to put on it, namely that things are terrible one way or the other. If it goes up, it&#8217;s bad, if it goes down, it&#8217;s bad. The point here is that if you believe in data, try to let it speak before you slap on your own mood, beliefs or expectations. There&#8217;s so much data about that you will often be able to find confirmation of your prior beliefs if you simply look around a bit. In other words, data journalism, to me at least, adds little value if you are not open-minded. It is only as objective as you strive to make it, and not by virtue of being based on numbers.
</p>
</li>
<li>
<p>
Uncertainty is ok. We associate numbers with authority and certainty. Often as not, the answer is that there is no answer, or the answer may be the best we have but still wouldn&#8217;t hit a barn door for accuracy. I think we should say these things. If that sounds like a good way of killing stories, I&#8217;d argue that it&#8217;s a great way of raising new questions. Equally, there can often be more than one legitimate way of cutting the data. Numbers don&#8217;t have to be either true or false.
</p>
</li>
<li>
<p>
The investigation is a story. The story of how you tried to find out can make great journalism, as you go from one piece of evidence to another - and this applies in spades to the evidence from data, where one number will seldom do. Different sources provide new angles, new ideas, richer understanding. I wonder if we&#8217;re too hung up on wanting to be authoritative and tell people the answer - and so we miss a trick by not showing the sleuthing.
</p>
</li>
<li>
<p>
The best questions are the old ones: is that really a big number? Where did it come from? Are you sure it counts what you think it counts? These are generally just prompts to think around the data, the stuff at the edges that got squeezed by looking at a single number, the real-life complications, the wide range of other potential comparisons over time, group or geography; in short, context.
</p>
</li>
</ol></div>
<div class="paragraph"><p>To add:
<a href="http://blogs.channel4.com/factcheck/welcome-to-the-new-factcheck-blog/18">http://blogs.channel4.com/factcheck/welcome-to-the-new-factcheck-blog/18</a></p></div>
</div>
<div class="sect2">
<h3 id="_basic_steps_in_working_with_data_by_steve_doig_walter_cronkite_school_of_journalism">Basic steps in working with data by Steve Doig (Walter Cronkite School of Journalism)</h3>
<div class="paragraph"><p>Working with data implies identifying, collecting, cleaning, analyzing, using and sharing the data you are using for your story. When working with data, you will find this iterative and potentially messy. Sometimes you will know exactly what you are looking for, and sometimes you will go fishing. Occasionally a story will emerge from the data. Often, you will be able to complement a story with more hard evidence or helpful information so readers can understand better.</p></div>
<div class="paragraph"><p>There are several key concepts you need to understand when starting a data project:
Data requestscollection should begin with a list of questions you want to answer.
Data often is messy and needs to be cleaned.
Data may have undocumented features
Be aware of the provenance and terms of use of your data (see chapter 2. Getting Data)
Understand the limitations of your data (Is it reliable? Is it accurate?) See section 3.1 Data Literacy of this handbook)
Be transparent about how you used the data so others can use it.</p></div>
<div class="paragraph"><p><strong>1. Know the questions you want to answer</strong></p></div>
<div class="paragraph"><p>In many ways, working with data is like interviewing a live source. You ask questions of the data and get it to reveal the answers. But just as a source can only give answers about that which he or she has information, a data set can only answer questions for which it has the right records and the proper variables. This means that you should consider carefully what questions you need to answer even before you acquire your data. Basically, you work backwards. First, list the data-evidenced statements you want to make in your story. Then decide which variables and records you would have to acquire and analyze in order to make those statements.
Consider an example involving local crime reports. Let&#8217;s say you want to do a story looking at crime patterns in your city, and the statements you want to make involve the times of day and the days of a week in which different kinds of crimes are most likely to happen, as well as what parts of town are hot spots for various crime categories.
You would realize that your data request has to include the date and the time each crime was reported, the kind of crime (murder, theft, burglary, etc.) as well as the address of where the crime occurred. So Date, Time, Crime Category and Address are the minimum variables you need to answer those questions.
But be aware that there are a number of potentially interesting questions that this four-variable data set CAN&#8217;T answer, like the race and gender of victims, or the total value of stolen property, or which officers are most productive in making arrests. Also, you may only be able to get records for a certain time period, like the past three years, which would mean you couldn&#8217;t say anything about whether crime patterns have changed over a longer period of time. Those questions may be outside of the planned purview of your story, and that&#8217;s fine. But you don&#8217;t want to get into your data analysis and suddenly decide you need to know what percent of crimes in different parts of town are solved by arrest.
One lesson here is that it&#8217;s often a good idea to request ALL the variables and records in the database, rather than the subset that could answer the questions for the immediate story. (In fact, getting all the data can be cheaper than getting a subset, if you have to pay the agency for the programming necessary to write out the subset.) You can always subset the data on your own, and having access to the full data set will let you answer new questions that may come up in your reporting and even produce new ideas for follow-up stories. It may be that confidentiality laws or other policies mean that some variables, such as the identities of victims or the names of confidential informants, can&#8217;t be released. But even a partial database is much better than none, as long as you understand which questions the redacted database can ‚Äì and ‚Äì can&#8217;t answer.</p></div>
<div class="paragraph"><p><strong>2. Find the right data sources</strong></p></div>
<div class="paragraph"><p>Where do you begin? There are a number of ways to get data.
Scraping (See chapter 2.3.1 Scraping data)
Download
FOI request (See section 2.2 Asking for data)
Purchasing datasets</p></div>
<div class="paragraph"><p>For ways to get data see chapter 2. Getting Data.</p></div>
<div class="paragraph"><p><strong>3. Checking the quality of your data and cleaning messy data</strong></p></div>
<div class="paragraph"><p>One of the biggest problems in database work when working with government data is that often you will be using for analysis reasons data that has been gathered for bureaucratic reasons. The problem is that the standard of accuracy for those two is quite different.
For example, a key function of a criminal justice system database is to make sure that defendant Jones is brought from the jail to be in front of Judge Smith at the time of his hearing. For that purpose, it really doesn&#8217;t matter a lot if Jones' birth date is incorrect, or that his street address is misspelled, or even if his middle initial is wrong. Generally, the system still can use this imperfect record to get Jones to Smith&#8217;s courtroom at the appointed time.
But such errors can skew a data journalist&#8217;s attempts to discover the patterns in the database. For that reason, the first big piece of work to undertake when you acquire a new data set is to examine how messy it is and then clean it up.</p></div>
<div class="paragraph"><p>The things you need to check include:
Input mistakes, e.g. letters and other odd characters in columns of digits
Logical errors, e.g. proportions with a percentage greater than 100%</p></div>
<div class="paragraph"><p>A good quick way to look for messiness is to create frequency tables of the categorical variables, the ones that would be expected to have a relatively small number of different values. (When using Excel, for instance, you can do this by using Filter or Pivot Tables on each categorical variable.)
Take "Gender", an easy example. You may discover that your Gender field includes any of a mix of values like these: Male, Female, M, F, 1, 0, MALE, FEMALE, etc., including misspellings like Femal. To do a proper gender analysis, you must standardize ‚Äì Decide on M and F, perhaps, and then change all the variations to match the standards. Another common database with these kinds of problems are American campaign finance records, where the Occupation field might list "Lawyer", "Attorney", "Atty", "Counsel", "Trial Lawyer" and any of a wealth of variations and misspellings; again, the trick is to standardize the occupation titles into a shorter list of possibilities.
Data cleanup gets even more problematic when working with names. Are "Joseph T. Smith", "Joseph Smith", "J.T. Smith", "Jos. Smith" and "Joe Smith" all the same person? It may take looking at other variables like address or date of birth, or even deeper research in other records, to decide. But tools like Google Refine can make the cleanup and standardization task faster and less tedious.
Logical errors, e.g. proportions with a percentage greater than 100%
Anomalous figures, e.g. data points that can&#8217;t possibly be true
Outliers, e.g. isolated or inconsistent data points</p></div>
<div class="paragraph"><p>Recognising anomalous figures requires some prior knowledge on your part, a familiarity with the subject of the data. If you were collecting the total number of US states won by candidate in the American presidential election, for example, you wouldn&#8217;t expect to see Arnold Schwarzenegger on the list (however much you might want it to be true).</p></div>
<div class="paragraph"><p>Correlation between two variables can help detect outliers in your data. Deciding when to ignore them in your analysis can be tricky, however. The human brain detects meaningful patterns in random data, which means we can be tempted to remove outliers to create a clearer picture of what we hope to find. If you keep looking hard enough at the data, patterns will emerge from the noise. The danger is that you&#8217;ll manipulate your data to fit the story you&#8217;re looking for, cherry-picking the points that help support a result you&#8217;re hoping to get, rather than giving the true story.</p></div>
<div class="paragraph"><p><strong>4. Data may have undocumented features</strong></p></div>
<div class="paragraph"><p>The Rosetta Stone of any database is the so-called data dictionary. Typically, this file (it may be text or pdf or even a spreadsheet) will tell you how the data file is formatted (delimited text, fixed width text, Excel, dBase, et al.), the order of the variables, the names of each variable and the datatype of each variable (text string, integer, decimal, et al.) You will use this information to help you properly import the data file into the analysis software you intend to use (Excel, Access, SPSS, Fusion Tables, any of various flavors of SQL, et al.)
The other key element of a data dictionary is an explanation of any codes being used by particular variables. For instance, Gender may be coded so that 1=Male and 0=Female. Crimes may be coded by your jurisdiction&#8217;s statute numbers for each kind of crime. Hospital treatment records may use any of hundreds of 5-digit codes for the diagnoses of the conditions for which a patient is being treated. Without the data dictionary, these data sets could be difficult or even impossible to analyze properly.
But even with a data dictionary in hand, there can be problems. An example happened to reporters at the Miami Herald in Florida some years ago when they were doing an analysis of the varying rates of punishment that different judges were giving to people arrested for driving while intoxicated. The reporters acquired the conviction records from the court system and analyzed the numbers in the three different punishment variables in the data dictionary: Amount of prison time given, amount of jail time given, and amount of fine given. These numbers varied quite a bit amongst the judges, giving the reporters' evidence for a story about how some judges were harsh and some were lenient.
But for every judge, about 1-2 percent of the cases showed no prison time, no jail time and no fine. So the chart showing the sentencing patterns for each judge included a tiny amount of cases as "No punishment," almost as an afterthought. When the story and chart was printed, the judges howled in complaint, saying the Herald was accusing them of breaking a state law that required that anyone convicted of drunk driving be punished.
So the reporters went back to Clerk of the Court&#8217;s office that had produced the data file and asked what had caused this error. They were told that the cases in question involved indigent defendants with first-time arrests. Normally they would be given a fine, but they had no money. So the judges were sentencing them to community service, such as cleaning litter along the roads. As it turned out, the law requiring punishment had been passed after the database structure had been created. So all the court clerks knew that in the data, zeros in each of the the prison-jail-fine variables meant community service. However, this WASN&#8217;T noted in the data dictionary, and therefore caused a Herald correction to be written.
The lesson in this case is to always ask the agency giving you data if there are any undocumented elements in the data, whether it is newly-created codes that haven&#8217;t been included in the data dictionary, changes in the file layout, or anything else. Also, always examine the results of your analysis and ask "Does this make sense?" The Herald reporters were building the chart on deadline and were so focused on the average punishment levels of each judge that they failed to pay attention to the scant few cases that seemed to show no punishment. They should have asked themselves if it made sense that all the judges seemed to be violating state law, even if only to a tiny degree.</p></div>
<div class="paragraph"><p><strong>5. Analyzing your data</strong></p></div>
<div class="paragraph"><p>After collecting data (Chapter 2) it is important to understand your dataset.
What type of analysis do you need to do? What are you comparing?
Time-based
Geographic regions
Cross-sectional: what are the portions of the whole (e.g., by race for census data)</p></div>
<div class="paragraph"><p>Depending on what kind of data analysis you are doing, there are a range of different types of tools you can use. For this see chapter 3.3 Tools for Analysing data.</p></div>
<div class="paragraph"><p>Detecting dodgy data</p></div>
<div class="paragraph"><p>Not all data comes from measurement of real-life numbers. From individuals to organisations, the pressure to support an argument with stats can unfortunately lead people to fabricate, falsify and omit data.</p></div>
<div class="paragraph"><p>Even those who understand the importance of good data aren&#8217;t beyond reproach: for example, 2% of scientists have admitted to tampering with data. This is a conservative estimate based on surveys, so the true number of researchers who have committed fraud through data manipulation may be much higher.</p></div>
<div class="paragraph"><p>So how do you identify dodgy data? Although it&#8217;s impossible to spot fraud simply by looking at numbers directly, luckily there are a few statistical tests that can help you detect anomalies. One of the most practical is a strange statistical phenomenon known as Benford&#8217;s law, or the leading-digit law.</p></div>
<div class="paragraph"><p>According to Benford&#8217;s law, in a list of numbers from natural sources of data, the leading digit (e.g. the 1 in 10) is not distributed randomly. While you might expect the digits 0 to 9 to each be found in 11 per cent of cases, the digit 1 actually appears about 30% of the time. The other digits appear in lower and lower frequencies, following an exponential decline, ending with numbers that have a leading 9 only occurring in under 5% of numbers from the list.</p></div>
<div class="paragraph"><p>This phenomenon appears in many lists of data from natural sources, which means that deviations from Benford&#8217;s law may indicate false data. If you plot each leading digit in a data set against the frequency with which they appear, you should get a graph showing an exponential decline, which is what you would expect from Benford&#8217;s law.</p></div>
<div class="paragraph"><p>To test whether your data is dodgy, all you have to do it take the leading digit from a list of numbers in your data, tally the number of times each digit appears, and then superimpose this over a Benford curve. If your data matches the expected curve, then your data is probably free of fabrication.</p></div>
<div class="ulist"><ul>
<li>
<p>
Link to 3.3 tools for analysing data (more specific on the types of things you can do to mend data with
</p>
</li>
</ul></div>
</div>
<div class="sect2">
<h3 id="_the_32_loaf_of_bread_by_claire_miller_walesonline">The £32 loaf of bread by Claire Miller (WalesOnline)</h3>
<div class="paragraph"><p>A story for Wales on Sunday about how much the Welsh Government is spending on prescriptions for gluten-free products, contained the headline figure that it was paying £32 for a loaf of bread. (<a href="http://www.walesonline.co.uk/news/wales-news/2011/07/17/prescriptions-for-gluten-free-bread-costing-welsh-taxpayers-32-a-loaf-91466-29067430/">http://www.walesonline.co.uk/news/wales-news/2011/07/17/prescriptions-for-gluten-free-bread-costing-welsh-taxpayers-32-a-loaf-91466-29067430/</a>)</p></div>
<div class="paragraph"><p>However, this was actually 11 loaves that cost ¬£2.82 each.</p></div>
<div class="paragraph"><p>The figures, from a Welsh Assembly written answer and a Welsh NHS statistics release, listed the figure as cost per prescription item. However, they gave no additional definition in the data dictionary of what a prescription item might refer or how a separate quantity column might define it.</p></div>
<div class="paragraph"><p>The assumption was that it referred to an individual item, e.g. a loaf of bread, rather than what it actually was, a pack of several loafs.</p></div>
<div class="paragraph"><p>No one, not the people who answered the written answer or the press office, when it was put to them, raised the issue about quantity&#8230;until the Monday after the story was published.</p></div>
<div class="paragraph"><p>So do not assume the people responsible for the data will realise the data is not clear even when you tell them your mistaken assumption.</p></div>
<div class="paragraph"><p>Generally newspapers want things that make good headlines, so unless something obviously contradicts an interpretation, t is usually easier to go with what makes a good headline and not check too closely and risk the story collapsing, especially on deadline.</p></div>
<div class="paragraph"><p>But the responsibility is to check the ridiculous claims even if it drops the story down the newslist.</p></div>
</div>
<div class="sect2">
<h3 id="_fresh_out_of_the_packet_data_journalists_discuss_their_tools_of_choice">Fresh out of the packet. Data journalists discuss their tools of choice.</h3>
<div class="paragraph"><p>Psssss. That is the sound of your data decompressing from its airtight wrapper. Now what? What do you look for? And what tools do you use to get stuck in?</p></div>
<div class="paragraph"><p>Infobox:
Tools of choice at the Chicago Tribune
by Brian Boyer</p></div>
<div class="paragraph"><p>Our tools of choice include Python and Django. For hacking, scraping and playing with data, and PostGIS, QGIS and the MapBox toolkit for building crazy web maps. R and NumPy + MatPlotLib are currently battling for supremacy as our kit of choice for exploratory data analysis, though our favorite data tool of late is homegrown: CSVKit. More or less everything we do is deployed in the cloud.</p></div>
<div class="paragraph"><p>Todo:</p></div>
<div class="ulist"><ul>
<li>
<p>
Reorganise notes below into a section with concise and compelling anecdotes rather than a list of software packages and web services
</p>
</li>
</ul></div>
<div class="paragraph"><p>To ask:</p></div>
<div class="ulist"><ul>
<li>
<p>
Friedrich Lindenberg
</p>
</li>
<li>
<p>
KNB
</p>
</li>
<li>
<p>
Simon Rogers
</p>
</li>
<li>
<p>
NYT
</p>
</li>
<li>
<p>
Tony @ OUseful
</p>
</li>
<li>
<p>
Examples for how people used different tools - e.g. ScraperWiki, Google Refine, Google Motion Charts, etc.
</p>
</li>
</ul></div>
<div class="paragraph"><p># Tools and techniques for analysing data #</p></div>
<div class="paragraph"><p>3.3 Tools, tutorials and case studies for all steps of the data journalism process</p></div>
<div class="paragraph"><p>Overview: Overview of different types of tools for analysing and working with datasets, examples of how they can be used, examples of how they have been used by journalists.
Authors: Liliana Bounegru, Lucy Chambers, Claire Miller
Length: 1-2 pages per case study</p></div>
<div class="paragraph"><p><strong>1. Collection + 2. Filtering</strong></p></div>
<div class="paragraph"><p><strong>1. Tools for data collection, storing, cleaning, structuring, quality checking and annotation</strong></p></div>
<div class="paragraph"><p><strong>2. Tools for data filtering, interrogation, combination and analysis</strong>
Beginner level
Spreadsheets
Manual on Excel and Pivot Tables: <a href="http://www.tcij.org/training-material/data-journalism">http://www.tcij.org/training-material/data-journalism</a>
Google Spreadsheets
Tutorial Excel: <a href="http://blog.buzzdata.com/post/11607498580/visualizing-torontos-water-usage-a-tutorial">http://blog.buzzdata.com/post/11607498580/visualizing-torontos-water-usage-a-tutorial</a>
Google Refine
Stanford Data Wrangler
Data Patterns
How journalists can use Google Refine to clean <em>dirty</em> data sets
The Overview Project
<a href="http://googlerefine.blogspot.com/">http://googlerefine.blogspot.com/</a>
Cleaning data with Google Refine
<a href="http://www.opendatacookbook.net/wiki/recipe/sliced_and_diced_aid_data_with_google_refine">http://www.opendatacookbook.net/wiki/recipe/sliced_and_diced_aid_data_with_google_refine</a>
<a href="http://www.propublica.org/nerds/item/using-google-refine-for-data-cleaning">http://www.propublica.org/nerds/item/using-google-refine-for-data-cleaning</a>
Combining data with Google Refine
Databases
Microsoft Access / Filemaker (Mac)
open-source SQL database managers
SQL Tutorial
A Gentle Introduction to SQL Using SQLite
Introducing SQL for Lightweight Data Manipulation
Structured Query Language (SQL): an introduction
NoSQL Databases
Key Value Store
Document Store
Google Fusion Tables
demo in Open Data Cookbook
What options exist for displaying icons, lines and polygon colors on the map?
How to map in Fusion Tables: a basic tutorial
How to combine multiple Fusion Tables into one map
Answering some FAQs about Fusion Tables</p></div>
<div class="paragraph"><p>Yahoo Pipes:
Introduction to Yahoo Pipes
Learn How to Build a Pipe in Just a Few Minutes on Yahoo!
Discovering Co-location Communities ‚Äì Twitter Maps of Tweets Near Wherever‚Ä¶</p></div>
<div class="paragraph"><p>XML</p></div>
<div class="paragraph"><p><a href="http://onlinejournalismblog.com/2011/08/05/sftw-asking-questions-of-a-webpage-and-finding-out-when-those-answers-change/">http://onlinejournalismblog.com/2011/08/05/sftw-asking-questions-of-a-webpage-and-finding-out-when-those-answers-change/</a></p></div>
<div class="paragraph"><p>Advanced level
Graph Database
Graph/Network Analysis
Comparing data links
UCINet, Gephi, NodeXL
Example: Social networks
R
R for Statistics: First Steps (PDF) by Peter Aldhous, Hands-on R, a step-by-step tutorial (PDF) by Jacob Fenton, and the project&#8217;s own An Introduction to R.
The R Statistics blog has a number of visualization samples.
Unlocking Big Data with R
Power Tools for Aspiring Data Journalists: Funnel Plots in R
Book for journalists: Where are the bodies buried on the web? Big data for journalists: <em>This short guide will cover my favorite resources, along with a few examples of how they&#8217;ve been used to create compelling journalism.</em>
CSVKit ("Got a fixed-width file and wish it was easier to work with? Look at FFS It&#8217;s a list of schemas for converting fixed-width files to CSV using this tool)</p></div>
<div class="paragraph"><p>Network Analysis
Download this detailed free NodeXL tutorial (PDF) or these basic step-by-step instructions on analyzing your own Facebook social network (PDF).
Following the money: making networks visible with HTML5</p></div>
<div class="paragraph"><p>Expert level
Statistical analysis
Searching for complex relationships, useful when working with a subset of a data
SAS
SPSS
R</p></div>
<div class="paragraph"><p><strong>3. Tools for publishing data, visualising data and building news apps</strong></p></div>
<div class="paragraph"><p>Mapping and visualisation
Geocoding
GPS Visualiser
How to: convert easting/northing into lat/long for an interactive map
Postcode Finder</p></div>
<div class="paragraph"><p><strong>4. Tools for distributing your data projects</strong></p></div>
<div class="paragraph"><p>Twitter</p></div>
<div class="paragraph"><p><strong>5. Tools for analysing the impact of your data projects</strong></p></div>
<div class="paragraph"><p>Google Analytics</p></div>
<div class="paragraph"><p>add propublica tutorials to this chapter</p></div>
<div class="paragraph"><p>Add: Edward Borasky DDJ Develper Studio - open sources - ask Liliana</p></div>
</div>
<div class="sect2">
<h3 id="_start_with_the_data_finish_with_a_story_by_caelainn_barr_eu_data_journalist">Start with the data, finish with a story by Caelainn Barr (EU Data Journalist)</h3>
<div class="paragraph"><p>Approach 1: Think Headline</p></div>
<div class="paragraph"><p>To draw your readers in you have to be able to hit them with a headline figure that makes them sit up and take notice. You should almost be able to read the story without having to know that it comes from a dataset.</p></div>
<div class="paragraph"><p>Make it exciting and remember who your audience are as you go.</p></div>
<div class="paragraph"><p>One example of this can be found in a project carried out by the Bureau of Investigative Journalism using the EU Commission&#8217;s Financial Transparency System. The story was constructed by approaching the data set with specific queries in mind.</p></div>
<div class="paragraph"><p>We looked through the data for key terms like <em>cocktail</em>, <em>golf</em> and <em>away days</em>. This allowed us to determine what the Commission had spent on these items and raised plenty of questions and story lines to follow up.</p></div>
<div class="paragraph"><p>But key terms don&#8217;t always give you what you want, sometimes you have to sit back and think about what you&#8217;re really asking for. During this project we also wanted to find out how much commissioners spent on private jet travel but the as the data set didn&#8217;t contain the phrase <em>private jet</em> we had to get the name of their travel providers by other means. Once we knew the name of the service provider to the Commission, <em>Abelag</em>, we were able to query the data to find out how much was being spent on services provided by Abelag.</p></div>
<div class="paragraph"><p>In this approach we had a clearly defined objective in querying the data; to find a figure that would provide a headline-the colour followed.</p></div>
<div class="paragraph"><p>Approach 2: Approach with a blacklist and look for exclusions</p></div>
<div class="paragraph"><p>An easy way to pull storylines from data is to know what you shouldn&#8217;t find in there!
A good example of how this can work is illustrated by the collaborative EU Structural Funds project between the Financial Times and the Bureau of Investigative Journalism.</p></div>
<div class="paragraph"><p>We queried the data, based on the Commission&#8217;s own rules about what kinds of companies and associations should be prohibited from receiving structural funds. One example was expenditure on tobacco and tobacco producers.</p></div>
<div class="paragraph"><p>By querying the data with the names of tobacco companies, producers and growers we found data that revealed British American Tobacco were receiving ‚Ç¨1.5m for a factory in Germany.</p></div>
<div class="paragraph"><p>As the funding was outside the rules of Commission expenditure, it was a quick way to find a story in the data.</p></div>
<div class="paragraph"><p>Approach 3: Just explore</p></div>
<div class="paragraph"><p>You never know what you might find in a dataset, so just have a look. You have to be quite bold and this approach generally works best when trying to identify obvious characteristics that will show up through filtering (the biggest, extremes, most common etc.).</p></div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_6_delivering_data">6. Delivering data</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_cunning_title_for_data_visualisation_chapter_by_geoff_mcghee_stanford_university">Cunning Title for data visualisation chapter by Geoff McGhee (Stanford University)</h3>
<div class="paragraph"><p>Roles of visualisation in journalism
what function(s) visualisations play in reportage (what do journalists use visualisations for): (1) to find stories, (2) to tell a story</p></div>
<div class="paragraph"><p>Tools, tutorials and good examples of using visualisations to find stories
When do you need to visualise a dataset to explore it and find a story? When don&#8217;t you need to?
How do you go about discovering a story? What tools do you use? What "protocol" do you follow? What clues do you follow, what do you pay attention to? (lessons, tips, advice).
Examples of how to explore a dataset with a visualisation tool with a step by step description of the "protocol" followed to find the story.</p></div>
<div class="paragraph"><p>Tools, tutorials and good examples of using visualisations to tell stories
When do you need to visualise a story and when don&#8217;t you need to?
What types of visualisations are good for presenting what types of stories?
How do you go about visualising a story? What tools do you use? What steps do you take? (lessons, tips, advice).
What makes a good visualisation, what makes a bad visualisation?
Examples of good and base use of visualisations to tell a story with explanation of what makes them a good/bad case.</p></div>
<div class="paragraph"><p>Note: The aim of this chapter is not to show journalists how to do a data visualisation but to explain when a visualisation could be useful in their work, what could visualisations help them with, how they could assess the quality of a visualisation, getting them familiar with the vocabulary so they know what to ask for from designers, getting them familiar, introducing and showing them how to use visualisation tools for non-experts.
Authors: David Erwin (New York Times), Aron Pilhofer (New York Times), Farida Vis (University of Leicester), Kate Hudson (openjournalism.ca), Lulu Pinney (infographics specialist), Mariano Blejman (Hacks/Hackers), Sarah Cohen (Knight Professor of the Practice of Journalism and Public Policy, Sanford)
Length: 1-2 pages per case study
Editor: Liliana Bounegru (European Journalism Centre)</p></div>
<div class="paragraph"><p>The Data Journalism Handbook
4.3 Visualising data
4.3.1 Roles of visualisation in journalism
4.3.2 Tools, tutorials and good examples of using visualisations to find stories
4.3.3 Tools, tutorials and good examples of using visualisations to tell stories
When to use and when not to use visualisations to tell a story
Types of stories visualisations can tell
Which is the right visualisation for your story?
Examples of good use of visualisations to tell stories
When is a visualisation good and when is it bad?
Toolkits and tutorials</p></div>
<div class="paragraph"><p>Roles of visualisation in journalism</p></div>
<div class="paragraph"><p>(Lulu Pinney)
The function of data visualisations in reportage is twofold. Firstly they are a tool for exploring the data to find where any stories are. Their second use is for telling a story. And it is worth noting that - for the same set of data - it is more than likely that the visuals best suited to finding a story will not be best suited to the telling of it.</p></div>
<div class="paragraph"><p>Using visualisations to explore data is a more objective exercise, and sometimes it is appropriate to create an interface so your audience can have a go themselves. Alternatively having explored and interpreted the data yourself, you can choose just to present the stories you have found in a more subjective - or editorial - manner to your audience, with or without supporting visualisations.</p></div>
<div class="paragraph"><p>4.3.2 Tools, tutorials and good examples of using visualisations to find stories</p></div>
<div class="paragraph"><p>When do you need to visualise a dataset to explore it and find a story? When don&#8217;t you need to?
How do you go about discovering a story? What tools do you use? What "protocol" do you follow? What clues do you follow, what do you pay attention to? (lessons, tips, advice).
Examples of how to explore a dataset with a visualisation tool with a step by step description of the "protocol" followed to find the story.
Case study</p></div>
<div class="paragraph"><p>4.3.3 Tools, tutorials and good examples of using visualisations to tell stories</p></div>
<div class="paragraph"><p>When do you need to visualise a story and when don&#8217;t you need to?
What types of visualisations are good for presenting what types of stories?
How do you go about visualising a story? What tools do you use? What steps do you take? (lessons, tips, advice).
One important step: user-testing prototype visualisations.
When is a visualisation good, when is a visualisation bad?
Examples of good and base use of visualisations to tell a story with explanation of what makes them a good/bad case.</p></div>
</div>
<div class="sect2">
<h3 id="_using_data_visualisation_to_find_insights_in_data_by_gregor_aisch_open_knowledge_foundation">Using data visualisation to find insights in data by Gregor Aisch (Open Knowledge Foundation)</h3>
<div class="paragraph"><p>Visualization is critical to data analysis. It provides a front line of attack, revealing intricate structure in data that cannot be absorbed in any other way. We discover unimagined effects, and we challenge imagined ones.</p></div>
<div class="paragraph"><p>(quote by William S. Cleveland in the preface of his book <em>Visualising data</em>)</p></div>
<div class="paragraph"><p>Everything is visualised
Q: When do you need to visualise a dataset to explore it and find a story? When donÔøΩt you need to?</p></div>
<div class="paragraph"><p>Data by itself, consisting of bits and bytes stored in a file on a computer hard-drive, is invisible. In order to be able to see and make any sense of data, we need to visualise it. In this chapter I&#8217;m going to use a broader understanding of the term visualising, that includes even pure textual representations of data. For instance, just loading a dataset into a spreadsheet software can be considered as data visualisation. The invisible data suddenly turns into a visible <em>picture</em> on our screen. Thus, the questions should not be whether journalists need to visualise data or not, but which kind of visualisation may be the most useful in which situation. In other words: when does it makes sense to go beyond the table visualisation.</p></div>
<div class="paragraph"><p>The short answer is: almost always. Tables alone are definitely not sufficient to give us some kind of overview or big picture of a dataset. Also tables alone don&#8217;t allow us to immediately identify patterns within the data. The most common example here are geographical patterns which can only be observed after visualizing data on a map. But there are also other kinds of patterns which we will see later in this chapter.</p></div>
<div class="paragraph"><p>How can visualisation help us to discover a story in a dataset?
How do you go about discovering a story? What tools do you use? What ÔøΩprotocolÔøΩ do you follow? What clues do you follow, what do you pay attention to? (lessons, tips, advice).</p></div>
<div class="paragraph"><p>Well, discovering a story through visualisation is a rather big goal to start with. In fact, there is no set of rules or "protocol" that will guarantee us to discover a story. Instead, I think it makes more sense to seek for smaller chunks of knowledge, from now on called insights.</p></div>
<div class="paragraph"><p>Every new visualisation is likely to give us some insights into our data. Some of those insights might be already known (but maybe not proven yet) while other insights might be completely new or even surprising to us. While some of those new insights might actually mean the beginning of a story, others could just be the result of errors in the data.</p></div>
<div class="paragraph"><p>In order to make the finding of insights in data more effective, I find the following process very helpful:</p></div>
<div class="paragraph"><p>Each of these steps will be discussed further in this section.</p></div>
<div class="paragraph"><p>Visualise data
Visualising data is the central point of this process. Each visualisation provides a unique perspective on the dataset.</p></div>
<div class="paragraph"><p>Some of the most important types of visualisations:</p></div>
<div class="paragraph"><p>table - displaying the raw numbers
charts - (<a href="http://www.flickr.com/photos/amit-agarwal/3196386402/sizes/l/">http://www.flickr.com/photos/amit-agarwal/3196386402/sizes/l/</a>)
maps - displaying geographical context
graphs - displaying relations in networks</p></div>
<div class="paragraph"><p>Todo:</p></div>
<div class="paragraph"><p>explain the vis types in more detail (provide examples)
maybe mention a few tools which can be used for data vis.
Analyse and interpret what you see
Once you have visualized your data, the next crucial step is to learn something from the picture you created. Of course, you can&#8217;t learn nothing from looking at a visualization. But you should take yourself some time to really get into what you see. We need to ask us the following questions:</p></div>
<div class="paragraph"><p>What can I see in this image? Are there any patterns?
What does this mean in the context of the data?
Sometimes you might end up with visualization that, despite of eventual beauty, seem to tell you nothing of value about your data. But, again, for this process it&#8217;s important that you force yourself to identify some new insights.</p></div>
<div class="paragraph"><p>Document your insights and next steps
If you think of this process as a journey through the dataset, the documentation is your travel diary. It will tell you where you have traveled to, what you have seen there and how you made your decisions for your next steps. You can even start your documentation before taking the first look at the data.</p></div>
<div class="paragraph"><p>In most cases when we start to work with a previously unseen dataset we are already filled up with expectations and assumptions about the data. Obviously, there is some kind of reason why we are interested that dataset. It&#8217;s a good idea to start the documentation by writing down these initial thoughts. This helps us to identify our bias and reduces the risk of mis-interpretation of the data by just finding what we actually wanted to find.</p></div>
<div class="paragraph"><p>I really think that the documentation is the most important step of the process; and it is also the one we&#8217;re most likely to tend to skip. As you will see in the following sample session, the described process involves a lot of plotting and data wrangling. Looking at a set of 15 charts you created might be very confusing, especially after some time has passed. In fact, those charts are only valuable (to you or any other person you want to communicate your findings) if presented in the context in which they have been created.</p></div>
<div class="paragraph"><p>Therefor you should take the time to document a few things while you are going to this process. The goal is not to write epic stories, but to keep notes about some really important questions:</p></div>
<div class="paragraph"><p>Why have I created this chart?
What have I done to the data to create it?
What insights have I found from this chart?
You can choose whatever format is convenient to you. Every tool that allows you to insert a sequence of texts and images (like PowerPoint or Word does) is perfect.</p></div>
<div class="paragraph"><p>Transform data
Naturally, with your new insights gathered from the last visualization you might have an idea of what you want to see next. You might have found some interesting pattern in the dataset which you now want to inspect in more detail.</p></div>
<div class="paragraph"><p>Possible transformations are:</p></div>
<div class="paragraph"><p>zoom into a. (e.g. limit to a certain time span)
filter
aggregate
outlier removal
Now the time has come where we can really move on. The good news is that we&#8217;ve already solved the hardest question of moving on, that is: in which direction should be move on? Remember that insight you got from the last visualization? The one you&#8217;ve written down in you story documentation? The next step is trying to remove this insight from our data. The theory behind this is that every insight in your data might prevent us from seeing some other insights. Removing something we already know is no problem, as long as we&#8217;re keeping everything for our records. And this is what I call the transform step.</p></div>
<div class="paragraph"><p>Which tools to use
Every data visualization tool available is good at something. However, here are a few requirements for choosing the right tools:</p></div>
<div class="paragraph"><p>Visualisation and data wrangling should be easy and cheap. If changing parameters of the visualizations takes you hours, you won&#8217;t experiment that much. That doesn&#8217;t necessarily mean that you don&#8217;t need to learn how to use the tool. But once you learned it, it should be really efficient.
For some reasons it makes much sense to choose a tool that covers both the data wrangling and the data visualisation issues. Separating the tasks in different tools means that you have to import and export your data very often.
The sample visualizations in the next section were created using the R, which is kind of the swiss army knife of scientific data visualisation.</p></div>
<div class="paragraph"><p>Example session: making sense of US election contribution data
Examples of how to explore a dataset with a visualisation tool with a step by step description of the ÔøΩprotocolÔøΩ followed to find the story.</p></div>
<div class="paragraph"><p>Let us have look at the US Presidential Campaign Finance database which contains about 450,000 contributions to US Presidential candidates. The CSV file is 60 megabytes and way to large to handle it easily in Excel.</p></div>
<div class="paragraph"><p>In the first step I will explicitly write down my initial assumptions on the FEC contributions dataset:</p></div>
<div class="paragraph"><p>Obama get&#8217;s the most contributions (since he&#8217;s the president and has the highest popularity)
The number of donations increases as the time moves closer to election date.
Obama gets more small donations than republican candidates.
To answer the first question we need to transform the data. Instead of each single contributions we need to sum the total amounts contributed to each candidate. After visualising the results in a sorted table we can confirm our assumption that Obama would raise the most money.</p></div>
<div class="paragraph"><p>Candidate        Amount ($)
Obama, Barack    72,453,620.39
Romney, Mitt     50,372,334.87
Perry, Rick      18,529,490.47
Paul, Ron        11,844,361.96
Cain, Herman     7,010,445.99
Gingrich, Newt   6,311,193.03
Pawlenty, Timothy        4,202,769.03
Huntsman, Jon    2,955,726.98
Bachmann, Michelle       2,607,916.06
Santorum, Rick   1,413,552.45
Johnson, Gary Earl       413,276.89
Roemer, Charles E. <em>Buddy</em> III   291,218.80
McCotter, Thaddeus G     37,030.00
But despite of showing the minimum and maximum amounts and the order the table does not tell very much about the underlying patterns in candidate ranking. Here is another view on the data, a chart type that is called dot chart in which we can see everything that is shown in the table plus the patterns within the field. For instance, the dot chart allows us to immediately compare the distance between Obama and Romney and Romney and Perry without needing to subtract values. (Footnote: The shown dot chart was created using R. You can find links to the source codes at the end of this chapter).</p></div>
<div class="paragraph"><p>Now, let us proceed with a bigger picture of the dataset. As a first step I visualised all contributed amounts over time in a simple plot. We can see that almost all donations are very very small compared to three really big outliers. Further investigation returns that these huge contribution are coming from the "Obama Victory Fund 2012" (also known as Super PAC) and were made on June 29th ($450k), September 29th ($1.5mio) and December 30th ($1.9mio).</p></div>
<div class="paragraph"><p>While the contributions by Super PACs alone is undoubtedly the biggest story in the data, it might be also interesting to look beyond it. The point now is that these big contributions disturb our view on the smaller contributions coming from individuals, so we&#8217;re going to remove them from the data. This transform is commonly known as outlier removal. After visualizing again, we can see that most of the donations are within the range of $10k and -$5k.</p></div>
<div class="paragraph"><p>According to the contribution limits placed by the FECA individuals are not allowed to donate more than $2500 to each candidate. As we see in the plot, there are numerous donations made above that limit. In particular two big contributions in May attract our attention. It seems that they are <em>mirrored</em> in negative amounts (refunds) June and July. Further investigation in the data reveals the following transactions:</p></div>
<div class="paragraph"><p>On May 10 Stephen James Davis, San Francisco, employed at Banneker Partners (attorney), has donated $25,800 to Obama.
On May 25 Cynthia Murphy, Little Rock, employed at the Murphy Group (public relations), has donated $33,300 to Obama.
On June 15 the amount of $30,800 was refunded to Cynthia Murphy, which reduced the donated amount to $2500.
On July 8 the amount $25,800 was refunded to Stephen James Davis, which reduced the donated amount to $0.
What&#8217;s interesting about these numbers? The $30,800 refunded to Cynthia Murphy equals the maximum amount individuals may give to national party committees per year. Maybe she just wanted to combine both donations in one transaction which was rejected. The $25,800 refunded to Stephen James Davis possibly equals the $30,800 minus $5000 (the contribution limit to any other political committee).</p></div>
<div class="paragraph"><p>Another interesting finding in the last plot is a horizontal line pattern for contributions to Republican candidates at $5000 and -$2500. To see them in more detail, I visualised just the Republican donations. The resulting graphic is kind of the perfect example for patterns in data that would be invisible without data visualisation.</p></div>
<div class="paragraph"><p>What we can see is that there are many $5000 donations to Republican candidates. In fact, a look up in the data returns that these are 1243 donations, which is only 0.3% of the total number of donations, but since those donations are evenly spread across time, the line appears. The interesting thing about the line is that donations by individuals were limited to $2500. Consequently, every dollar above that limited was refunded to the donors, which results in the second line pattern at -$2500. In contrast, the contributions to Barack Obama don&#8217;t show a similar pattern.</p></div>
<div class="paragraph"><p>So, it might be interesting to find out why thousands of Republican donors did not notice the donation limit for individuals. To further analyze this topic, we can have a look at the total number of $5k donations per candidate.</p></div>
<div class="paragraph"><p>Of course, this is a rather distorted view since it does not consider the total amounts of donations received by each candidate. The next plot shows the percentage of $5k donations per candidate.</p></div>
<div class="paragraph"><p>What learn from this
Often, such a visual analysis of a new dataset feels like an exciting journey to an unknown country. You start as a foreigner with just the data and your assumptions, but with every step you make, with every chart you render, you get new insights about the topic. Based on those insights you make decisions for your next steps and what issues are worth further investigation. As you might have seen in this chapter, this process of visualizing, analyzing and transformation of data could be repeated nearly infinitely.</p></div>
<div class="paragraph"><p>Source codes for the generated graphics
All of the charts shown in this chapter were created using the wonderful and powerful software R. Created mainly as a scientific visualization tool, it is hard to find any visualization or data wrangling technique that is not already built into R. For those who are interested in how to visualise and wrangle data using R, here&#8217;s the source code of the charts generated in this chapter. Also, there is a wide range of books and tutorials available.</p></div>
<div class="paragraph"><p>dotchart: contributions per candidate
plot: all contributions over time
plot: contributions by authorized committees
Further Reading
The author of this chapter has been influenced by the book Visualizing Data by William S. Cleveland, published in 1993.</p></div>
</div>
<div class="sect2">
<h3 id="_when_to_use_and_when_not_to_use_visualisations_to_tell_a_story_by_aron_pilhofer_new_york_times">When to use and when not to use visualisations to tell a story by Aron Pilhofer (New York Times)</h3>
<div class="paragraph"><p>There are times when data can tell a story better than words or photos, and this is why terms like "news application" and "data visualization" have attained buzzword status in so many newsrooms of late. Also fueling interest is the bumper crop of new tools and technologies designed to help even the most technically challenged journalist turn data into a piece of visual storytelling‚Äîmany of them free.</p></div>
<div class="paragraph"><p>Tools like Google Fusion Tables, Many Eyes, Tableau, Dipity and others make it easier than ever to create maps, charts, graphs or even full-blown data applications that heretofore were the domain of specialists. But with the barrier to entry now barely a speed bump, the question facing journalists now less about whether you can turn your dataset into a visualization, but whether you should.</p></div>
<div class="paragraph"><p>Bad data visualization is worse in many respects than none at all, but journalists have few resources (until now) available to learn the right way to approach visual storytelling.</p></div>
<div class="paragraph"><p>Types of stories visualisations can tell</p></div>
<div class="paragraph"><p>NYT&#8217;s deputy graphics director Matthew Ericson provides a list of different functions that visualisations can play in a story (<a href="http://www.ericson.net/content/2011/04/international-journalism-festival-links/">http://www.ericson.net/content/2011/04/international-journalism-festival-links/</a>)</p></div>
<div class="paragraph"><p>Provide Context</p></div>
<div class="paragraph"><p>Satellite Photos - Japan Before and After Tsunami
Compare satellite images of areas of Japan before and after the disaster.
Assessing the Radiation Danger, Near and Far
Assessments of the radiation from the Fukushima Daiichi nuclear power plant by the Japanese authorities, the International Atomic Energy Agency and others.
Map of Radiation Readings Near Fukushima Daiichi
The Jobless Rate for People Like You - Interactive Graphic - NYTimes.com
Not all groups have felt the recession equally.
Obama&#8217;s 2012 Budget Proposal: How It&#8217;s Spent - NYTimes.com
Explore every nook and cranny of President Obama&#8217;s budget proposal.</p></div>
<div class="paragraph"><p>Reveal Patterns</p></div>
<div class="paragraph"><p>Interactive Map Showing Immigration Data Since 1880
See how foreign-born groups settled in your area and across the United States from 1880 to 2000.
The Shifts in the Map - How Obama Won
Find breaking news, multimedia, reviews &amp; opinion on Washington, business, sports, movies, travel, books, jobs, education, real estate, cars &amp; more.
The Danger of Digging Deeper
A project financed by the Energy Department aims to capture geothermal energy from hot bedrock ‚Äî a process that can cause earthquakes.</p></div>
<div class="paragraph"><p>Describe Processes</p></div>
<div class="paragraph"><p>What Happens in a Nuclear Meltdown
The operating reactors at Fukushima Daiichi power station automatically shut down during the earthquake. But after cooling failures, two of them went into partial meltdown.
Dissecting a Dance
Alastair Macaulay, The Times&#8217;s chief dance critic, analyzes Merce Cunningham&#8217;s "Biped."
Explain the Geography
Map of the Damage From the Japanese Earthquake
Map and Estimates of the Oil Spill in the Gulf of Mexico - Interactive Map - NYTimes.com
The spreading slick, day by day, and a chart of how much oil has been spilled.</p></div>
<div class="paragraph"><p>Report and Research</p></div>
<div class="paragraph"><p>Virginia Tech Shooting - New York Times
A recounting of the events on the day of the deadliest shooting rampage in American history. From official and survivor accounts of the shooting at Virginia Tech.
Map of Evacuation Zones Around Japan Nuclear Plant - Interactive Feature - NYTimes.com
Map of the evacuations zones recommended by the American Embassy in Tokyo and Japanese officials.
What&#8217;s the Story?
How Mariano Rivera Dominates Hitters
The closer has confounded hitters with mostly one pitch: his signature cutter.
House Map - Election Results 2010
House Big Board - Election Results 2010
Iowa City Water Department Water System - Interactive Database - The New York Times</p></div>
<div class="paragraph"><p>Sketch with Data</p></div>
<div class="paragraph"><p>A Peek Into Netflix Queues - Interactive Graphic - NYTimes.com
Story+Data &gt; Data
Preview of Key House Races - Election Results 2010
Inaugural Words - 1789 to the Present
Turning a Corner?
Layers of Ownership
The Information Is What&#8217;s Important
New York, NY 10017 Today&#8217;s Weather Forecast - AccuWeather.com
weather.ericson.net: New York, NY Weather
Satellite Photos - Japan Before and After Tsunami
Compare satellite images of areas of Japan before and after the disaster.
Map of Radiation Readings Near Fukushima Daiichi
Fractions of a Second: An Olympic Musical - Interactive Graphic - NYTimes.com</p></div>
<div class="paragraph"><p>4.3.x Visualization vocabulary (Kate hudson)
Types of graphs/visualizations: choosing an appropriate visualization tool
Comparison
Distribution
Data-ink / data-pixel ratio
Color
Grid lines
Typography
2D v.s. 3D
Integrity
Time-based; interactive experiences</p></div>
<div class="paragraph"><p>Timelines (via Claire Miller)
TimelineSetter: A New Way to Display Timelines on the Web
Tiki Toki</p></div>
<div class="paragraph"><p>(Lulu: I&#8217;d add in:)
Sound and Motion
Layout
Keys (or legends)
Platform</p></div>
<div class="paragraph"><p>Which is the right visualisation for your story?</p></div>
<div class="paragraph"><p>Not all types of visualisations are appropriate for all data sets. When you are deciding on a visualisation, keep in mind that you are trying to tell a story and that some types of visualisations will tell that story better than others.</p></div>
<div class="paragraph"><p>A map, for example, will highlight geographical distribution of a variable, but it will suppress quantitative comparisons or changes over time.
The bottom chart isn&#8217;t eye candy, but it is very functional; not a single pixel is wasted. Every dot conveys information relevant to the data set, and important relationships are immediately visually evident. How much of your chart is wasted on decoration, and how much communicates information?</p></div>
<div class="paragraph"><p>Maps (David)
Maps can be a powerful tool to visualize data when geographical information plays a fundamental role in the telling of the story. They can be used to overlay more than one data sets to show concepts such as areas of overlap or divergence, concentrations, etc. Further, they put data in the context of the viewer&#8217;s understanding of the area and, for local maps, tap into their daily experience.</p></div>
<div class="paragraph"><p>Your audience must immediately (or at least clearly) intuit the information you&#8217;re presenting from the visual markers. Choosing the appropriate overlays is essential. Objects can be plotted and made clickable for more information, the size of markers can be used to indicate quantity or severity (with or without a legend), overlays or polygons can represent zones.</p></div>
<div class="paragraph"><p>There are many tools that simplify the creation of maps from digital data.
<a href="http://code.google.com/apis/maps/index.html">http://code.google.com/apis/maps/index.html</a></p></div>
<div class="paragraph"><p>This classic map shows the advance and retreat of Napolean&#8217;s army in Russia using width to indicate army size, color to represent direction and a sharp aesthetic to show the severity of the event. <a href="http://www.edwardtufte.com/tufte/posters">http://www.edwardtufte.com/tufte/posters</a></p></div>
<div class="paragraph"><p>It&#8217;s important to note that the fact that data can be mapped does not mean that it should be mapped. If location isn&#8217;t central to the narrative, the presence of a map will probably be distracting. <a href="http://www.ericson.net/content/2011/10/when-maps-shouldnt-be-maps/">http://www.ericson.net/content/2011/10/when-maps-shouldnt-be-maps/</a></p></div>
<div class="paragraph"><p>Graphs
There are many ways to use graphs to visualize data. Data sets that focus on pairs of information such as quantity x over time or age over height are often prime. A third piece of data can usually be shown through size or color of data points. For trending information and regular distributions, such as population over time lines can connect points. Multiple sets of relevant information can also be plotted so long as they are distinguished. This can be helpful in showing upper and lower bounds or the way in which two related quantities move together over time.</p></div>
<div class="paragraph"><p>It is important to keep your target audience in mind when using graphs, more so even than most other forms of visualization. Graphs are technical things. If they aren&#8217;t clear or contain extraneous information, they can lead to confusion and even viewers to drawing wrong conclusions. (This is also true of graphs that are clear, but less so.) Further, what is lacking in understanding is often overcompensated in conviction. Many misconceptions have been fueled and spread through misleading graphs, or proper graphs accompanied by misleading captions.</p></div>
<div class="paragraph"><p>Charts
Charts can focus a great deal of attention on a small amount of information or used in a similar way as graphs to show relationships and trends. As with graphs, the choices made in creating the graph is service of the story is vitally important.</p></div>
<div class="paragraph"><p>(Lulu) Motion Graphics is one to add to taxomony:
Motion graphics
With a tight script, well timed animations and clear explanations motion graphics serve to bring complex numbers or ideas to life, guiding your audience through the story.
Example 1: How Mariano Rivera dominates hitters
NYT&#8217;s Joe Ward, sports graphics editor, is interviewed here on the making of it, a case-study of a piece of data journalism that might have been an interactive but resulted in a narrated piece of animated graphic video:
Example 2: The Hans Rosling video is another good example of this type of presentation.
Example 3: Whether or not you agree with their methodology I think the Economist&#8217;s Shoe-throwers' index is a good example of using video to tell a numbers-based story. You wouldn&#8217;t, or shouldn&#8217;t, present this graphic as a static image. There&#8217;s far too much going on. But having built up to it step by step you&#8217;re left with an understanding of how and why they got to this index.</p></div>
<div class="paragraph"><p>Custom Graphics and Interactives
If you have an interesting story and the requisite production skills or resources, data visualization can become a deeply artistic endeavor. As a journalist, it is important to keep an editorial eye on custom graphics and interactives to ensure they stay on story, as there is a tendency with strong creative designers or coders to move toward showcases for aesthetics or functionality. See the case studies and appendix for more on custom visualizations.</p></div>
<div class="paragraph"><p>(Lulu) Agrees&#8230;
Don&#8217;t be tempted to throw an uber-complex interactive interface at the audience just because you&#8217;ve spent days developing it. If it has led you to some good stories then it has been worthwhile, but a simpler visualisation that tells a specific story might be more appropriate for the final presentation.</p></div>
<div class="paragraph"><p>Making choices about which charts to use, great overview here: <a href="http://i.imgur.com/YjWta.jpg">http://i.imgur.com/YjWta.jpg</a></p></div>
<div class="paragraph"><p>Interaction
(Lulu)
Depending on what you want your audience to take away, different types of interaction can be used, as illustrated by these examples:
- Story-telling: NYT: Turning a Corner (Economic Cycles)
- No story, free data exploration by user: ONS: Population Pyramids
- Learn by having a go: BERG/BBC: How big really?</p></div>
<div class="paragraph"><p>When you think you&#8217;re nearly there, try out your visualisation on a handful of other people. Observe them using it, exploring it. It becomes very clear very quickly what users do and don&#8217;t <em>get</em>. If your users don&#8217;t realise there&#8217;s another screen to progress to, all your hard work will have been wasted. Go back and adjust your work to address the obstacles that you saw users encounter.</p></div>
<div class="paragraph"><p>Examples of good use of visualisations to tell stories</p></div>
<div class="paragraph"><p>NYtimes election visualization (Aron)
Progressively updated through election night
Lead shown clearly at top (speaks to <em>why do I care</em>)
More information below deepening information</p></div>
<div class="paragraph"><p>Guardian Riot map (Farida)</p></div>
<div class="paragraph"><p>During the summer of 2011 the UK was hit by a wave of riots (full government report here todo: find link). Politicians suggested that these actions were categorically not linked to poverty and those that did the looting were simply criminals. The Guardian newspaper played a crucial role in using data driven journalism and a range of data visualizations in telling this story, allowing for better understanding who was doing the looting and why. By using simple maps (Google Fusion Tables) they showed the location of confirmed riots spots and through mashing up deprivation data with where the riots took place started debunking the main political narrative that there was no link to poverty. Both these examples use off the shelf visualization tools and in the second example combine location data with another data set, to start making other links. A final example worth mentioning here is the visualization the paper built around the use of social media. Again politicians had started to suggest that social media use might have to be curtailed during such troubles in future, but offering little concrete evidence that social media, Twitter in this case, played a role in encouraging people to riot. A visualization of riot related hashtags used during this period, highlighted that Twitter was mainly used to respond to the riots rather than to organize people to go looting, with #riotcleanup showing the most significant spike during the riot period.</p></div>
<div class="paragraph"><p>These three examples show three different types of data visualization used very effectively.</p></div>
<div class="paragraph"><p>¬∑   The first one a simple map highlighting where riots took place using Google Fusion Tables.
¬∑   The second map linked to an additional data set offering a further reading of events and generating more story leads.
¬∑   Finally by looking at twitter data, visualizing the hashtags showed that significant traffic was concerned with cleaning up after the riots rather than inciting people to riot (if you were to do this yourself, you would need to think about where you would get this data).
¬∑   Most of these visualizations used off the shelf tools to great effect for telling important stories (quickly).</p></div>
<div class="paragraph"><p>Example from Finland (Farida has contacted Petri Kola, with the request to quickly explain the story and send me the link to the vis they used) - where the first time the main Finish paper ran a DDJ story + vis was inspired by the work of the Guardian. The story highlighted election data and who the main sponsors to the main parties were. The vis was simple, but cause a buzz.</p></div>
<div class="paragraph"><p>Buenos Aires Elections (HacksHackers Buenos Aires - do we need a further elaboration on this example or possibly replace it with something else?)
<a href="http://elecciones2011.hhba.info/">http://elecciones2011.hhba.info/</a> Google Fusion Tables, scrapping data in Real time, created a platform in real time and then we sended code to embed to diferent medias.
<a href="http://www.ted.com/talks/lang/eng/hans_rosling_on_global_population_growth.html">http://www.ted.com/talks/lang/eng/hans_rosling_on_global_population_growth.html</a></p></div>
<div class="paragraph"><p>When is a visualisation good and when is it bad?</p></div>
<div class="paragraph"><p>A good visualisation clarifies, explains, reveals or adds meaning. It makes the audience say <em>ah-ha, I get it</em>. They take away with them the story you set out to tell.</p></div>
<div class="paragraph"><p>A good visualisation can turn bad however if no-one sees it, or knows how to use it. In both cases your time and effort will have been wasted. Design and UX can help solve this.</p></div>
<div class="paragraph"><p>If a visualisation confuses or misleads then it is bad, regardless of how pretty or user-friendly it might be. It still has to be good journalism, even if it is visual.</p></div>
<div class="paragraph"><p>In reality most visualisations add little over and above what you can just write in words or say into a microphone. When presented side by side they are not wasted: saying the same thing two different ways plays to the fact that every audience is made up of a mix of both visual and verbal people. However since visualisations such as these have the potential to work harder it is a shame not to have made them do so.</p></div>
<div class="paragraph"><p>It is in a big middle-ground of OK-ness that most visualisations lurk. And the two things that will elevate a visualisation from being OK to good is knowing your story and knowing your audience. Use both of these, from the outset, to inform your decisions and the resulting visualisation stands every chance of being a great one.</p></div>
<div class="paragraph"><p>(bad example) NYTimes election 2004 (Aron - needs input)
<a href="http://www.nytimes.com/packages/html/politics/2004_ELECTIONRESULTS_GRAPHIC/">http://www.nytimes.com/packages/html/politics/2004_ELECTIONRESULTS_GRAPHIC/</a>
What is the important thing?
Location is shown first but the location of the state isn&#8217;t important.
Burrying the lead</p></div>
<div class="paragraph"><p>Data-to-pixel ratio
BAD     GOOD
Use language to sound intelligent/ interesting/ like a brilliant writer.        Use language to illuminate meaning in a concise way
Use design language to make things pretty/ interesting/ convince everyone you&#8217;re a brilliant designer.  Use design language to illuminate meaning in a concise way.</p></div>
<div class="paragraph"><p>How do you use design language to illuminate meaning in a concise way? (Just ask Tufte!)
Colour: use colour sparingly to highlight information that is more important than others, or show the distinction between two sets of variables. Choose colours that are distinct enough that colourblind individuals are able to read your visualization, and don&#8217;t use extra colours or patterns unnecessarily.</p></div>
<div class="paragraph"><p>(Lulu) Colour accessibility tools:
Color Oracle: Shows you in real time what people with common color vision impairments will see <a href="http://colororacle.cartography.ch/">http://colororacle.cartography.ch/</a>
snook.ca: Allows you to check colour contrast of a foreground colour against a different background colour: <a href="http://snook.ca/technical/colour_contrast/colour.html">http://snook.ca/technical/colour_contrast/colour.html</a>
Grid lines: grid lines do not add important information to your chart; remove them, and ask yourself if the chart still communicates its message as effectively. If you must use them, make sure they are as visually minimal as possible</p></div>
<div class="paragraph"><p>(Lulu)
Typography: Complement the story&#8217;s hierarchy with a visual hierarchy:</p></div>
<div class="ulist"><ul>
<li>
<p>
make the title both big and insightful
</p>
</li>
<li>
<p>
label key points directly and bold them up
</p>
</li>
<li>
<p>
hand-hold the user through the structure of the story by distinguishing header text from body text
2D v.s. 3D: Never, ever use 3D graphs. They distract from information and add nothing.
</p>
</li>
</ul></div>
<div class="paragraph"><p>(Lulu)
Sound and Motion: Primarily leading to motion graphics, a massively growing field. Simultaneously reinforcing what your audience is hearing from a voice-over with explanatory visuals provides a very powerful and memorable way of telling a story.
Layout: Size, position, white space are the variables you have to play with. As a rule of thumb adjust these so the design works for the extreme cases - longest labels, most data, biggest sample, and their smallest counterparts too. Then the rest should all fall into line.
Keys (or legends): Avoid using them whenever possible. It&#8217;s much more effective to label lines, points, bars directly. If you do have to use a key, put it at the top, in a prominent position.
Platform: Design for the platform that your visualisation is going to live on. If both print and online, re-design it to work on the other one. A print graphic directly uploaded to a website rarely works as well as it could online, and vice versa of course. It&#8217;s almost always necessary to re-work.</p></div>
</div>
<div class="sect2">
<h3 id="_how_data_visualisation_is_used_at_the_most_read_daily_newspaper_in_norway_by_john_bones_verdens_gang">How data visualisation is used at the most read daily newspaper in Norway by John Bones (Verdens Gang)</h3>
<div class="paragraph"><p>Some comments:
* English translation of the titles.
* Tools being used are mentioned.
* We don&#8217;t develop protocols/guides.
* Lessons learned: Great variations!
* Links: Only when it is on the web, not in the paper product.
John
Visualizing the data
News journalism is about bringing new information as fast as possible to the reader. The fastest way may be a video, a photo, a text, a graph, a table or a combination of the different methods.
Concerning visualizations, the purpose should be the same: Quick information. The introduction of new data tools has given journalists the possibilities of making stories they could not do without them, new possibilities to find stories and new ways of presenting them.
Here are some examples showing how data visualization is used in the most read newspaper in Norway, Verdens Gang (VG).</p></div>
<div class="paragraph"><p><strong>1. When the visualization is the story:</strong></p></div>
<div class="paragraph"><p>Translation: Here is crime happening ‚Äì hour by hour
Link: <a href="http://www.vgtv.no/#!id=46131">http://www.vgtv.no/#!id=46131</a></p></div>
<div class="paragraph"><p>In this animated heatmap combined with a simple bar chart you can follow the development of crime downtown Oslo, Norway, hour by hour, during the weekend for several months. In the same animated heatmap, you can see the number of police officers working at the same time. When crime really is happening, the number of police officers is at the bottom.
Tools used: ArcView with Spatial Analyst.</p></div>
<div class="paragraph"><p><strong>2. When the visualization helps us finding the story:</strong></p></div>
<div class="paragraph"><p>Translation: The network of the movie people
Skuespillere = actors/actresses
Regiss√∏rer = directors
In this Social Network Analysis, VG downloaded all relations in all Norwegian movies during more than ten years. Who were playing with whom, and which actors and actresses were playing with the different directors? A database with all relevant content was exported to UCINET, a program for social network analysis, and then it was quite easy to see who were connected with whom with different strength of ties.
.
Translation: Rich children are playing together
The same methods were used when downloading 157 friendlists from Facebook. These 157 people were sons and daughters of the richest people in Norway, and the social network analysis was showing that heirs of the richest persons in Norway also herited their parents' network.
Tools used: Access, Excel Notepad, Ucinet.
Before publication, the designer downloaded the Ucinet illustration file as a JPG-file and used it as a background when working in Adobe Illustrator. There all is handmade, before the finish in Adoble Photoshop.</p></div>
<div class="paragraph"><p>Lessons learned: When making social network analysis, we get an enormous amount of data. When making the presentation, we have to do manual work at the end; make some choices.
Anecdotes: The most central person in the movie network denied there was a network!</p></div>
<div class="paragraph"><p><strong>3. When the visualization is telling the story:</strong></p></div>
<div class="paragraph"><p>Translation: Out of the ghetto.
Pakistanis of the middle class are moving out of downtown Oslo
These mapping examples are showing the intern migration in Oslo, based upon data from The Norwegian Bureau of Statistics. The angle for the story is the development among the biggest immigration groups in Oslo, and the reader does not have to read the text to understand the news.
Tools used: Excel, Access, ArcView.
Lessons learned: Too much text on the pages. It would have been better to concentrate 100 percent on the graphs.</p></div>
<div class="paragraph"><p><strong>4. When the visualization is supplying the text:</strong></p></div>
<div class="paragraph"><p>Translation: Erna is the queen of the applauses
Antall ord = the number of words
Vokabular = vocabulary
Setningslengde = length of periods
Ord pr. minutt = words pronounced each minute
Link: <a href="http://www.vg.no/nyheter/innenriks/valg-2009/artikkel.php?artid=570321">http://www.vg.no/nyheter/innenriks/valg-2009/artikkel.php?artid=570321</a></p></div>
<div class="paragraph"><p>This visualization is based upon text mining of the speeches held by the seven Norwegian party leaders during their Conventions. All speeches were analyzed, and the analyses gave the angles to make some stories. Every story was linking to the graph, and the readers could play around, studying the language of the politicians.
Tools used: Excel, Access, Flash and Illustrator.
Lessons learned: In 2012 we would have made the interactive graph in Javascrips.
We do not yet have good tools for text mining.</p></div>
<div class="paragraph"><p><strong>5. When the visualization is an integrated part:</strong></p></div>
<div class="paragraph"><p>Translation: Mekka of the slots in the north (of Norway)
This story is about the use of slots in the different parts of Norway. The text, the tables, the graph and the photo are all parts of an integrated solution.
Tools used: Most important was Access database. The mapping solution is made in ArcView.</p></div>
<div class="paragraph"><p><strong>6. When the graphs are interactive:</strong></p></div>
<div class="paragraph"><p>Translation: Here people are using most of their income on Lotto
Link: <a href="http://www.vg.no/nyheter/innenriks/artikkel.php?artid=569832">http://www.vg.no/nyheter/innenriks/artikkel.php?artid=569832</a></p></div>
<div class="paragraph"><p>This story is based upon a database containing data from the Norwegian Bureau of Statistics, taxpayers data and data from the national Lotto monopolist. In this interactive graph, the reader could find different kind of information from each Norwegian county and municipality. The actual table is showing the percent of the income used on games.
Tools used: Access, Excel, MySql, Flash</p></div>
<div class="paragraph"><p>Translation:
Poeng landet har gitt = points given.
Poeng landet f√•r = points received.
Differanse mellom gir og f√•r = the difference between what you give and what you get.
Poeng mottatt √•r for √•r = points received each year.
MGP vinner √•r for √•r = the winner each year.
LILIANA:
Link DEAD AT THE MOMENT. WE COULD REACTIVATE IT IF YOU WANT TO.</p></div>
<div class="paragraph"><p>After downloading all points given in The Eurovision Song Contest from 1994 to 2009 into a database, VG made this interactive graph, documenting who is voting for whom. In this interactivity of entertainment, the reader could click at each country to get all information interesting him or her.
Tools used: Access, Excel, MySql, Flash</p></div>
<div class="paragraph"><p>Translation of the column titles: Pristilskudd = price given by the dairy.
Produksjonstilskudd = national subsidies
Link: <a href="http://www.vg.no/spesial/jordbrukstilskudd/">http://www.vg.no/spesial/jordbrukstilskudd/</a></p></div>
<div class="paragraph"><p>The data in this table is containing farm subsidies to all Norwegian farmers, some 46 000. By clicking at the drop down menu at the top of the graph, the readers could choose county and municipality.
Tools used: Access, Excel, MySql.
Lessons learned: This table and the stories connected to it was the most read that day. The graphs do not necessarily need to be fancy to capture readers.</p></div>
<div class="paragraph"><p><strong>7. When the visualization is user-generated:</strong></p></div>
<div class="paragraph"><p>Translation: Here people is most afraid in Oslo
Link: <a href="http://www.vg.no/nyheter/innenriks/artikkel.php?artid=10091789">http://www.vg.no/nyheter/innenriks/artikkel.php?artid=10091789</a></p></div>
<div class="paragraph"><p>This Google heat map is based upon information given by 16 000 readers. There had been several rapes downtown Oslo, and the readers could click at a Google map to show were in Oslo they did not feel safe.
Tools used: Google Fusion Tables with heatmap, Google maps, MySql.</p></div>
<div class="paragraph"><p>Do not give them all!
So; when do we need to visualize a story? Most of the times we do not need to do it, but sometimes we have to do so to help the readers. Stories containing a huge number of data quite often need visualization. However, we have to be quite critical when choosing what kind of data we are going to present. OK, we know everything about the stuff, but what kind of basic information does the reader really need to know? Perhaps a table is enough, or a simple graph showing a development from year A to year C.
When working with data journalism, the point is not necessarily to present huge amounts of data. The data gives us, as reporters, the possibilities to make stories we could not do without our data knowledge. It&#8217;s all about journalism!
Like a good picture
A clear trend the last 2-3 years has been to create interactive graphs and tables, and then the reader has the possibility to go into the debth of the themes.
A good visualization is like a good picture: You understand what it is about just by looking on it a moment or two. When starting to study the picture, or the graph, you see and enjoy more and more of the details.
The visualization is bad when the reader does not know where to start or where to stop, and when the visualization is overloaded by details. Perhaps a long text had been better?
The question of quality is also a platform question. A perfect and big print visualization can not be automatically adopted to the web or the smart phone platforms. But that is another discussion.</p></div>
<div class="paragraph"><p>Toolkits and tutorials</p></div>
<div class="paragraph"><p><a href="http://www.mirkolorenz.com/">http://www.mirkolorenz.com/</a></p></div>
<div class="paragraph"><p>Tools as listed by NYT&#8217;s Matt Ericson here: <a href="http://www.ericson.net/content/2011/04/international-journalism-festival-links/">http://www.ericson.net/content/2011/04/international-journalism-festival-links/</a>
Google Fusion Tables Example from The Guardian
Tableau Public: Power Hitters
Google Charts from New York State Test Scores - The New York Times
HTML, CSS and Javascript: 2010 World Cup Rankings
jQuery: The Write Less, Do More, JavaScript Library
jQuery UI - Home
Protovis
Rapha√´l‚ÄîJavaScript Library
The R Project for Statistical Computing
Processing.org</p></div>
<div class="paragraph"><p>Google&#8217;s tools <a href="http://code.google.com/apis/ajax/playground/">http://code.google.com/apis/ajax/playground/</a>
<a href="http://www.google.com/support/mapmaker/bin/static.py?page=guide.cs&amp;guide=30028">http://www.google.com/support/mapmaker/bin/static.py?page=guide.cs&amp;guide=30028</a>
Image plot (Lev Manovich - Software Studies; explore patterns in large image collections) <a href="http://lab.softwarestudies.com/p/software.html">http://lab.softwarestudies.com/p/software.html</a>
A quick exercise for aspiring data journalists (from Paul Bradshaw):
<a href="http://onlinejournalismblog.com/2011/10/31/a-quick-exercise-for-aspiring-data-journalists/">http://onlinejournalismblog.com/2011/10/31/a-quick-exercise-for-aspiring-data-journalists/</a>
Visualizing Toronto&#8217;s water usage: a tutorial
<a href="http://datadrivenjournalism.net/resources/visualizing_torontos_water_usage_a_tutorial">http://datadrivenjournalism.net/resources/visualizing_torontos_water_usage_a_tutorial</a>
Data visualizations for non-programmers (via Knight Digital Media Center):
<a href="http://multimedia.journalism.berkeley.edu/tutorials/intro-dataviz/">http://multimedia.journalism.berkeley.edu/tutorials/intro-dataviz/</a>
Visualizing Data (Andy Kirk): <a href="http://www.visualisingdata.com/index.php/resources/">http://www.visualisingdata.com/index.php/resources/</a>
How We Visualized 23 Years of Geo Bee Contents (via Datavisualization.ch):
<a href="http://datavisualization.ch/opinions/how-we-visualized-23-years-of-geo-bee-contests/">http://datavisualization.ch/opinions/how-we-visualized-23-years-of-geo-bee-contests/</a></p></div>
<div class="paragraph"><p>Scattered Examples
<a href="http://www.guardian.co.uk/news/datablog/2011/oct/17/data-visualisation-visualization">http://www.guardian.co.uk/news/datablog/2011/oct/17/data-visualisation-visualization</a>
<a href="http://upload.wikimedia.org/wikipedia/commons/2/27/Snow-cholera-map-1.jpg">http://upload.wikimedia.org/wikipedia/commons/2/27/Snow-cholera-map-1.jpg</a>
<a href="http://www.guardian.co.uk/news/datablog/2010/aug/13/florence-nightingale-graphics">http://www.guardian.co.uk/news/datablog/2010/aug/13/florence-nightingale-graphics</a></p></div>
<div class="paragraph"><p>Articles and personalities
<a href="http://lilt.ilstu.edu/gmklass/pos138/datadisplay/sections/goodcharts.htm">http://lilt.ilstu.edu/gmklass/pos138/datadisplay/sections/goodcharts.htm</a>
<a href="http://www.ericson.net/content/2011/10/when-maps-shouldnt-be-maps/">http://www.ericson.net/content/2011/10/when-maps-shouldnt-be-maps/</a></p></div>
<div class="paragraph"><p>Infographic and Information Design Conferences
The Design of Understanding: January 2012, London
Malofiej: March 2012, Pamplona, Spain
Information Design Conference: April 2012, London</p></div>
<div class="paragraph"><p>Books:
Fresh Dialogue 9: In/Visible: Graphic Data Revealed
Edward Tufte The Visual Display of Quantitative Information
The Online Journalism Handbook: Skills to survive and thrive in the digital age (Liisa Rohumaa, Bournemouth Media School | Paul Bradshaw, Birmingham City University) - todo: to go in general section?</p></div>
<div class="paragraph"><p><a href="http://catalogue.pearsoned.co.uk/educator/product/The-Online-Journalism-Handbook-Skills-to-survive-and-thrive-in-the-digital-age/9781405873406.page">http://catalogue.pearsoned.co.uk/educator/product/The-Online-Journalism-Handbook-Skills-to-survive-and-thrive-in-the-digital-age/9781405873406.page</a></p></div>
<div class="paragraph"><p>More resources:</p></div>
<div class="paragraph"><p>Exploring survey data with IBM Many Eyes</p></div>
<div class="paragraph"><p><a href="http://www.opendatacookbook.net/wiki/recipe/exploring_survey_data_with_many_eyes">http://www.opendatacookbook.net/wiki/recipe/exploring_survey_data_with_many_eyes</a></p></div>
<div class="paragraph"><p>Mapping:</p></div>
<div class="paragraph"><p><a href="http://mapalist.com/">http://mapalist.com/</a>
<a href="http://geocommons.com/">http://geocommons.com/</a>
Costs money but recommended - ArcGIS <a href="http://www.esri.com/software/arcgis/index.html">http://www.esri.com/software/arcgis/index.html</a></p></div>
<div class="paragraph"><p>(Via Claire Miller)</p></div>
<div class="paragraph"><p>Stamen Design
Try this OpenLayers Simple Example. A good sample is Ushahidi&#8217;s Haiti map.
There are other JavaScript libraries for overlaying information on maps, such as Polymaps. And there are a number of other mapping platforms, such as Google Maps, which offers numerous mapping APIs; Yahoo Maps Web Services, with its own APIs; the Bing Maps platform and APIs; and GeoCommons.
Leaflet
How To Build an Interactive Map with Open-Source Tools
Making maps, part 1: Less interactivity</p></div>
<div class="paragraph"><p>Tableau (via Claire Miller)
Blue things and Green things
Making Table Calculations work in Tableau
Show Me! ‚Äì When less is definitely more
Preprocess shapefile for Tableau Tutorial Video
Hey! Your Tableau Public Viz is Ugly <strong>and</strong> Confusing</p></div>
<div class="paragraph"><p># Data driven applications #</p></div>
<div class="paragraph"><p>Overview: Step by step guide, tips and tricks for how newsrooms can produce data driven applications
What are the resources (skills, costs, etc.) needed?
What are the steps to take when you want to build a data driven application?
What useful lessons did you learn from your own experience?
Why should newsrooms be interested in producing data driven applications?
What is the potential of such projects?
Authors: Aron Pilhofer, Matt Stiles
Length: 2-3 pages (including examples)</p></div>
</div>
<div class="sect2">
<h3 id="_data_driven_apps_case_studies_äì_internews_in_kenya_by_mark_irungu_internews_network">Data-driven apps case studies ‚Äì Internews in Kenya by Mark Irungu (Internews Network)</h3>
<div class="paragraph"><p>On the 6th of July 2011, the Kenya government became the first in Africa to have an open data portal to the public. Making government data open is not a very popular idea that regimes would readily embrace in Africa, not with the rampant corruption and mismanagement of public funds. Understandably, the Kenyan Government has received a lot of accolades for that.</p></div>
<div class="paragraph"><p>While this World Bank supported initiative received a lot of praise globally (from the New York Times to government heads in the developed world) the scenario was different locally. It was great news in the techie world but just another tiny story in the world of journalism. The story was not covered beyond the day of the launch. Few journalists visited portal after the launch! The data is accessible online and just like all data sets; it is not in the smallest way friendly to the eye, at least not a journalist&#8217;s eye.</p></div>
<div class="paragraph"><p>The GAP</p></div>
<div class="paragraph"><p>Whereas the techies have taken the idea and ran with it, trying to think of apps that could be developed to mine the data, clean the data and visualize it making it more accessible to people beyond the developer community..
A new digital divide exists, not just in Kenya between journalists and developers in working with Open Data. Journalists need to learn more on the mechanics of using data. Developers, also called techies, also have a lot to learn. . Developers may be brilliant with code, but have a lot to learn with regard to content, the opposite is true for journalists. Why not leverage these strengths to form a <em>tag team</em>? They do not necessarily have to be at the same place at the same time, all we need is an interface co connect them. What the data world needs is a good app that can serve these groups..</p></div>
<div class="paragraph"><p>The APP</p></div>
<div class="paragraph"><p>The power of data cannot be under estimated. It is time that newsrooms and journalists moved away from the "he said, she said" reporting. Numbers don&#8217;t lie, they can help journalists produce in-depth features, and explore untold stories and trends that are not immediately known or visible to the general public.. Unfortunately, journalists in Kenya have not made good use of the opened data. I work at Internews in Kenya ‚Äì a media development organisation that trains and supports journalists to be better storytellers, In our journalism workshops we have designed a number of courses to help journalists use numbers and data better for their storytelling. This is important as it helps journalists to enrich their enquiry and stories</p></div>
<div class="paragraph"><p>It is with this realization that the idea of developing an app to query KODI was born. There is need for a a solution that would help journalists clean up raw data in the back end and help journalist to identify emerging stories and trends.That app should also help in presenting the content in visually appealing ways.</p></div>
<div class="paragraph"><p>The app should:
1. Query the entire Kenya open data portal.
2. Clean the data/ provide a friendly interface for cleaning the data.
3. Visualize the data.
4. Generate reports that can be used as is.</p></div>
<div class="paragraph"><p>The Target</p></div>
<div class="paragraph"><p>We are facing a unique scenario. Who were we targeting? We have had to determine whether this is going to be a case of old hands, new tricks or new hands, new tricks. Traditional journalism has undergone a radical evolution process with the coming in of web 2.0. Maybe that was enough. Yes there is a significant gap between journalists and techies, but we have also established that the desire to among journalists to learn and possibly close this gap is very strong . The <em>evolved</em> traditional journalist has been showing willingness to undergo a second metamorphosis and the upcoming journalist doesn&#8217;t mind this baptism of fire. The app we are working will serve journalists at all levels.</p></div>
<div class="paragraph"><p>The Team and the Technology</p></div>
<div class="paragraph"><p>Over the past 8 years, Internews has worked in Kenya with journalists to build capacity in health and conflict sensitive reporting through training, mentoring, travel grants and availing resources for journalists at the Nairobi and Eldoret offices. The idea of developing an APP has come hand in hand with the idea of setting up a Center for Digital journalism at the Kenya office. This center does more or less what Internews has been doing but at a digital scale. The center also <em>incubates</em> upcoming and evolving data/ digital journalists and has facilities that aid such journalists in production of complex and compelling stories.</p></div>
<div class="paragraph"><p>To develop this app, we have had to face the fact that we do not know everything and we needed to talk to those who may have taken our path before leveraging on our relationship with various developer communities in Nairobi to form a team that would work towards our end product. Our first choice approach was the use of PHP ZEND framework and ADOBE flex and flash builder 4.5 for php. We initially felt these platforms would be sufficient to start us off. However, along the way we have picked up on other technologies that we believe will spice up the end result e.g. the DATA journalism developer studio running on open Linux. The tool will run across all platforms; web, desktop, mobile. With the high uptake and use of mobile phones in Kenya, it is Important that all Open data tools should be able to run on mobile platforms (J2ME, Android).</p></div>
<div class="paragraph"><p>Developers vary in coding strengths and language preferences. After determining which languages our platform would run on, we have had to identify strong team members that would deliver the quality of code that would make this world-class product. We have also distinguished those working on the front end (user experience) and those working on the back end (dirty work). The two teams have to sync at the application level and there is need for a strong working relationship.</p></div>
<div class="paragraph"><p>Kenya has clearly made some bold steps but there is still a lot of ground to cover.
The biggest challenge is not coming up with the idea but in having a worthy team. An outcome based approach team that endears its energies to the overall objective makes product successful. If used well, Data can be life changing. Electoral dynamics especially in voting patterns in Kenya have been devoid of content and objectivity. With available data, this and many other systems in Kenya can change for the better.</p></div>
<div class="paragraph"><p><strong>2. How to build a news app</strong></p></div>
<div class="paragraph"><p>A news app is a software (web, desktop, mobile,ussd) that acts as an interface between the database layer of the information architecture and the client side. News apps make data friendlier to the client as compared to the format and layout at the storage engine.</p></div>
<div class="paragraph"><p>News Apps vary in type:</p></div>
<div class="olist arabic"><ol class="arabic">
<li>
<p>
Curation- This is an app that gathers already existing information and brings it to one platform based on a particular keyword(s). This kind of app may not necessarily have its own storage engine, some depend on a <em>Google type</em> indexing mechanism that crawls the net and indexes searchable information. Examples of Curation APPS; Storify, Storyful.
</p>
</li>
<li>
<p>
News Portals- these are online news portals that deliver news in real time. News websites, news applications running on mobile platforms , aggregated feed readers fall in this category
</p>
</li>
<li>
<p>
Citizen reporting- these are apps whose main source of data is the citizen. These apps provide an interface for citizen-journalism and then visualize the data graphically or on a map. E.g.: Ushahidi, crowdmap, swiftriver
</p>
</li>
<li>
<p>
DATA apps ‚Äì These are apps that query DATA sets from governments, organizations, civil society etc. These Apps may have their own storage engine that is updated regularly from the main data source e.g. World Bank finances, Kenya open data. Examples of such applications: CDF monitor (Kenya) Internews Open DATA app(Kenya)
</p>
</li>
</ol></div>
<div class="paragraph"><p>Why and for who?
News Apps are not for journalists only. Most news apps are coded to produce interactive and friendly reports that can be used by any news savvy person. The act of interacting with a news app looking for news is in itself journalism. Newsrooms, News organizations, governments, and civil society should commission development of news apps based on need.</p></div>
<div class="paragraph"><p>In the age of Open Source tools, re-inventing the wheel is not necessary. When making the decision to develop a news app, it is imperative to survey the current news landscape and identify tools that do at least half of what you&#8217;d like to see. Chances are that in most instances you may just combine two different tools with great precision. For example, when working on large data sets you may be looking for a tool that will clean the data and also visualize it in clean interactive graphs, charts, bubbles and possibly maps. You may want, in this instance, to clean the data on Google refine, and then export the data to Tile Mill to visualize it on a map and to infogr.am to plot it on a matrix!</p></div>
<div class="paragraph"><p>However, there are rare scenarios like that of Kenya where relationships between datasets need to be established on the fly in order to generate some visualizations that may require you to develop an app altogether. This saves the non-techie journalist the hustle e.g. of trying to join a table with all constituencies in Kenya with another with their Longitude and latitude co-ordinates in order to plot the info on a map.</p></div>
<div class="paragraph"><p>The main idea of developing a news app should be motivated by the need to make data friendlier. If your news app does not make your numbers sexy/savvy then it&#8217;s not worth the time. And money!</p></div>
<div class="paragraph"><p>A good news app should:</p></div>
<div class="olist arabic"><ol class="arabic">
<li>
<p>
Mine the data- extracting data sets from the source.
</p>
</li>
<li>
<p>
Clean the data- pick out anomalies in the data and highlight them to the user. The user then corrects the anomalies.
</p>
</li>
<li>
<p>
Export data- have export data functionality.
</p>
</li>
<li>
<p>
Visualize the data
</p>
</li>
</ol></div>
<div class="paragraph"><p>Open source apps have indeed made things easier for all of us; the ability to even alter the source code of a free product release under the GPNU license is particularly enticing. It is however important to apply news apps to the context for which they were developed and not to the context that cuts costs for you as a news organization.</p></div>
<div class="paragraph"><p>A classical example is the USHAHIDI platform; Ushahidi was built mainly for crisis citizen reporting. It worked perfect during the post election violence in Kenya and mapping of hotspots, and even better during the Haiti earthquake and the BP oil spill. But it stops working when used for activities that do not engage citizens to report e.g. mapping out country offices of an organization.</p></div>
<div class="paragraph"><p>Packaging</p></div>
<div class="paragraph"><p>The success or failure of an app is strongly dependent on how it is packaged. Packaging in this case refers mainly to the platforms for which it is built. Developers should aim at presenting very flexible and useable apps. Limiting an APP to one operating system e.g. windows is a big setback and significantly contributes to the failure of the app.</p></div>
<div class="paragraph"><p>To counter such issues, developers are encouraged to come up with fully functional online versions of applications before thinking of the desktop app. An online app only locks out someone without a browser! Desktop versions and mobile versions should be added features and not the other way round.</p></div>
<div class="paragraph"><p>Resources</p></div>
<div class="paragraph"><p>Having powerful computers, big screens, servers and test platforms is a good thing when developing a news app, having good human resources is even better. The skill set required in this case is unique: a combination of UX, backend and security is required. Developers should take advantage and participate in online communities, developer meet ups, hackerthons, workshops and boot camps to sharpen their skills. Technology, just like traditional journalism, is constantly evolving and every developer should work with this realization. Laxity is not an option</p></div>
<div class="paragraph"><p>App Makers (via Claire Miller)
www.appmakr.com
Android training</p></div>
<div class="paragraph"><p># Delivering a story through social media #</p></div>
<div class="paragraph"><p>Author: Luca Dello Iacovo
Editor: Lucy Chambers</p></div>
<div class="paragraph"><p>Ideas for points to cover:
Ways to engage readers (encourage interaction)
Presentation channels and things to bear in mind when using them
Case studies, good uses, bad uses
When social media is a good choice, when a bad choice</p></div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_7_telling_the_world">7. Telling the world</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_needs_a_title_by_duncan_geere_wired_co_uk">Needs A Title by Duncan Geere (Wired.co.uk)</h3>
<div class="paragraph"><p>Almost as important as publishing the data in the first place is getting reaction from your audience. You&#8217;re human - you&#8217;re going to make mistakes, miss things and get the wrong idea from time to time. Your audience is one of the most useful assets that you&#8217;ve got&#8201;&#8212;&#8201;they can fact-check and point out things that you may not have considered.</p></div>
<div class="paragraph"><p>Engaging that audience is tricky, though. You&#8217;re dealing with a group of people who&#8217;ve been conditioned over years of internet use to hop from site to site, leaving nothing but a sarcastic comment in their wake. Building a level of trust between you and your users is crucial&#8201;&#8212;&#8201;they need to know what they&#8217;re going to get, know how they can react to it and offer feedback, and know that that feedback is going to be listened to.</p></div>
<div class="paragraph"><p>But first you need to think about what audience you&#8217;ve got, or want to get. That will both inform and be informed by the kind of data that you&#8217;re working with&#8201;&#8212;&#8201;if it&#8217;s specific to a particular sector, then you&#8217;re going to want to explore particular communications with that sector. Are there trade bodies that you can get in touch with, that will be willing to publicise the resources that you&#8217;ve got and the work that you&#8217;ve done to a wider audience? Is there a community website or a messageboard that you can get in touch with?</p></div>
<div class="paragraph"><p>Social media is an important tool, too, though it again depends on the type of data that you&#8217;re working with. If you&#8217;re looking at global shipping statistics, for example, you&#8217;re unlikely to find a group on Facebook or Twitter that&#8217;ll be especially interested in your work. On the other hand if you&#8217;re sifting through corruption indices across the world, or local crime statistics, that&#8217;s likely to be something that&#8217;s going to be of interest to a rather wider audience.</p></div>
<div class="paragraph"><p>Then you need to think about how those people are going to interact with your work. Sure, they might read the story that you&#8217;ve written and look at the infographics or maps, but giving your users an outlet to respond is immensely valuable. More than anything it&#8217;s likely to give you greater insight into the subject you&#8217;re writing about, informing future work on the topic.</p></div>
<div class="paragraph"><p>It goes without saying that you need to publish the raw data alongside your articles, but think about if there&#8217;s other ways that you can get the audience to interact. Keep an eye on metrics on which parts of your datasets are getting attention&#8201;&#8212;&#8201;it&#8217;s likely that the most trafficked areas could have something to say that you might have missed.</p></div>
<div class="paragraph"><p>Think beyond the comment box, too. Can you attach comments to particular cells in a spreadsheet? Or a particular region of an infographic? Make sure that other users can see those comments too&#8201;&#8212;&#8201;they have almost as much value as the original data, in a lot of cases.</p></div>
<div class="paragraph"><p>Finally, other people might want to publish their own infographics and stories on the same sources of data&#8201;&#8212;&#8201;think about how best to tie that information together. You could use a hashtag specific to the dataset, for example, or if it&#8217;s highly pictorial then you could share it in a Flickr group. Having a route to share information more confidentially could be useful too&#8201;&#8212;&#8201;in some cases it might not be safe for people to publicly share their contributions to a dataset, or they might simply not be comfortable doing so.</p></div>
</div>
<div class="sect2">
<h3 id="_public_data_goes_social_by_full_name_here">Public Data goes Social by Full Name Here</h3>
<div class="paragraph"><p>Data is invaluable. To the uninformed person where information asymmetry thrives, access to data illuminates the path to facts and provokes emotions that trigger results. Nevertheless, poor handling of data puts valuable facts in an opaque structure that communicates nothing. Data could be in a maze of thick document riddled with complex terms or iterations of figures that doesn’t connect the user.  Not being able to promote discussion nor give provide contextual understanding, data may could be worthless.</p></div>
<div class="paragraph"><p>Nigeria returned to democracy in 1999 after lengthy years of military rule. Data under the barrel of the gun was an exclusive preserve of rulers as probing the facts behind data is taken as an affront to authority and strive to question the stained reputation of the junta. Civil servants were bound by the Official Secrets Act not to share government information hereby putting the citizens in the dark. Even after thirteen years of return to democracy, there is clearly a gap in accessing public data with some government officials still stricken with military era hangover. Data especially in terms of public expenditure communicates little to the larger sections of public who are not versed in financial accounting nor have requisite understanding to evaluate the complex arithmetic.</p></div>
<div class="paragraph"><p>BudgIT, creative startup sees a huge opportunity in using creative data visualization to stimulate interests concerning public expenditure. Understanding the ubiquity of the mobile device within the Nigerian locality and the increasing number of Nigerians online, BudgIT sees the opportunity to engage Nigerians and explain public expenditure in a simpler way. This thrives on building engagement across all platforms, encouraging the community to action via NGOs and reaching out to everyone to promote citizen participation. This is about making public data a social object and building an extensive network that demands change.</p></div>
<div class="paragraph"><p><strong>Building The Community</strong></p></div>
<div class="paragraph"><p>Across our interactions with users, we see a gulf in understanding what the budget is and what the citizens usually expect. We have engaged over 10,000 Nigerians over the budget and we profile them into three to ensure optimum value is delivered. The categories are briefly explained below:</p></div>
<div class="paragraph"><p><strong>Singular Users</strong>:  These are users who want information in simple and quick format. There are interested in data but not in a deeper scale of analyzing the content or probing further. A series of simple tweets is enough for them to retell the stories or interactive applications that give a snapshot.</p></div>
<div class="paragraph"><p><strong>Closed Loop Users</strong>:  A set of  users who stimulate a discussion pattern, engage the data channel effectively to increase their knowledge of subject matter or  challenge the assumptions of data. These set of users are adherents of the platform via social media, hence they ensure in building the platform through feedack systems or referral to their social connections.</p></div>
<div class="paragraph"><p><strong>The Data Hog</strong>: This set of users want the raw datasets to rework visualization or do extensive analysis for personal or enterprise purposes. We simply give them the data for their definitive purposes.</p></div>
<div class="paragraph"><p><strong>Engaging the Citizen</strong></p></div>
<div class="paragraph"><p>Across every society lies a literacy span and engaging every component is highly critical to the societal growth and stability. There is always a storyboard in every life that volumes of data can be matched with. People constantly want to be more informed especially concerning issues that they find difficult to understand. Engaging citizens is to take critical analysis of the target users at a time and itemize the possible profiles. A look at user profile demands a thorough analysis of their empathy, attention and insight towards the data available to them. What does the Nigerian citizen care about? Where is the information gap? How quickly can we reach out to them and place data in the storyboard of their lives? A critical understanding of the user’s psychology and the perceived response to the data is the first needed level of analysis. BudgIT’s immediate reach is to the average literate Nigerian connected to online forums and social media. Most online users amidst the array of interests in gaming, reading and sharing social connections within a limited timeframe will definitely need data in a brief and concise manner. After a snapshot of data  either as a tweet or infographics, there’s an opportunity to build linked data on other platforms where the big picture can be set and interaction can be enhanced.</p></div>
<div class="paragraph"><p>An important angle of visualization to us is understanding the data appreciation level of the users. Complex diagrams, superb infographics and aesthetic interactive applications might not convey the exact meaning to user based on his/her previous approach to data. Data vizualization needs to take into consideration how users can easily grasp the vizualized data and subject it to personal interpretation. A good visualization transfers knowledge and mostly important brings forth a story the user can easily connect with.</p></div>
<div class="paragraph"><p>For us in BudgIT, our engagement model is anchored on the following:</p></div>
<div class="paragraph"><p><strong>Stimulating discussion around Trends</strong>: In engaging with users on public data, BudgIT keeps track of online and offline discussions and seeks to provide data to enliven the interactions. A glaring example was the fuel strikes in January 2012 where there was a constant agitation among the protesters on the need to reduce the size of governance. BudgIT tracking the discussion via social media in 36 active hours quickly built an app that allows citizens to cut the Nigerian budget. The huge response with over 3,000 users who interacted with the budget using the app refined our  engagement model. We keep looking for trends in the polity and matching it with relevant data quickly rendered into tweets or infographic display that quickly  extends our influence.</p></div>
<div class="paragraph"><p><strong>Constructive feedback mechanism and Balanced outlook</strong>: Data speaks volumes and individuals subject it to personal interpretations. In the engagement with users, feedback is enabled through discussion boards or retweets. Most users throw up discussions that tend to ask about stories behind the data and seeking opinions of BudgIT. Its of utmost priority to ensure that opinions only explain the facts  behind the data and does not conform to individual disposition to the subject matter. It’s most necessary to build up feedback channels and engage the users creatively to ensure the community built around the data is sustained.</p></div>
<div class="paragraph"><p><strong>Localize Outlook of Data</strong>: For a dataset targeted at a particular group, BudgIT is building competency to localize its content and promote a channel of discussion that connects to the users. This involves taking a cultural outlook at the icons, symbols, objects and language to ensure that engagement concerning the budget flows seamlessly. This phase of engagement is with the grassroots who  mostly don’t have access to Internet but only possess SMS-based mobile phones.</p></div>
<div class="paragraph"><p>After making the public expenditure data  available in an easy-to-read format as shown on our portal yourbudgit.com, we reach out to the citizens through the civil society organizations to ensure that citizens monitor capital projects in the budget. We also plan to develop a participatory framework where citizens and government institutions can meet in town halls to define key items in the budget that needs to be prioritized. Once we get citizens to be aware of capital projects in the budget and connect them with civil societies where BudgIT is not located, citizens can track report projects and report status. Ensuring that citizens of any literacy span are armed with data and possess a clear path to demand action, BudgIT is crossing the rubicon from open data to open action.</p></div>
</div>
<div class="sect2">
<h3 id="_citizen_engagement_with_data_journalism_by_c_sar_viana_estacio_de_sa_university">Citizen engagement with data journalism by C√©sar Viana (Estacio de Sa University)</h3>
<div class="paragraph"><p>Journalism demands engagement with the public. This is a feature since the first newspapers that also influenced the creation of nation-states in the 17th century. At that time readers identified themselves with each other because at least they shared similar culture, imaginary, language and community interests. Whether this interaction is now through text, audio, audiovisual, animation or any other new visualization processes, trust and transparency are still crucial among journalists and citizen stakeholders.
Much before having any kind of communication technology our civilization were founded on storytelling and connection among people. A story is relevant as far as the audiences feel represented on it. But it took us centuries to improve the ways to bring people, and nations, closer to each other in a global - and almost apt to be scrutinized - scene. But the challenge still resides on experimenting emerging principles and practice of citizen engagement on inclusive journalistic perspectives to foster information access and popular participation.
Preparing and implementing editorial products based on datasets include as well participating on social conversations and considering user-centred design of information to better achieve citizen involvement. There are innovative ways to monitor feedbacks and civic participation on the social web and to encourage the creation of environments for dialog and social innovation.
A news professional might be prepared to attend the demands of the public and influence their capacity to act alongside their peers. The more people take ownership of these communication resources; greater are the reflections of the journalistic efforts in the everyday of society and vice versa.
Despite of being able to create data-driven journalism applications and services that are representative and able for accomplishing social improvements, the role of journalists is also enhance the social inclusion and education. A decisive way to do this is collaborate with the approach between the civic society stakeholders and the practitioners or researchers of ICT&#8217;s into human-centred outlooks.
The multidisciplinarity common to the variety of editorial sections of journalism has always collaborated with the interrelationship among different domains. When drawing or implementing strategies and plans consider leveraging opportunities and providing the knowledge and the skills to warrant inclusiveness, diversity and innovation in our societies. Make sure your project contributes to empower people through technology and/or media literacy.
If only by consuming or sharing news, the public is involved in modeling of information systems. Therefore, it is necessary to provide the best resources for collaboration and joint construction of stories. There is a growing emergence of examples that stand out in this foray:</p></div>
<div class="paragraph"><p>DocDiver is an application to stimulate readers on working beside ProPublica investigative reporters' team to identify and share key bits of information in documents.</p></div>
<div class="paragraph"><p>DocumentCloud is preparing a new feature to engage readers in discussing and crowdsourcing news by allowing them to add their own notes and comments to material collected and displayed.</p></div>
<div class="paragraph"><p>EUfunds is collaboration between the Financial Times and the Bureau of Investigative Journalism to help readers on tracking European Union structural funds.</p></div>
<div class="paragraph"><p>MoJo is the Knight-Mozilla partnership to bring together technologists and journalists in developing innovative solutions.</p></div>
<div class="paragraph"><p>Africa: What&#8217;s Your Story" is a collaborative platform of the citizen news agency A24Media to implicate people from across the continent on sharing information about media freedom, corruption, health, education and other issues in video, text and audio.</p></div>
<div class="paragraph"><p>The collaborative ways of treating or recirculating information and the bias for popular mobilizations are already means for important changes in this recent world of new media, big data sources and participatory culture. Even if only around 30 percent of the world population was online by the end 2010 (source: 2011 ITU report), there are space for commitment to spread the principles of freedom and to share experiences, best practice and opportunities to improve the capability of citizens and other stakeholders to write/interact/play/sense the next chapters of the history of press.</p></div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_8_the_bottom_line">8. The bottom line</h2>
<div class="sectionbody">
<div class="paragraph"><p># How to pitch a data story to your editor (possibly) #</p></div>
<div class="paragraph"><p># Measuring impact #</p></div>
<div class="paragraph"><p>Overview: Give overview of the potential of data journalism (e.g. engaging with new audiences, the future of journalism on the web) and how it could be measured.
Authors: Mirko Lorenz (Deutsche Welle), Lorenz Matzat (Open Data City)
Length: 1 page
Editor: Liliana Bounegru</p></div>
<div class="paragraph"><p>Measuring the success of a data story is in no way different than the generally used indicators: page views, how long people stay on a page, etc.</p></div>
<div class="paragraph"><p>Data journalists report though that people appear to engage longer with data stories than with other types of posts.</p></div>
<div class="paragraph"><p>The Texas Tribune databases drive majority of site&#8217;s traffic, according to a Poynter article.</p></div>
<div class="paragraph"><p># Case study 1: How audiences engage with data stories at Zeit Online #</p></div>
<div class="paragraph"><p>By Sascha Venohr</p></div>
<div class="paragraph"><p>Data journalism projects can bring newsrooms into the exciting position to bring a Wow-Effect to the audience and give a new way of bringing stories and facts on their side. For example, there was a wide coverage about the situation at the nuclear plant in japanese Fukushima after the Tsunami. After radioactive material escaped from the power plant, everyone within 30 kilometres of the plant was evacuated. People could read and see a lot about the evacuations. ZEIT ONLINE found a innovative way to explain it&#8217;s German audience the impact using data journalism. We asked: How many people live near a nuclear power plant in Germany? How many people lives within a radius of 30 kilometres? A map showes how many people would have to be evacuated in a similar situation in Germany. The result: A big traffic success and one of these Wow-Effects waving over the social media sphere. In addition, projects based on structured data can easily adopted to other perspectives and languages. So we did. We transponded the Map to nuclear power plants in the US in an english version. A great traffic-motor again.
Another point is, that news labels want to be respected as a trusted source. Using data journalism projects combined with enabling the readers to look and reuse the raw data brings a high value of credibility to these newsrooms. Facts are sacred and they RULEZ!</p></div>
<div class="paragraph"><p># Case study 2: Texas Tribune #</p></div>
<div class="paragraph"><p># Sustainability and business models #</p></div>
<div class="paragraph"><p>Overview: Discuss costs, sustainability and business models for data journalism. Provide successful and less successful examples and explain what lessons can be learned from them.
Authors: Mirko Lorenz (Deutsche Welle), Sascha Venohr (Zeit Online), Lorenz Matzat
Length: 1-2 pages
Editor: Liliana Bounegru</p></div>
<div class="sect2">
<h3 id="_needs_a_title_by_mirko_lorenz_deutsche_welle">Needs A Title by Mirko Lorenz (Deutsche Welle)</h3>
<div class="paragraph"><p>Go into any newsroom these days that relies primarily on traditional models and there is a sense of dispair. "Five years ago we had eight people on the local desk, now we are three and expected to do print plus parts of online reporting". There is no question that journalists have gone through ten years of web innovation, often even being early adopters and still seeing there own share of money from digital media go down, down, down.</p></div>
<div class="paragraph"><p>Can data journalism actually change that? Is it that big? While one must be very careful predicting the future, there are a few facts that should be considered when thinking about business models for data journalism and journalism as a whole.</p></div>
<div class="paragraph"><p><strong>1. Attention is no longer a scarce resource</strong></p></div>
<div class="paragraph"><p>This might be obvious, but still puzzles many, even very large and venerable media organizations. The "value" of attention has moved. The new electronic models, where the money goes, are based on providing highly targeted services. But that is not the business of newspapers or media companies, it&#8217;s the kingdom of the big search and social tech companies.</p></div>
<div class="paragraph"><p>So, what is left for media companies? In any economic process value is usually created by scarcity. Stuff that is available in abundance has a low price, stuff that is scarce will usually sell for higher prices.</p></div>
<div class="paragraph"><p>Today, in this multi-channel world, attention can be generated in abundance - but trust, another, highly important quality of any information is the scarce resource. Don&#8217;t take this lightly. While "trust" might seem something that is hard to measure and hard to monetize, it actually is relatively simple if the value of trust is understood. First of all: If you are not sure whether you can trust someone, you go to great length being careful and at the same time checking the information you get. If you trust someone, because of your long time contact or simply multiple proofs that a person or an organization values trust very highly, your costs of checking the information are going down. For example, if someone you trust tells you that a certain price vs. quality is the best you can get, then you will often make a decision about the purchase very quickly. If an organization you trust tells you that this financing plan is cool, you will be relieved and probably just sign the dotted line.</p></div>
<div class="paragraph"><p>Of course this is all oversimplified. But you get the idea, don&#8217;t you? Data-journalism combined with concepts based on building and maintaining trust could propel media organizations, small and big, old and new into a segment in society and the economy that is currently too small, while the attention-getters segment is simply too big.</p></div>
<div class="paragraph"><p>This, we would argue is the basis for future business models based on data-journalism. And actually, in a world where many organizations go to great length distorting clarity, hiding facts and trying to maintain there market share by little tricks, fine print in contracts or confusing names, etc. building media structures based on the core concept of trust might open up a really big market for journalism (in a good way) going into the future.</p></div>
<div class="paragraph"><p>One very funny thing here is that to make that happen the market as a whole must not even grow. Today, a lot of money is made based on mis-information. Are billions and billions of Euros drained away from you, me and our institutions, because it is possible and there is no better way.</p></div>
<div class="paragraph"><p>So, news publishers, media organizations and - more specifically - single journalists should think long and hard where there readers/users are currently underserved in terms of trustable information. From that starting point they should get creative and think about what kind of data would form a highly needed, trustable information offering. Our best guess at this moment is: If you solve this riddle for particular group or a segment of the market, making money won&#8217;t be your main concern in relatively short time.</p></div>
<div class="paragraph"><p><strong>2. Being first is not the goal</strong></p></div>
<div class="paragraph"><p>Many media companies believe in having "exclusive" content. They hope that by doing investigations they might get a share of attention from the audience which translates into more willingness from advertisers and so on. That chain, too, is kind of broken. Sometimes, adversly the revelation that one organization or a group of people are operating against the law, where the investigation of connections took six month or even a year might receive lukewarm surprise in the public. People in the streets will tell you that they "knew" in advance that there was something. Depressing, isn&#8217;t it.</p></div>
<div class="paragraph"><p>What would be an alternative? To say it in a memorable way: Don&#8217;t do it quick, do it right. Want an illustrative example: For the last months many articles and newscasts reported about the Euro crisis. Did you understand what was said? Or more precisely: Where you able to understand what this development might mean to you? This is where reporting is failing today: Often journalists are simply re-telling what they heard elsewhere - so, it&#8217;s the Euro crisis in much detail, but not in depth. What people would want and need (this even includes the high ranking politicians who must make decisions with not enough information) is a clear, understandble, deep and ideally individual/interactive view of how this crisis developed, how it maps out now and which concepts and investment policies could potentially really work. Yes, very difficult. But, again: Clarity is the scarce resource here, not being fast.</p></div>
<div class="paragraph"><p><strong>3. Data is the new black box and journalists can help unpack it</strong></p></div>
<div class="paragraph"><p>The main point of this section is a claim, and it goes like this: While we can not proof how much money could be made to finance journalism, the whole development currently gets a fresh new twist once a newsroom manages to untangle the web of data connections.</p></div>
<div class="paragraph"><p>To do that, they do not need multi-million databases or expensive IT. Instead they should invest into the knowledge of journalists, extending their options and ways to find the information that is really important.</p></div>
<div class="paragraph"><p>What if we would have thousands of really experienced data-journalists, providing information on all levels of society: Locally, in regions, for countries or the world as a whole. There surely would be problems and wrong interpretations of data, too. But by and large we could hope that many investigations into data could help all of us get along better.</p></div>
<div class="paragraph"><p>Sometimes visualizing the results of data interrogations becomes an art, but equally often being able to show a specific development in relatively simple charts can still come as a surprise. A prime example of that is the cool "How the deficit got this big" story from the New York Times, in which they compare the estimated and actual budgets of the US. Having a longer memory, being able to boil down something into one picture that puzzles everybody is the new scoop. Data journalism is a tool to do that. It&#8217;s like switching on a light in a black room.</p></div>
<div class="paragraph"><p><strong>4. Money is no scarce resource</strong></p></div>
<div class="paragraph"><p>In the world of journalism money is scarce. The hard work of writing and photographing is and has been paid low. One reason is that the while writing is pretty complex to master, it is very difficult if not impossible to attach a certain value tag to one well-written text. It&#8217;s just hard. The text might move you to tears or laugh out loud - but now one knows that prior to publication.</p></div>
<div class="paragraph"><p>Using data as a basis for reporting, this might change. Data in today&#8217;s interconnected world can help to get the big picture as well as the very detailed view how a certain development affects me and you - even telling us separately how much money out of our own pockets will go into the bail-out of a failed bank or country.</p></div>
<div class="paragraph"><p>This is why stories based on data can be much more powerful than a typical article. This is not implying that one form is better than the other. But it says that being able to point people to data analysis if they ask why a certain conclusion is drawn is much more reliable than an opinion. There are more facts, more data points, the whole story can become much deeper and - using the data - can even be broken down to show the very individual effects of a development.</p></div>
<div class="paragraph"><p>In the data world, which is right now owned by large institutions, banks and big companies, money is no scarce resource. In Germany, as of November 2011, there was a very peculiar case, where a "bad bank" which was set-up after the 2008 crisis, actually managed to "loose" 53 billion Euros, due to a software mistake. That money was gone for a few months before it surfaced again.</p></div>
<div class="paragraph"><p>53 billion Euros though would define one of the very big media/advertising markets in a Western country. Using data, we should stop worrying about refinancing journalism. If the analysis is really, really good there a many options for future media companies to sell trustable services - such as an individual investment guide, reliable calculators to finance a house, buy a car, etc.
The point is: While this market is not there nor easy to create - who would argue against the need for trustable, reliable analysis that even extends to help you and me making good decisions?</p></div>
<div class="paragraph"><p>Plus, if you look around from this angle, there actually are examples of companies using a combination of journalism and data with some success.
Statista is one. The Hamburg-based company takes data and helps users to find good charts. What they do is basically service, not journalism in the original concept - but the basic principle is intact. There are others, often in niches, constantly collecting data and then publishing the findings. Often these compaies are combining different areas such as consulting, research, publishing based on data. Often they are quite successful in this, even in slack economic periods. What media companies and journalists need to understand though is that these data-driven companies might not resemble what we identify as a media company today. So, a big need for further development of data-driven journalism is to understand and model sustainable data-driven newsrooms.</p></div>
<div class="paragraph"><p><strong>5. Look for examples, then re-model</strong></p></div>
<div class="paragraph"><p>It is said that changing media is incredible hard. Which is true as long as there are only the old models of making money with media content are applied. The sources of money for a typical media company are: Subscriptions, advertising and - so some extend - single items sales, e.g. like a book publisher. The relation of these major sources of income vary, but in most markets around the world will comprise the biggest source of revenue that is used to finance journalism.</p></div>
<div class="paragraph"><p>It&#8217;s not so much a sales problem, it&#8217;s a product value problem. People - you, me, everybody - buy something all the time. And there is a huge unmet demand for reliable, trustable and understandable information. Examples of such information could be described for almost any area of life: Politics, economics, sports. Or even more concrete: An interactive guide with reliable calculators how to buy a house in a specific region. A guide to green energy, broken down to your particular needs. A guide how to get a car, where all the costs factors can be used to make a decision beyond the advertisers claims, such as cost of ownership, resale prices, etc. These are just examples, painted with a few brushes and far from complete. But in effect, producing reliable, individual packages of information is a potential market - with many ways how to combine it with journalism or using such end-products to finance journalism.</p></div>
<div class="paragraph"><p>Building this market is challenge. To be successful, three fundamental aspects must be covered:
First, there must be a real value. The "services" must be so good, that they provide a real new option to make better descisions.
Secondly, the value should be higher than the price. This sounds abstract but isn&#8217;t. Just imagine a data-driven media company offers a guide for house buying that goes far beyond what a bank would be willing to tell you, even comparing one bank versus the other. If the cost of such a service would 20 Euro, but the potential savings using it go into thousands of euros - this is when such services are actually marketable.
Thirdly, and this is often overlooked, there must be a whole, working and accepted system for such payments. Right now, both journalists and potential customers do not even expect such services from most media companies. To get this to a high level we first need pioneers who proof that this is a market and then apply the models to newsrooms, old and new.</p></div>
<div class="paragraph"><p>To give one example how such shifts and markets can be created, just take Google. The search company has masterfully automated a steady stream of income through it&#8217;s AdWords concept. Around the world single people and large companies are booking AdWords, ranging from investments of 10, 20 Euros to millions of Euros. This is how Google makes money.</p></div>
<div class="paragraph"><p>So, the best guess we can make at this moment goes like this: The future of media and journalism might still rely on subscriptions and sales of advertising space, but to a (hopefully big) extend datajournalism can help to create novel services.
Datajournalism offers an opportunity to provide insightful stories, which - once the data is understood - can be transformed into services, tools, apps and personalized information.</p></div>
<div class="paragraph"><p>Right now, many if not most media companies are from having something like that. There was no incentive to cover topics in depth, instead the main incentive was to drum up attention.</p></div>
<div class="paragraph"><p>But, it is not impossible to master that shift. If you look more deeply there are quite a number of working models in this space. The list would range from larger media companies like Thompson Reuters to The Economist to many information providers who fill a niche and employ thousands of people based on looking into data. This would to some extend include companies like eMarketer, Marketing Sherpa - often with blurring lines to make the distinction whether these are primarily journalism companies or research companies. Effectively, we will have to watch this space and make descions whether to lean more towards one side or the other.</p></div>
<div class="paragraph"><p>Two very good examples are Thompson Reuters and The Economist. In the case of Thompson, just look at the history of this company that has grown out of one regional newspaper in Canada into one of the big data providers for financial markets. Preparing for this Thompson relied on in-depth, professional information streams. Simplified they built a database, maintained by a number of editors to cover one market niche in depth and then selling this information once it was good enough. These models are often successful after a longer period of building expertise, one benefit being that they are pretty stable over long periods.</p></div>
<div class="paragraph"><p>The Economist is another example. The magazine has built an excellent, influential brand on it&#8217;s media side. At the same time the "Economist Intelligence Unit" is now more like a consultancy, reporting about relevant trends and forecasts for almost any country in the world. They are employing hundreds of journalists and claim to serve about 1,5 million customers worldwide.</p></div>
<div class="paragraph"><p>Finally, we should be aware of the many examples for new offerings in all stages of data usage. An uncomplete here could include quite a number of new offerings which are all buidling blocks for a better use of data for the public. Technology today enables many new ways to extract value. Examples of companies and offerings making use of this would include: DataSift, Timetric, Junar, Needlebase, Flowing Data, The Guardian Data Blog and Data Store - just to provide some examples. And there is room to try out new approaches for many levels of data use. Take BuzzData, a site where datasets can be published and discussed in public.</p></div>
<div class="paragraph"><p>"This is not journalism"
Yes, yes - at this point in discussions, journalists often exclaim that "this is not journalism". This is true sometimes, sometimes not. Good journalism goes beyond just being a commercial product or service, good journalism plays a role as a public service, too.</p></div>
<div class="paragraph"><p>When inventing new ways to get both sides in line - the financing and the special role of journalism, we must think of all components, not just the newsroom. Selling advertising, which is a department in every commercial media organization is not journalism, too. But it is surely needed to make the work of the newsroom possible. A lot of the work done in media companies effectively has the role of a supporting role for the core business of journalism.</p></div>
<div class="paragraph"><p>When looking at new models and trying to come up with something really new we should accept that these models will need several components: Good (data-driven) reporting, but maybe other lines of business and services to enable the journalism part of what the company is doing.</p></div>
<div class="paragraph"><p>This is just starting..
This whole shift is just starting. The assumptions presented here what might work surely have some big holes in them. But be patient, keep on asking the right questions. And help us finding the missing elements to make the case for data journalism complete and sturdy. Saving journalism is worth the effort.</p></div>
<div class="paragraph"><p>Understanding/refining/inventing models is one of the big challenges in this space. While it is not very satisfactory to leave it like that, two things might be accepted: It is possible. But as of now we simply have not enough experience and examples to understand these new models. One data point towards a future where editors and reporters will use data consistently as a starting point and not as an after thought is a study conducted by McKinsey in 2010, which got cited on "Flowing Data": There is a growing need for data heads and data managers. People who know how to transform raw collections of numbers into words and stories.</p></div>
<div class="paragraph"><p>Right now we as the journalists experiencing this shift from old to new models do not have information ourselves. But this is the big challenge we have to master: Finding new, sustainable models to run journalism projects and reliable media companies using data more and better. If we succeed a new perspective for journalism could open up: New offerings, new sources to finance good reporting and in the long run even more jobs in this space than before.</p></div>
</div>
<div class="sect2">
<h3 id="_our_stories_come_as_code_by_lorenz_matzat_opendatacity_de">Our Stories Come As Code by Lorenz Matzat (OpenDataCity.de)</h3>
<div class="paragraph"><p>How we started our data journalism company and interface design became part of our journalistic work</p></div>
<div class="paragraph"><p>Building a business</p></div>
<div class="paragraph"><p>End of 2010 we, then two partners, founded our company named OpenDataCity. There was more or less no modern data journalism happening in German media before this year.</p></div>
<div class="paragraph"><p>Why did we do this? Now so often we heard from people working for newspapers and broadcasters: "No, we are not ready to put up a data journalism unit in our newsroom. But we would be happy to outsource this to someone." So we got ready.</p></div>
<div class="paragraph"><p>Until today we are the only company in Germany, specialized in data journalism as far as we know. Nowadays we are three partners, two of us with a journalism background and one with a deep understanding of code and visualisation. We are working within a network of a handful freelancing hackers, designers and journalists. In the last twelve month we did four data projects with newspapers, gave several trainings and consultations to media workers, scientists and journalism schools. The first app we did was an interactive tool on airport noise around the the newly built airport in Berlin. Second worth mentioning was an application about data retention of the mobilephone usage of a German politician (.For this we won together with ZEIT Online a Grimme Online Award and Lead Award in Germany. And in the US an Online Journalism Award by the Online Journalism Association.</p></div>
<div class="paragraph"><p>Today, end of 2011, we are having several projects in our pipeline - ranging from more simple interactive infographics up to full blown realtime data applications.
Of course, winning prizes helps to built a reputation. But when we talk to the publishers, who have to ok the often expensive projects our argument for investing into data journalism is not about winning prizes. It&#8217;s about getting attention on a long run in a sustainable way. Building things for the long term effect, not for the scoop, which often is forgotten after some days.</p></div>
<div class="paragraph"><p>There a three arguments for convincing publishers to build things for the long term effect:</p></div>
<div class="olist arabic"><ol class="arabic">
<li>
<p>
Depending on their design, new content can be added to data journalism apps. And they are not for the users alone, but can be used internally for reporting - the problem after the app is published: the competition can too. That can be countered by keeping some features or data for internal use only.
</p>
</li>
<li>
<p>
Besides the sustainable effect concerning the content side of a data project, you often build code, which is reusable, updateable or at least you gain experience doing it. The next development probably cost half the time, because you know much better what to do (and what not to).
</p>
</li>
<li>
<p>
In the end, its often about the money first and second about journalism. But there is a strong argument why data journalism in the end pays for itself. Because it&#8217;s much cheaper then traditional marketing campaigns. In online journalism much money is spent by bigger media outlets in the realm of search-engine-optimisations (SEO) and search-engine-marketing (SEM). A well done data project draws a lot of clicks and people link to it. Often there is a viral effect in the social media sphere. One reason for this is, that a data project is real, a authentic work of journalism. It can&#8217;t be copied and pasted. As a publisher you&#8217;re likely to pay much less for it then trying to generate the same attention by the somehow artificially clicks and links achieved through SEM.
</p>
</li>
</ol></div>
<div class="paragraph"><p>Software design as a journalistic skill</p></div>
<div class="paragraph"><p>Our work is not very different from other agencies, providing applications or services for news outlets. But maybe we differ in our self-conception: We think of ourselves first of all as journalists. In our eyes the products we deliver are articles or stories. Though not provided in letters, pictures or audio but in code.</p></div>
<div class="paragraph"><p>When we are talking about data-driven journalisms we have to talk about technology, software, devices and how to tell a story using them for our means.</p></div>
<div class="paragraph"><p>To give an example: We right now working on an application, which pulls in realtime data via a scraper. Data which is update every minute or so. We started doing this some month ago, thus having collected a huge dataset which grows every hour - by now it amounts to some hundred thousand rows of data. We hope to create a data-articles which provides the user with the opportunity to dive in to this realtime data. And to do research in the archive of the foregone months. In the end the story we are telling will be significantly defined by the individual action of the users.</p></div>
<div class="paragraph"><p>In traditional journalism, due to the linear character of written or broadcasted media, we have to think about a beginning, the end, the story arc and often the length of our piece. With datajournalism apps is mostly different. There is a beginning, yes. People come to the website, get a first impression of the interface. But then they are on their own. Maybe they stay for a minute - or half an hour.</p></div>
<div class="paragraph"><p>Our job as data journalists is to provide the framework or environment for this. Letting the reporting, the programming part and how to push lots of data to the frontend aside, we have to find working solutions for the design. The User Experience (UX) derives mostly from the (graphical) User Interface (GUI). In the end it comes to this crucial part. You could have the best code working in the background handling an exiting dataset. But if the frontend sucks nobody will care about it. So right decisions have to be made, e.g. which liberties and options user have to change from and where we hinder them to explore further.</p></div>
<div class="paragraph"><p>There is still a lot to learn and experiment. But luckily there is the games industry, which designs digital narratives, ecosystems and interfaces for several decades now. So when developing for datajournalism we should closely watch how gamedesign works and stories are told in games. Why are casual games like Tetris such fun? And what makes the open worlds of sandbox games like Grand Theft Auto or Skyrim rock?</p></div>
</div>
<div class="sect2">
<h3 id="_kaas_og_mulvad_semi_finished_content_for_stakeholder_groups_by_mark_lee_hunter_and_luk_n_van_wassenhove_affiliations">Kaas og Mulvad: Semi-finished content for stakeholder groups by Mark Lee Hunter and Luk N. Van Wassenhove (Affiliations?)</h3>
<div class="paragraph"><p>Stakeholder media often lack capacity even more cruelly than do the news media, and the function of Kaas og Mulvad, a for-profit Danish corporation, is to provide it. The firm originated in 2007 as a spinoff of the non-profit Danish Institute for Computer-Assisted Reporting (Dicar), which sold investigative reports to media and trained journalists in data analysis. Its founders, Tommy Kaas and Nils Mulvad, were previously reporters in the news industry. Their new firm offers what they call "data plus journalistic insight" to stakeholder media, which finalise the content and distribute it to news media, as well as through their own media.  Direct clients include government institutions, PR firms, labour unions and NGOs such as EU Transparency and the World Wildlife Fund. The NGO work includes monitoring farm and fishery subsidies, and regular updates on EU lobbyist activities generated through "scraping" of pertinent websites. Indirect clients include foundations that fund NGO projects. The firm also works with the news industry: A tabloid newspaper purchased a celebrity monitoring service.</p></div>
<div class="paragraph"><p><strong>1. Processes: Innovative IT plus analysis</strong></p></div>
<div class="paragraph"><p>The firm undertakes about 100 projects per year, ranging in duration from a few hours to a few months. It also continuously invests in projects that expand its capacity and offerings. The celebrity monitoring service was one such experiment. Another involved scraping the Internet for news of home foreclosures and creating maps of the events. The partners say that their first criteria for projects is whether they enjoy the work and learn from it; markets are sought after a new service is defined. They make it clear that in the news industry, they found it difficult to develop new methods and new business. Comments Mulvad:</p></div>
<div class="paragraph"><p>"We have no editors or bosses to decide which projects we can do, which software or hardware we can buy. We can buy the tools according to project needs ‚Äì like the best solutions for text scraping and mining. Our goal is to be cutting edge in these areas. We try to get customers who are willing to pay, or if the project is fun we do it for a lower charge."</p></div>
<div class="paragraph"><p><strong>2. Value created: Personal and firm brands and revenue</strong></p></div>
<div class="paragraph"><p>Turnover in 2009 was approximately 2.5 million Danish kroner, or ‚Ç¨ 336,000. The firm also sustains the partners' reputations as cutting edge journalists, which maintains demand for their teaching and speaking services. Their public appearances, in turn, support the firm&#8217;s brand.</p></div>
<div class="paragraph"><p><strong>3. Key insights of this example</strong></p></div>
<div class="paragraph"><p>‚Ä¢ The news industry&#8217;s crisis of declining capacity is also a crisis of under-utilisation of capacity. Kaas and Mulvad had to leave the news industry to do work they valued, and that pays. Nothing prevented a news organisation from capturing that value.</p></div>
<div class="paragraph"><p>‚Ä¢         In at least some markets, there exists a profitable market for semi-finished content that serves the interests of stakeholder groups.</p></div>
<div class="paragraph"><p>‚Ä¢ However, this opportunity raises the issue of how much control journalists can exercise over the presentation and use of their work by third parties. We recall that this issue already exists within the news industry (where editors can impose changes on a journalist&#8217;s product), and it has existed within other media industries (such as the film industry, where conflicts between directors and studios over "final cuts" are hardly rare). It is not a particular moral hazard of stakeholder media.</p></div>
<div class="paragraph"><p>‚Ä¢ From a revenue standpoint, a single product or service is not enough. Successful watchdog enterprises would do better to take a portfolio approach, in which consulting, teaching, speaking and other services bring in extra revenue, support the watchdog brand, and enrich the lifestyle of the operators.</p></div>
</div>
<div class="sect2">
<h3 id="_when_code_pays_for_words_by_cl_ment_renaud_sharism_lab">When code pays for words by Cl√©ment Renaud (Sharism Lab?)</h3>
<div class="paragraph"><p>Example from OWNI</p></div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_appendix">Appendix</h2>
<div class="sectionbody">
<div class="paragraph"><p>Overview: Lists of links, resources, examples and other bits and pieces that don&#8217;t fit in the handbook
Authors: everyone! Special thanks to Claire Miller for her list of Data Journalism Resources <a href="http://clairemiller.net/blog/2012/01/huge-list-of-data-journalism-resources/">http://clairemiller.net/blog/2012/01/huge-list-of-data-journalism-resources/</a>
Length: 5 pages</p></div>
<div class="paragraph"><p>Data Blogs</p></div>
<div class="paragraph"><p><a href="http://blogs.montrealgazette.com/category/montreal/data-points/">http://blogs.montrealgazette.com/category/montreal/data-points/</a>
<a href="http://onlinejournalismblog.com/">http://onlinejournalismblog.com/</a>
Hard core: <a href="http://thingsivelearned.posterous.com/">http://thingsivelearned.posterous.com/</a>
<a href="http://datadrivenjournalism.net">http://datadrivenjournalism.net</a>
<a href="http://www.datajournalismblog.com/">http://www.datajournalismblog.com/</a>
<a href="http://www.calculatedriskblog.com/">http://www.calculatedriskblog.com/</a>
<a href="http://www.datauncovered.com/about">http://www.datauncovered.com/about</a>
<a href="http://blogs.channel4.com/factcheck/">http://blogs.channel4.com/factcheck/</a>
<a href="http://www.freakonomics.com/blog/">http://www.freakonomics.com/blog/</a>
<a href="http://www.guardian.co.uk/news/datablog">http://www.guardian.co.uk/news/datablog</a>
<a href="http://localdata.citizenshipfoundation.org.uk/">http://localdata.citizenshipfoundation.org.uk/</a>
<a href="http://www.madwdata.org.uk/blog">http://www.madwdata.org.uk/blog</a>
<a href="http://www.propublica.org/tools/">http://www.propublica.org/tools/</a>
<a href="http://blog.timetric.com/">http://blog.timetric.com/</a>
<a href="https://wikileaksdatajournalism.wordpress.com/">https://wikileaksdatajournalism.wordpress.com/</a></p></div>
<div class="paragraph"><p>FOI blogs</p></div>
<div class="paragraph"><p><a href="http://heatherbrooke.org/">http://heatherbrooke.org/</a></p></div>
<div class="paragraph"><p>Tech Discussion</p></div>
<div class="paragraph"><p><a href="http://gigaom.com/cloud/">http://gigaom.com/cloud/</a>
<a href="http://highscalability.com/">http://highscalability.com/</a></p></div>
<div class="paragraph"><p>Data Visualization Blogs:
Stephen Few&#8217;s Perceptual Edge
<a href="http://www.perceptualedge.com/examples.php">http://www.perceptualedge.com/examples.php</a>
David McCandless&#8217;s Information is Beautiful
<a href="http://www.informationisbeautiful.net/">http://www.informationisbeautiful.net/</a>
Doug McCune&#8217;s Adobe Flex- and ActionScript-focused blog:
<a href="http://dougmccune.com/blog/">http://dougmccune.com/blog/</a>
<a href="http://flowingdata.com/">http://flowingdata.com/</a>
<a href="http://infosthetics.com/">http://infosthetics.com/</a>
<a href="http://www.pdviz.com/">http://www.pdviz.com/</a>
<a href="http://chartporn.org/">http://chartporn.org/</a>
<a href="http://eagereyes.org/">http://eagereyes.org/</a>
<a href="http://visualoop.tumblr.com/">http://visualoop.tumblr.com/</a></p></div>
<div class="paragraph"><p>Examples</p></div>
<div class="paragraph"><p>TODO: group by chapter</p></div>
<div class="paragraph"><p><a href="http://www.nytimes.com/interactive/2011/10/23/sunday-review/an-overview-of-the-euro-crisis.html">http://www.nytimes.com/interactive/2011/10/23/sunday-review/an-overview-of-the-euro-crisis.html</a>
The New York Times' interactive tool, allowing you both to check a few different standard perspectives (links on the left) and to explore the landscape on your own (hover/click on particular countries).</p></div>
<div class="paragraph"><p><a href="http://www.oecdbetterlifeindex.org/">http://www.oecdbetterlifeindex.org/</a>
Combines data on political support for, and citizens' opinions of, various aspects of their quality of life. Enables each use both to compare among countries and to see how countries rank relative to their own priorities.</p></div>
<div class="paragraph"><p><a href="http://www.gapminder.org/">http://www.gapminder.org/</a>
The classic example of a site that permits the telling of many different historical back-stories to key social issues of today.</p></div>
<div class="paragraph"><p><a href="http://www.politifact.com/subjects/afghanistan/">http://www.politifact.com/subjects/afghanistan/</a>
An example of how political dialogue can be partially visualized according to its relative "truthiness." There are may examples at politifact.com; this is just one.</p></div>
<div class="paragraph"><p><a href="http://poligraft.com/">http://poligraft.com/</a>
A project of the Sunlight Foundation (which has many excellent projects), this particular one allows the addition of influence-peddling metadata to an existing article or blog post via the submission of plain text or the URL.</p></div>
<div class="paragraph"><p>Crowd sourcing data:
<a href="http://www.bbc.co.uk/news/technology-14644507">http://www.bbc.co.uk/news/technology-14644507</a>
Many examples - investigate further&#8230;
<a href="http://greatjournalism.net/">http://greatjournalism.net/</a></p></div>
<div class="paragraph"><p>Case Studies FOI :</p></div>
<div class="paragraph"><p>Campaign for Freedom of Information www.cfoi.org.uk has summaries, e.g. <a href="http://www.cfoi.org.uk/pdf/FOIStories2006-07.pdf">http://www.cfoi.org.uk/pdf/FOIStories2006-07.pdf</a>
BBC Open Secrets blog: <a href="http://www.bbc.co.uk/blogs/opensecrets/2011/04/commissioner_attacks_cabinet_office_foi_delays.html">http://www.bbc.co.uk/blogs/opensecrets/2011/04/commissioner_attacks_cabinet_office_foi_delays.html</a>
The BBC News website contains a section featuring selected news stories obtained under freedom of information - <a href="http://news.bbc.co.uk/1/hi/in_depth/uk/2006/foi/default.stm">http://news.bbc.co.uk/1/hi/in_depth/uk/2006/foi/default.stm</a>.
Guardian <a href="http://www.guardian.co.uk/politics/freedomofinformation">http://www.guardian.co.uk/politics/freedomofinformation</a>
Legal Leaks: we have a couple of stories, one from Bulgaria one from Spain, and plan to put up more: <a href="http://www.legalleaks.info/blog.html">http://www.legalleaks.info/blog.html</a>
See also the excellent study released by AP: <a href="https://www.facebook.com/note.php?note_id=10150914656220651">https://www.facebook.com/note.php?note_id=10150914656220651</a></p></div>
<div class="paragraph"><p>Tools</p></div>
<div class="paragraph"><p>Investigative Dashboard
Access Social data, analyze it, and get value from it. <a href="http://blog.datasift.com/">http://blog.datasift.com/</a> ? Include</p></div>
<div class="paragraph"><p>Resources</p></div>
<div class="paragraph"><p>Resources for FOI:</p></div>
<div class="paragraph"><p>‚Ä¢ Reporters Committee for Freedom of the Press FOIA Letter Generator
‚Ä¢ National Freedom of Information Coalition sample FOIA letters
‚Ä¢ FOIAnet"</p></div>
<div class="paragraph"><p>Learn to code</p></div>
<div class="paragraph"><p>Coding Q and A
Code Academy (beginning lesson = JavaScript)</p></div>
<div class="paragraph"><p>Ruby
The Bastards Book of Ruby
Hackety Hack
Rails Casts</p></div>
<div class="paragraph"><p>Python
Think Python
Learn Python The Hard Way
Crash into Python
Khan Academy</p></div>
<div class="paragraph"><p>CSS
Learn Html &amp; CSS in 30 days
Javascript
Nathan&#8217;s Lessons</p></div>
<div class="paragraph"><p>Communities</p></div>
<div class="paragraph"><p>NICAR mailing list
Data-driven-journalism mailing list
HacksHackers <a href="http://hackshackers.com/">http://hackshackers.com/</a>
Q&amp;A: Quora
<a href="http://www.quora.com/Data-Journalism">http://www.quora.com/Data-Journalism</a></p></div>
<div class="paragraph"><p>Books</p></div>
<div class="paragraph"><p>syndicate from other chapters</p></div>
<div class="paragraph"><p>Thomas Pettigrew, "How to Think Like a Social Scientist," Harpercollins 1995. (Recommended by Philip Meyer as textbook for teaching investigative methods)</p></div>
</div>
</div>
</div>
<div id="footnotes"><hr /></div>
<div id="footer">
<div id="footer-text">
Last updated 2012-02-21 18:41:17 GMT
</div>
</div>
</body>
</html>
